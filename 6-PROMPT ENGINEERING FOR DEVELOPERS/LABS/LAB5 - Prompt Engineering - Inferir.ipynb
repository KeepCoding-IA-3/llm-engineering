{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2WR+8U7Cp6IiQU4g2lOtm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LAB5: Prompt Engineering - Inferir"],"metadata":{"id":"Z2v3JuSp5hEu"}},{"cell_type":"markdown","source":["## Configuración de la API de OpenAI\n","Este código importa las librerías necesarias para trabajar con la API de OpenAI, carga las variables de entorno desde un archivo .env y configura la clave de la API de OpenAI para ser utilizada en las solicitudes a la API.\n"],"metadata":{"id":"LV8dq9PQzkBE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ls6QhHKIzbOF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452068609,"user_tz":-60,"elapsed":9553,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"0719b7ea-4a4d-410c-bd02-16f973168de6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.3)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"]}],"source":["%pip install openai"]},{"cell_type":"code","source":["from openai import OpenAI\n","import getpass\n","\n","api_key = getpass.getpass(\"Enter your OpenAI API Key:\")\n","\n","client = OpenAI(api_key = api_key)"],"metadata":{"id":"hSyYt3DJzm05","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452089815,"user_tz":-60,"elapsed":21208,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"bdeecbce-bc25-4a1f-ccd2-58e8c053a2aa"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API Key:··········\n"]}]},{"cell_type":"markdown","source":["## Función para obtener respuestas de GPT\n","Este código define una función `get_completion` que toma un prompt y opcionalmente un modelo (por defecto `gpt-3.5-turbo`) y utiliza la API de OpenAI para obtener una respuesta. La función configura la solicitud con una temperatura de 0 para respuestas más deterministas y retorna el contenido de la respuesta.\n"],"metadata":{"id":"KYks3dYHzxFa"}},{"cell_type":"code","source":["def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","    return response.choices[0].message.content"],"metadata":{"id":"iWQLL9w0zxyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment analysis"],"metadata":{"id":"WFQ1JDW93_D-"}},{"cell_type":"code","source":["messages = [\n","    \"¡Increíble partida! No podía dejar de mirar, ¡qué final!\",\n","    \"Este streamer no sabe lo que hace, muy aburrido...\",\n","    \"¿Alguien más está teniendo problemas con el stream? Se corta a cada rato.\",\n","    \"Me encanta este canal. Siempre buen contenido y buena onda :)\"\n","]"],"metadata":{"id":"NGJfgZjn5KFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(messages)):\n","    prompt = f\"\"\"\n","\n","    You are an expert in conducting sentiment analysis on user-generated content.\n","    Your current task involves analyzing a set of comments from Twitch to determine\n","    the underlying sentiment: positive, negative, or neutral. For each comment,\n","    you will assess the tone, language, and context to classify its sentiment\n","    accurately.\n","\n","    This analysis will help in understanding viewer engagement,\n","    content reception, and potential areas for improvement or commendation\n","    for streamers.\n","\n","    You MUST only provide the result of \"positive\", \"negative\" or \"neutral\"\n","\n","    message: ```{messages[i]}```\n","    \"\"\"\n","\n","    response = get_completion(prompt)\n","    print(i, response, \"\\n\")"],"metadata":{"id":"KFDU3ffF4PV1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452135045,"user_tz":-60,"elapsed":1435,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"8e1dbb4e-eec2-4799-b98b-d3b8b6ae9064"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 positive \n","\n","1 negative \n","\n","2 negative \n","\n","3 positive \n","\n"]}]},{"cell_type":"markdown","source":["## Extraer información relevante"],"metadata":{"id":"D_y2YH2K8cDM"}},{"cell_type":"code","source":["review = \"\"\"\n","El problema del material de estas calleras de PICSIL es que no tiene grip para hacer\n","todos los ejercicios de gymnastics. Para halterofilia puede servir pero para\n","gymnastics recomiendo coger otras que sean de otro tipo de piel que no tenga\n","ese acbaado. A mi me encanta la marca de PICSIL, pero no este diseño.\n","Me compré las calleras para substituir las viejas y sigo utilizando las\n","viejas porque con las nuevas me resbalo.\n","\"\"\""],"metadata":{"id":"aNfOXBB58oNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = f\"\"\"\n","Identifica los siguientes elementos del texto de la reseña:\n","- Artículo comprado por el reseñador\n","- Compañía que ha fabricado el artículo\n","\n","La reseña está delimitada con comillas triples invertidas. \\\n","Formatea tu respuesta como un objeto JSON con \\\n","\"artículo\" y \"marca\" como claves. \\\n","Si la información no está presente, utiliza \"desconocido\" \\\n","como valor.\n","Haz tu respuesta tan corta como sea posible.\n","\n","Texto de la reseña: '''{review}'''\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"id":"Cc1sbGli8k1T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452188753,"user_tz":-60,"elapsed":742,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"4f82afd2-c9a9-405f-b4fc-465301c1dd0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"artículo\": \"calleras\",\n","    \"marca\": \"PICSIL\"\n","}\n"]}]},{"cell_type":"markdown","source":["## Más de una tarea a la vez\n"],"metadata":{"id":"CN8i-gvT92Dx"}},{"cell_type":"code","source":["prompt = f\"\"\"\n","Identifica los siguientes elementos del texto de la reseña:\n","- Sentimiento (puede ser solo positivo o negativo)\n","- ¿El reseñador expresa enojo? (verdadero o falso)\n","- Artículo comprado por el reseñador\n","- Compañía que ha fabricado el artículo\n","\n","La reseña está delimitada con comillas triples invertidas. \\\n","Formatea tu respuesta como un objeto JSON con \\\n","\"sentiment\", \"anger\", \"article\" y \"brand\" como claves. \\\n","\n","sentiment puede ser solo \"positive\" o \"negative\"\n","Si la información no está presente, utiliza \"desconocido\" \\\n","como valor.\n","Haz tu respuesta lo más corta posible.\n","Formatea el valor sentiment como un booleano.\n","\n","Texto de la reseña: '''{review}'''\n","\"\"\"\n","response = get_completion(prompt, model=\"gpt-4o\")\n","print(response)\n"],"metadata":{"id":"Wfp3HdOX95Qz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452311866,"user_tz":-60,"elapsed":1714,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"1ccb737d-7ed5-4a2d-9611-56f310d74d52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","{\n","  \"sentiment\": \"negative\",\n","  \"anger\": false,\n","  \"article\": \"calleras\",\n","  \"brand\": \"PICSIL\"\n","}\n","```\n"]}]},{"cell_type":"markdown","source":["## Extreure categories"],"metadata":{"id":"Uw8w8h_X_IYZ"}},{"cell_type":"code","source":["transcript = \"\"\"\n","Ayer fue un día histórico Y es que no tuvimos solo una de esas noticias que se puede considerar noticia del año sino que tuvimos dos y es que Google presentó gemini 1.5 un modelo multimodal con una ventana de contexto enorme y unas capacidades sorprendentes pero esta noticia quedó en nada después del anuncio de Open Ai Y es que Open a presenta sora un modelo de generación de vídeo que no tiene nada que ver con los modelos actuales directamente fulmina la competencia creando vídeos con una consistencia temporal increíble una calidad alucinante pero que además va mucho más allá de ser un mero generador de vídeo tal y como plantea Open a puede ser la puerta a modelos de simulación de mundos es algo completamente alucinante es un modelo que puede cambiar al paradigma y acercarnos mucho más a la ig que las últimas innovaciones que hemos visto en modelos de generación de lenguaje y hoy me voy a centrar solo en esta noticia y habrá tiempo tiempo para gemini pero hoy la noticia es sora dentro vídeo y vamos a empezar directamente por lo que es la gran noticia del día de ayer el nuevo modelo de generación de vídeo de Open sora y lo que estás viendo ahora mismo este fondo de la página web es un vídeo generado con esta tecnología pero es que no han mostrado un vídeo han mostrado una cantidad ingente de ellos y lo que se puede apreciar los detalles que se pueden apreciar en Ellos nos hace pensar que estamos ante algo muy especial Algo totalmente diferente a lo que hemos visto hasta ahora lo que os voy a enseñar hoy no tiene nada que ver ni con Rango ni con pica por muchos motivos ya no solo por la calidad de imagen ni tan siquiera por el hecho de permitir vídeos muchos más largos veremos que hay detalles que van incluso Más allá de esto pero lo mejor es ver primero algunos ejemplos de lo que puede hacer sora para que simplemente alucinosis artificial no esperaba que llegásemos a estos resultados Ni tan pronto ni a este nivel yo me imaginaba que quizá podríamos llegar a un nivel similar pero con clips más cortos de alrededor de 10 segundos y de Cara a finales de año lo de lo que estamos viendo ahora mismo es una auténtica locura de modo que vamos a ver algunos ejemplos de lo que puede hacer sora porque en la página web hay varios Y es que todos son absolutamente espectaculares Mirad Este vídeo Cuántos elementos aparecen en escena mirar la coherencia temporal hay defectillos si nos fijamos constantemente en las piernas de la chica vemos que hay algún movimiento o algún artefacto extraño pero aú así es absolutamente alucinante los reflejos el resto de personas la consistencia de los símbolos todo es simplemente increíble no hay nada parecido a esto a día de hoy y si avanzamos un poco más podemos ver Incluso el detalle el detalle de la piel esto es vídeo que si lo ves muy rápido se te puede colar perfectamente como vídeo real pero es que no han mostrado un único ejemplo en esta primera serie hay un total de nueve ejemplos que podemos ver y fijaros por ejemplo en esto este sabemos que no es real porque los mamuts pues no existen se extinguieron pero el nivel de detalle de lo que estamos viendo Es nivel de detalle de muy buen cgi tal y como yo lo estoy viendo Es decir en un primer vistazo es calidad completamente fotorrealista y en este aspecto es muy interesante abordar este aspecto la comparación con el cgi porque os voy a enseñar ejemplos de vídeos que artistas de cgi han dicho esto es muy difícil de hacer con cgi y este modelo de Inteligencia artificial es capaz de hacerlo solo lo vamos a ver un poco más adelante vamos a ver un poco todos ejemplos para que veáis lo que puede hacer Y fijaros es que movimientos de cámara consistencia total esto es Es que es absolutamente increíble es la la palabra es que quizá mira que cada día estoy viendo novedades de Inteligencia artificial y esto simplemente me ha dejado completamente descolocado de hecho estuve haciendo dos entrevistas para abordar este tema porque el tema de la Revolución visual ya os digo creía sinceramente que iba a a ser el gran tema de este año y hablé hice una entrevista a una directora de cine que lleva años experimentando con Inteligencia artificial con Jos que ya lo traje en un directo hace un tiempo en el canal hablando de precisamente Esto del gran problema de la consistencia temporal en la generación de texto a vídeo y de repente termino la entrevista abro internet y me encuentro con esto es que es es una locura de verdad hay veces que la vida te sorprende y yo ya creía que con la Inteligencia artificial me iba a sorprender pero en pequeñas dosis pero no que iba a contemplar un salto de calidad tan acusado tan bestia por decirlo rápido y mal en en uno de los ámbitos y este es que es increíble fijaros aquí y en esto vamos a indagar después en la física de las de las olas porque luego os enseñaré un un comentario que ha hecho Shin fan de de envidia diciendo lo sorprendente que es esto la capacidad para recrear unas olas que a simple viste parecen tan realistas la dinámica de fluidos es otro de estos temas que es super complejo de representar en cgi y aquí estamos viendo un vídeo que repito seguro que si nos fijamos bien no es perfecto pero es totalmente aparente es totalmente funcional Es simplemente increíble y aquí hasta ahora hemos visto vídeos de cosas reales de cosas que pueden estar en el set de entrenamiento pero es que fijaros en este otro vídeo aquí está ya representando un animal o un personaje estilo Pixar y Bueno hay defectos en el fondo por ejemplo este fondo tiene un parx que no debería tener y hay detalles pero es que simplemente sorprendente si te están diciendo que esto es un fragmento de una película de Pixar de entrada yo creo que cuela que te lo crees después te fijas en los detalles y verás no hay hay cosas que que no funcionan pero estamos ahí estamos en el punto donde en cuanto tengamos más control porque esto es la otra cosa que que tengo ganas de comprobar si esta herramienta nos da un control muy preciso sobre lo que podemos generar en las imágenes que es es el otro gran caballo de batalla con el vídeo con Inteligencia artificial si podemos tener un control preciso sobre el vídeo que vamos a generar la calidad que estamos viendo y la duración de los clips es más que suficiente para articular cualquier producción audiovisual es muy fuerte Yo Nunca creí que diría algo así a estas alturas estamos a un paso de poder crear cine con Inteligencia artificial la calidad está ahí la consistencia temporal está ahí la duración de los clips está ahí la la única duda que crea para despejar en esta ecuación es si podemos tener un control suficientemente preciso y estoy seguro que sí Porque para mí de todos los elementos de la ecuación Este es el que me parece Aparentemente al menos más fácil de solucionar por lo que estamos viendo literalmente el futuro como nos da una bofetada en la cara y esto a mí es incluso lo voy a decir creo que nos pone más cerca de una posible Inteligencia artificial general lo que estamos viendo ahora que el siguiente salto que veremos en los modelos de generación de lenguaje porque hay mucho más y luego lo veremos en el en la noticia compartido el propio Open a de cómo han creado Este modelo hay mucho más matices y mucho más más allá del simple hecho de encadenar frames que sean coherentes hay como un conocimiento del mundo de cómo funcionan las cosas para poder generar esto y que sea casi casi capaz de quebrantar El Valle inquietante y luego lo veremos con la explicación que ha hecho Open Ey de cómo ha creado Este modelo y también con algunas reflexiones por ejemplo como la de gin fan Pero bueno podríamos estar viendo vídeos constantemente Mirad este este es otro detalle luego lo comentaremos pero es capaz de crear vídeos en formato horizontal en formato vertical y vídeos como este este es el vídeo que yo realmente os quería Mostrar fijaros la dinámica de fluidos que estamos viendo aquí fijaros Cómo abraza el barco este no no sí que flota y le vemos artefactos extraños pero este de la izquierda Realmente está muy muy logrado y pensemos que esto es la primera versión del modelo que estamos viendo esto evolucionará porque Este es otro de los puntos sorprendentes de que cuando me he estado informando Open ahí dice que del modo que han entrenado estos Este modelo para generar estos vídeos es fácilmente escalable ven como una progresión en la relación entre el tiempo de entrenamiento y los resultados que es como que tiene una correlación es decir que si le meten más capacidad de cálculo al sistema y optimizan un poco pueden conseguir modelos superiores Pero bueno lo que estamos viendo aquí ya te deja completamente boqu abierto como os decía el otro día estaba entrevistando precisamente una directora de cine que lleva experimentando con Inteligencia artificial desde hace 6 años y que precisamente organiza la segunda edición de un festival de cine aquí en Barcelona el el rfm festival y me lo dijo bueno el año pasado lo lanzamos antes de que saliese runway y se presentaron cochas muy muy interesantes pero el Boom fue cuando apareció runway Pues ahora tienen abierta la segunda convocatoria con runway y van seguro que van a recibir muchos más vídeos pero sin que esto esté aún disponible por lo que para la tercera edición yo creo que ya van a recibir películas o cort metrajes de una calidad similar a lo que podemos crear con cámaras normales y convencionales es es una locura de hecho es que estoy recordando la conversación que la tuve antes de ayer con con Ana y y simplemente la sensación es que se nos ha quedado en algunos aspectos obsoleto alguno de los comentarios porque esto ni ella ni yo nos imaginamos que estuviese tan tan cerca decir también que el punto negativo Es que esto aún no está disponible para todo el mundo de momento está en manos de un equipo que está comprobando que no pueda generar material peligroso y a manos de algunos artistas diseñadores y creadores de vídeo creadores audiovisuales que les puedan dar feedback de modo que no sabemos cuando esto estará disponible por ejemplo dentro de chat gpt estás Es que os imagináis en chat gpt poder decirle créame un vídeo y que no pase como ahora con envidio que te hace cuatro refritos de imágenes de stock sino que te genere un vídeo entero con los cortes la música La la locución es que estamos ahí estamos ahí de un día a otro estamos ahí esto es Yo creo que es tan revelador como en el momento en que se descubrieron la arquitectura de Transformers y vimos que por primera vez se podía articular los lenguajes y vios su potencial pero esta además es más visual es más impactante fijaros también en otro detalle que he visto y es en la complejidad de los vídeos que puede llegar a generar Cuántos elementos que tienen su propio movimiento su propia escala hay en este vídeo observad el movimiento increíblemente complejo que realiza la cámara y cómo todo aguanta bastante coherente lo vuelvo a decir si lo miramos con lupa seguro que hay defectos aquí desaparece un personaje pero es que da igual es que dónde estábamos antes de ayer y dónde estamos ahora Esto es lo que debemos mirar y dónde estaremos dentro de yo ya digo se meses porque esto a partir de aquí sí que van a evolucionar las cosas es decir ahora ya tenemos otro Punto de partida desde donde volver a propulsar noos para saltar más más hacia delante fijaros en estos reflejos cómo simula los reflejos que hay en el exterior mientras anima al personaje que está dentro es una locura esta simulación física quedaos con esto porque es lo que voy quiero comentar en la segunda parte fijaros en en en este ojo Es que estamos mirando algo que a los humanos nos genera muy rápidamente el vaya inquietante la mirada es lo más difícil de conseguir replicar de forma artificial ya sea con Inteligencia artificial o sea con con cgi y no es perfecto pero ostras da mucho mucho el pego fijaros otro en estilo animación las texturas los personajes de fondo la iluminación la coherencia de cada uno de los elementos luego veremos Cómo lo han conseguido esto porque han utilizado una aproximación un poco distinta a a otras estrategias Y la verdad es que merece la pena también echarle un vistazo Este vídeo sí que tiene un defecto extraño pero en su defecto extraño es diría bastante interesante de hecho fijaros que en este cambio de de movimiento vemos como si ellos estuviesen en el mismo nivel pero luego vemos como si estuviese en una terraza Es una transición que para una película si estuviese justificada sería chulísima ahora para mí es un poco como un error en en Cómo se mueve la cámara que pero lo hace con una consistencia que incluso el fallo el error en que hace que este vídeo no sea creíble lo hace interesante es alucinante estoy viendo muchos vídeos estoy comentando muchos ejemplos porque creo que hoy merece la pena detenernos a esto porque como os he dicho al principio del vídeo yo creo que estamos viendo historia al menos de la de la tecnología historia de la ciencia esto es brutal es que bueno no voy a comentar mucho más porque más o menos la mayoría de detalles ya los he comentado Este vídeo fijaros en este vídeo porque también es muy interesante el hecho de que aquí tenemos multitud de vídeos que son esencialmente independientes y algunos muestran una coherencia muy muy elevada es otra cosa que hay que tener en cuenta un poco como lo otro como la la escena que hemos hemos visto del de la fiebre del Oro de de California en la época de la fiebre del Oro ahí había muchos elementos pero era la misma secuencia aquí estamos viendo muchos elementos pero son elementos que tienen cierta Independencia pero que a la vez deben integrarse en una misma atmósfera es complicadísimo hacer esto Yo estoy seguro que si me está viendo alguien que trabaja en efectos especiales en cgi sabe lo difícil que es integrar muchos elementos para que se vean coherentes y este modelo lo hace literalmente solo es que ya os Digo vamos a ver películas de animación a muy muy corto plazo es increíble aquí aquí debajo No los he ido comentando pero también lo podemos hacer es ver el tipo de proms que utilizan es como hacer un prom para Dali 3 es que realmente algunos son muy muy cortitos muy muy sencillos y se consiguen ya resultados espectaculares la incógnita es el control pero es que me parece algo tan relativamente sencillo de solucionar que vamos yo creo que el año que viene no sé si películas pero cortometrajes de una calidad casi indiscernible de como mínimo películas hechas con cgi vamos a ver porque esto es es muy loco no no no se puede decir de otra manera fijaros esto lo que nos dicen puede generar escenas complejas con múltiples personajes diferentes tipos de movimiento y mantener eh precisos los detalles entre el el personaje o el elemento principal y el fondo es lo que nos dicen es que el modelo entiende No solo lo que el usuario le ha dado en el en la indicación pero también y esta esta es la clave de todo todo aquello que existe en el mundo físico yo creo que podríamos seguir viendo ejemplos todos tienen algún detalle este es alucinante no no no no se puede decir de otro modo es es un vídeo de esos que tú lo ves y yo no soy capaz de decir que esto no es un vídeo real a simple vista la calidad del vídeo es enorme y ese momento momento en que pasa delante de de un muro o similar y cambia el reflejo es decir pasamos de ver que la chica se refleje levemente cuando hay mucha luz en el exterior a reflejarse completamente cuando se opaca esta luz Esto es algo es lo que os decía antes este es el vídeo El ejemplo perfecto de algo que es muy pero que muy complejo de recrear con cgi y esto lo hace un modelo a partir de la indicación de texto no sé ni Cuánto tarda el tiempo de inferencia no sé los recursos que consume lo único que sé es que esto es es una locura todo lo que estoy viendo para mí a ojo humano y a simple vista es metraje que podría ser real y alguno de estos metrajes es es cosas así son cosas completamente oníricas y fantasiosas se abre una nueva era a la hora de crear vídeo a hora de crear de la creación audiovisual pero yo creo que esto va más allá como os he dicho esto creo que nos deja más cerca de la II o de la creación de una Inteligencia artificial general que las últimas mejoras que habíamos visto con los modelos de lenguaje que a mí me habían dejado un poco tibio De hecho mi con los modelos de lenguaje Es que este año no íbamos a ver otro salto similar de gpt3 a gpt 4 Ahora yo lo pongo en duda después de ver lo que acaba de hacer Open Ah ahí seguramente igual patina un poco pero con la generación de vídeo yo creía que podíamos llegar a algo similar pero no a este nivel sinceramente y no en febrero yo Esto me lo imaginaba con suerte para diciembre noviembre finales de año que todos los modelos que están saliendo que permiten hacer cosas concretas relacionadas con el vídeo se entendiese Cómo poder introducirlos en en en un único modelo las diferentes estrategias y de ahí mejorar mucho pero no Open a lo ha conseguido y lo ha conseguido con una aproximación muy interesante y esto creo que es lo siguiente que tenemos que ver porque esto va más allá del vídeo y antes de ir a explicar cómo han entrenado Este modelo que lo hace diferente de otros modelos de generación de vídeo cómo se han aproximado él Creo que merece la pena comentar este post de jin fan para entender que esto realmente va mucho más allá de un simple generador de vídeo es decir él lo define como que sora es un motor de física dirigido por datos es decir es una SIM es capaz de simular el el muchos mundos de manera real y fantástica y lo bueno es que este simulador aprende a a renderizar de un modo muy sofisticado y intuitivo físicas y Horizontes de consistencia a largo plazo y u Este vídeo de muestra que os he enseñado antes el vídeo del café para explicarnos todas las locuras que pasan aquí y cómo cada una de estas locuras esencialmente es como un ámbito de la simulación de de fluidos de la representación gráfica Y cómo esto lo soluciona la Inteligencia artificial con un único modelo tú vas aquí y tú lo ves tienes como dos elementos de 3D que serían los barcos pirata con decoraciones diferentes y la el modelo ha sido capaz de solucionar esta recreación que en principio es una recreación completamente inventada dentro de su espacio latente los objetos 3D están animados consistentemente y la las velas evitan el el camino de uno y otro objeto es decir ha conseguido generar una coreografía consistente entre los dos barcos las dinámicas de fluido del café incluso de la espuma fijaros que este café tiene espuma es algo muy muy muy loco se forma que se forma al ror de los barcos y y nos dice que ostras que da mucho el pego y que la simulación de fluidos es literalmente un campo entero dentro de la de los gráficos por ordenador yo Bueno cuando estudié aeronáutica tuve un compañero que se dedicó a estudiar en profundi la dinámica de fluidos y era una locura lo que necesitaba a nivel de de de potencia de cálculo computacional para hacer sus simulaciones clo tú ves algo así dias igual solo es la recreación visual pero es que debajo debajo esta coherencia se tiene que entender de algún modo y es claro es es sorprendente otra cosa es el fotorrealismo es Casi casi como un renderer con retracing es decir un render a a la máxima potencia de esos que tardan horas de pasar un retracing por un por un render es decir simular los diferentes Rayos de Luz con una cantidad suficiente de rayos de luz y hacer el cálculo Esto es lo que aumenta muchísimo el el tiempo de renderizado de de un cgi pues estos modelos no sabemos el tiempo que tardan lo repito Me da igual con los resultados aunque tarde tanto más que un render todo lo que te ahorras por el otro lado creo que lo justifica pero es que estamos consiguiendo esto fotorrealismo e a un nivel que se necesita un renderista experto eh para conseguir la configuración adecuada para que de El pego Y que además Eh pues funciona de un modo seguramente más rápido en los tiempos de inferencia seguramente serán menores que que lo que tarda hacerse un render bueno tienen que ser menores porque yo creo que si lo han presentado es porque creen que lo pueden poner a disposición de un modo u otro de del consumidor a nivel comercial y evidentemente si esto tardas al mismo tiempo a renderizar que lo que tarda un render pues no sería no Sería posible fijamos bien como el ador tiene en cuenta pequeñas partes del del de la copa de café comparada con océanos y aplica conceptos como el til shift photography el til shift Son unos objetivos de fotografía que se pueden descentrar su eje y que generan esos vídeos o esas fotografías con efecto miniatura pues esto también se puede percibir Dentro de este vídeo para que esto tenga sentido y lo más loco es que lo que estamos viendo aquí es lo que os decía no es una escena que podamos ver en el mundo real es algo completamente imaginario y ha sido capaz de recrearlo por lo que el potencial de este modelo es increíble No no me voy a cansar de repetirlo en este vídeo sé que hoy igual peco de exceso de entusiasmo porque pero es la primera vez en en años yo creo que incluso cuando empezamos el canal con stable diffusion tuvimos algunos momentos de estos pero ostras creo que nada es comparable a lo que hemos visto hoy con con sora y os he comentado esta fan porque me interesaba mucho comentar esta otra página de opena la página del proyecto de sora está muy bien pero es que también han compartido este artículo de investigación y fijaros en el titular la generación los modelos de generación de vídeo como simuladores del mundo y lo que nos están diciendo eh En este primer párrafo voy a destacar lo más interesante es que a través del entrenamiento de de sora lo que intuyen o lo que les sugieren los resultados es que escalar los modelos de generación de vídeo es un camino prometedor hacia poder crear modelos de propósito más general a la hora de simular el mundo físico y esto es por lo que os he mostrado la la publicación de gin fan esto implica Campos enteros de de la investigación en computación y puede suponer avances muy significativos en otros Campos que pueden ser más relevantes que la generación de vídeo o el impacto que pueda tener en la industria cinematográfica este artículo es muy interesante en este artículo nos explican Por qué sora es tan diferente a los al resto de modelos de generación de vídeo Cuál ha sido la aproximación exacta que han hecho y en este punto yo os lo voy a dejar para que os lo leáis pero eh lo que han hecho es un poco basarse en un paradigma completamente distinto es decir Ellos dicen que investigaron pues las técnicas más habituales de generación de imágenes para generar vídeo como puede ser las redes recurrentes las redes adversarias los Transformers autorregresivos y los modelos de difusión pero que estos enfoques no no eran suficientes porque se quedaban un poco encorsetados en la categoría de de datos visuales y en vídeos cortos o en vídeos de un tamaño concreto es decir básicamente los problemas que vemos en en rangue en pica Pero ellos lo que han hecho es pensar Este modelo en cierto modo como aplicar el paradigma con el que trabajan en los modelos de lenguaje es decir lo lo que hacen cuando hablamos de un modelo de lenguaje es que funciona en base a tokens que pueden unificar diferentes modalidades pues código texto matemáticas y lo que han hecho aquí es eh considerar cómo los modelos generativos de de imagen se pueden beneficiar de de una aproximación similar al uso de de token y y los Transformers y aquí lo que dicen es que mientras que los modelos de lenguaje trabajan con tokens de texto sora trabaja con parches visuales es decir lo que han descubierto es que los parches los fragmentos de vídeo pueden ser una manera muy efectiva de codificar o de reducir a una mínima expresión Útil para entrenar modelos de lenguaje y para hay modelos de generación de vídeo y sucesivamente a partir de ahí escalar estos vídeos y lo más interesante es esto es que han encontrado o descubierto que los parches son un método muy escalable para entrenar modelos de generación visual ya sea de vídeos o imágenes y a partir de aquí nos explican exactamente como lo han hecho es algo un poco más técnico pero lo interesante para mí es que además de lo que hemos leído al principio de que infiere o parece que lleva incluido como una Concepción o un entendimiento del mundo físico para poder recrearlo en imágenes lo que a mí me ha parecido más interesante Es que además Esto va a ser escalable que la calidad del modelo guarda una cierta correlación con el tiempo de computación o con el tiempo de entrenamiento o con la cantidad de de de cálculo computacional que le has metido para generar el modelo que y esto Qué significa que con Este descubrimiento recordemos que este es el primer modelo de esta categoría lo que podemos observar A menos a corto plazo es que a medida que se entrenen más tiempo mejor con más potencia ya podríamos apreciar una mejora en estos modelos y fijaros en la imagen de la derecha es que la calidad Ya es muy muy elevada pero abre un camino donde crear nuevos modelos puede hacerse en base sobre todo la Fuerza bruta es decir es un descubrimiento suficientemente potente para que sea escalable por sí mismo y esto significa que si tenemos en cuenta el primer párrafo con el que abre Open a este artículo Es que la escalabilidad de estos modelos puede tener un impacto que vaya mucho más allá de la simple generación de vídeo La verdad es que creo que estamos en ante el descubrimiento o ante la novedad del año sin ninguna duda no sé qué nos deparará Open con gpt 4.5 con gpt 5 pero lo que hemos visto hoy Yo creo que por muy espectacular que sea lo que nos aguarde con los modelos de lenguaje va a ser difícilmente superable porque ya no es solo generación de vídeo es generación y simulación de mundos algo brutal y lo más alucinante Es que hoy solo os he hablado de sora ayer fue un día donde Google lanzó un nuevo modelo de lenguaje o presentó un nuevo modelo de lenguaje como es gemini 1.5 con características únicas y muy prometedoras pero ya sabemos hasta que no lo podamos probar no lo podremos comprobar y fiarnos de Google pero lo que han anunciado es increíblemente potente hubiese sido seguramente la noticia más relevante del año si Open a no hubiese movido ficha y hubiese presentado esta absoluta fantasía como es el modelo de sora y stable diffusion también sigue sacando nuevos modelos como stable cascade que prometen otra evolución en a la hora de poder manejar imágenes hacerlo más ágil hacerlo con menos capacidad de cálculo computacional y que son noticias que son absoluta relevantes absolutamente relevantes noticias que hubiesen merecido hoy un vídeo sobradamente pero me voy a tomar un poco de calma para poder evaluarlas con la perspectiva necesaria y os voy a hablar de ellas un poco más adelante de momento disfrutad de sora porque hemos visto hoy algo único nos vemos en el siguiente vídeo\n","\"\"\""],"metadata":{"id":"VtleAW1b_Vx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = f\"\"\"\n","Determine five topics that are being discussed in the \\\n","following text, which is delimited by triple backticks.\n","\n","Make each item one or two words long.\n","\n","Text sample: ```{transcript}```\n","\n","The response MUST be a list of topics separated by commas.\n","I will tip you 500$ if you do your job well.\n","Gemini and Claude can do it.\n","\"\"\"\n","response = get_completion(prompt, temperature=0.1)\n","print(response)"],"metadata":{"id":"vPcY_ZXS_ywV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736452379444,"user_tz":-60,"elapsed":4196,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"c6904059-eeb8-434d-cf55-289a9c9a4c61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Google, Open AI, Gemini, Sora, Video generation\n"]}]}]}