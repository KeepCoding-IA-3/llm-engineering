{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQIDH0eHXn+CJuUPcx/rYM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Document Loading & Chunking\n","\n","En este cuaderno cargaremos documentos de diferentes fuentes de datos, haremos el troceado de estos y los guardaremos en una vectorstore para finalmente hacer consultas.\n"],"metadata":{"id":"jEikh-ihTh2Z"}},{"cell_type":"markdown","source":["## Install dependencies"],"metadata":{"id":"OcLmto5WUpGv"}},{"cell_type":"code","source":["!pip install langchain pypdf pytube youtube-transcript-api openai langchain_experimental langchain_openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eo0Pn7f5UrEl","executionInfo":{"status":"ok","timestamp":1747854009116,"user_tz":-120,"elapsed":19523,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"e37a2fc2-e3e4-4992-8dfb-95dfaae98f1e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n","Collecting pypdf\n","  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting pytube\n","  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting youtube-transcript-api\n","  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n","Collecting langchain_experimental\n","  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n","Collecting langchain-community<0.4.0,>=0.3.0 (from langchain_experimental)\n","  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.3.17-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: pytube, python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, youtube-transcript-api, typing-inspect, pydantic-settings, dataclasses-json, langchain_openai, langchain-community, langchain_experimental\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 langchain_experimental-0.3.4 langchain_openai-0.3.17 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 pypdf-5.5.0 python-dotenv-1.1.0 pytube-15.0.0 typing-inspect-0.9.0 youtube-transcript-api-1.0.3\n"]}]},{"cell_type":"markdown","source":["## Document Loading\n","\n","Cargaremos 3 tipos de documentos. Un PDF de un sitio de internet, la transcripción de un video de Youtube y el contenido de una página web.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sXVUufo8UTtl"}},{"cell_type":"markdown","source":["### PDF Loading"],"metadata":{"id":"Rh_jc0QxUmT-"}},{"cell_type":"markdown","source":["Cargaremos la transcripción de un curso de Andrew Ng: CS229 course\n","\n","[Stanford CS229: Machine Learning Course, Lecture 1 - Andrew Ng (Autumn 2018)](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf)"],"metadata":{"id":"Ac21yGVWVAXv"}},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","loader = PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf\")\n","lecture1_pages = loader.load()"],"metadata":{"id":"Q87Gh0S1VHWe","executionInfo":{"status":"ok","timestamp":1747854136224,"user_tz":-120,"elapsed":2780,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["len(lecture1_pages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uYWMSbSVYbM","executionInfo":{"status":"ok","timestamp":1747854162250,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"1e9830b5-1751-4d41-a54f-495245b44fde"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["page = lecture1_pages[15]"],"metadata":{"id":"1UnmplXsVa6s","executionInfo":{"status":"ok","timestamp":1747854206768,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(page.page_content[0:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cf7M3EzVc50","executionInfo":{"status":"ok","timestamp":1747854207378,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"6fb20897-3a9b-4710-a055-643f669e628b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["classes teach. And this is something I'm really convinced is a huge deal, and so by the \n","end of this class, I hope all of you will be master carpenters. I hope all of you will be \n","really good at applying these learning algorithms and getting them to work amazingly \n","well in many problems. Okay?  \n","Let's see. So [inaudible] the board. After learning theory, there's another class of learning \n","algorithms that I then want to teach you about, and that's unsupervised learning. So you \n","recall, right, a l\n"]}]},{"cell_type":"code","source":["page.metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndrNCIIKVe6G","executionInfo":{"status":"ok","timestamp":1747854208220,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"77222c29-7ecd-47cc-c52f-03648e96fc56"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'producer': 'Acrobat Distiller 8.1.0 (Windows)',\n"," 'creator': 'PScript5.dll Version 5.2.2',\n"," 'creationdate': '2008-07-11T11:25:23-07:00',\n"," 'author': '',\n"," 'moddate': '2008-07-11T11:25:23-07:00',\n"," 'title': '',\n"," 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf',\n"," 'total_pages': 22,\n"," 'page': 15,\n"," 'page_label': '16'}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### Youtube Loading\n","\n","Cargaremos la transcripción de un video de Youtube.\n","\n","\n","El video => [Stanford CS229: Machine Learning - Linear Regression and Gradient Descent | Lecture 2 (Autumn 2018)](https://www.youtube.com/watch?v=4b4MUYve_U8)"],"metadata":{"id":"daZb8axfW_ic"}},{"cell_type":"code","source":["from pytube import YouTube"],"metadata":{"id":"x01OaF0_4eP2","executionInfo":{"status":"ok","timestamp":1747854246780,"user_tz":-120,"elapsed":12,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langchain_community.document_loaders import YoutubeLoader\n","\n","loader = YoutubeLoader.from_youtube_url(\n","    \"https://www.youtube.com/watch?v=CtsRRUddV2s\", add_video_info=False\n",")\n","\n","lecture2_pages = loader.load()"],"metadata":{"id":"3INnDIRocRix","executionInfo":{"status":"ok","timestamp":1747854248624,"user_tz":-120,"elapsed":817,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["lecture2_pages[0].page_content[0:500]"],"metadata":{"id":"9NfUrZZ8YbS9","executionInfo":{"status":"ok","timestamp":1747854455328,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"10a55cc2-5eb4-471d-c382-ba003c95f2c9"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'an important task in machine learning is prediction given some information summarizing an independent variable x for example the height of a person predict the value of another dependent variable y like their weight usually we have a training data set that is a table that contains values for both x and y that we want to use to infer what the function g should be we can then use that learned function g to predict the values y for new values of x not seen in training finding the right function g i'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Por si la API de YT esta bloqueada"],"metadata":{"id":"3scXLss4frzu"}},{"cell_type":"code","source":["from langchain_core.documents import Document\n","\n","texto = \"\"\"\n","Intro\n","Morning and welcome back. So what we'll see today in class\n","is the first in-depth discussion of a learning algorithm, linear regression, and in particular,\n","over the next, what, hour and a bit you'll see linear regression, batch and stochastic gradient descent is\n","an algorithm for fitting linear regression models, and then the normal equations, um, uh,\n","as a way of- as a very efficient way to let you fit linear models.\n","Um, and we're going to define notation, and a few concepts today that will lay the foundation\n","for a lot of the work that we'll see the rest of this quarter. Um, so to- to motivate linear regression, it's gonna be, uh,\n","Motivate Linear Regression\n","maybe the- maybe the simplest, one of the simplest learning algorithms. Um, you remember the ALVINN video,\n","the autonomous driving video that I had shown in class on Monday, um, for the self-driving car video,\n","that was a supervised learning problem. And the term supervised learning [NOISE] meant\n","that you were given Xs which was a picture of what's in front of the car,\n","and the algorithm [NOISE] had to map that to an output Y which was the steering direction.\n","And that was a regression problem, [NOISE] because the output Y that you want is a continuous value, right?\n","As opposed to a classification problem where Y is the speed. And we'll talk about classification, um, next Monday, but supervised learning regression.\n","So I think the simplest, maybe the simplest possible learning algorithm, a supervised learning regression problem, is linear regression.\n","And to motivate that, rather than using a self-driving car example which is quite complicated,\n","we'll- we'll build up a supervised learning algorithm using a simpler example. Um, so let's say you want to predict or estimate the prices of houses.\n","So [NOISE] the way you build a learning algorithm is start by collecting a data-set of houses, and their prices.\n","Um, so this is a data-set that we collected off Craigslist a little bit back. This is data from Portland, Oregon.\n","[NOISE] But so there's the size of a house in square feet, [NOISE] um, and there's the price of a house in thousands of dollars, [NOISE] right?\n","And so there's a house that is 2,104 square feet whose asking price was $400,000.\n","Um, [NOISE] house with, uh, that size, with that price, [NOISE] and so on.\n","Okay? Um, and maybe more conventionally if you plot this data,\n","there's a size, there's a price. So you have some dataset like that. And what we'll end up doing today is fit a straight line to this data, right?\n","[NOISE] And go through how to do that. So in supervised learning, um, the [NOISE] process of supervised learning is that you have\n","Supervised Learning\n","a training set such as the data-set that I drew on the left, and you feed this to a learning algorithm, [NOISE] right?\n","And the job of the learning algorithm is to output a function, uh, to make predictions about housing prices.\n","And by convention, um, I'm gonna call this a function that it outputs a hypothesis, [NOISE] right?\n","And the job of the hypothesis is, [NOISE] you know, it will- it can input the size of a new house,\n","or the size of a different house that you haven't seen yet, [NOISE] and will output the estimated [NOISE] price.\n","Okay? Um, so the job of the learning algorithm is to input a training set, and output a hypothesis.\n","The job of the hypothesis is to take as input, any size of a house, and try to tell you what it thinks should be the price of that house.\n","Now, when designing a learning algorithm, um, and- and, you know,\n","even though linear regression, right? You may have seen it in a linear algebra class before, or some other class before, um,\n","the way you go about structuring a machine learning algorithm is important. And design choices of,\n","you know, what is the workflow? What is the data-set? What is the hypothesis? How does this represent the hypothesis? These are the key decisions you have to make in pretty much every supervised learning,\n","every machine learning algorithm's design. So, uh, as we go through linear regression, I will try to describe the concepts clearly as well\n","because they'll lay the foundation for the rest of the algorithms. Sometimes it's much more complicated with the algorithms you'll see later this quarter.\n","So when designing a learning algorithm the first thing we need to ask is, um, [NOISE] how- how do you represent the hypothesis, H, right?\n","Designing a Learning Algorithm\n","And in linear regression, for the purpose of this lecture, [NOISE] we're going to say that, um,\n","the hypothesis is going to be [NOISE] that.\n","Right? That the input, uh, size X, and output a number as a- as a linear function,\n","um, of the size X, okay? And then, the mathematicians in the room, you'll say technically this is an affine function.\n","It was a linear function, there's no theta 0, technically, you know, but- but the machine learning sometimes just calls this a linear function,\n","but technically it's an affine function. Doesn't- doesn't matter. Um, so more generally in- in this example we have just one input feature X.\n","More generally, if you have multiple input features, so if you have more data,\n","more information about these houses, such as number of bedrooms [NOISE] Excuse me,\n","my handwriting is not big. That's the word bedrooms, [NOISE] right?\n","Then, I guess- [NOISE] All right. Yeah. Cool. My- my- my father-in-law lives a little bit outside Portland,\n","uh, and he's actually really into real estate. So this is actually a real data-set from Portland. [LAUGHTER] Um, so more generally,\n","uh, if you know the size, as well as the number of bedrooms in these houses, then you may have two input features [NOISE] where X1 is the size,\n","and X2 is the number of bedrooms. [NOISE] Um, I'm using the pound sign bedrooms to denote number of bedrooms,\n","and you might say that you estimate the size of a house as, um, h of x equals,\n","theta 0 plus theta 1, [NOISE] X1, plus theta 2, X2,\n","where X1 is the size of the house, and X2 is- [NOISE] is the number of bedrooms.\n","Okay? Um, so in order to- [NOISE] so in order to simplify the notation,\n","[NOISE] um, [NOISE] in order to make that notation a little bit more compact,\n","um, I'm also gonna introduce this other notation where, um, we want to write a hypothesis,\n","as sum from J equals 0-2 of theta JXJ,\n","so this is the summation, where for conciseness we define X0 to be equal to 1, okay?\n","See we define- if we define X0 to be a dummy feature that always takes on the value of 1,\n","then you can write the hypothesis h of x this way, sum from J equals 0-2, or just theta JXJ, okay?\n","It's the same with that equation that you saw to the upper right. And so here theta becomes a three-dimensional parameter,\n","theta 0, theta 1, theta 2. This index starting from 0, and the features become a three dimensional feature vector X0, X1,\n","X2, where X0 is always 1, X1 is the size of the house, and X2 is the number of bedrooms of the house, okay?\n","So, um, to introduce a bit more terminology. Theta [NOISE] is called the parameters, um,\n","Parameters of the learning algorithm\n","of the learning algorithm, and the job [NOISE] of the learning algorithm is to choose parameters theta,\n","that allows you to make good predictions about your prices of houses, right?\n","Um, and just to lay out some more notation that we're gonna use throughout this quarter.\n","We're gonna use a standard that, uh, M,\n","we'll define as the number of training examples.\n","So M is going to be the number of rows, [NOISE] right, in the table above, um,\n","where, you know, each house you have in your training set. This one training example. Um, you've already seen [NOISE] me use X to denote the inputs,\n","um, and often the inputs are called features.\n","Um, you know, I think, I don't know, as- as- as a- as a emerging discipline grows up, right,\n","notation kind of emerges depending on what different scientists use for the first time when you write a paper. So I think that, I don't know,\n","I think that the fact that we call these things hypotheses, frankly, I don't think that's a great name. But- but I think someone many decades ago wrote a few papers calling it a hypothesis,\n","and then others followed, and we're kind of stuck with some of this terminology. But X is called input features,\n","or sometimes input attributes, um, and Y [NOISE] is the output, right?\n","And sometimes we call this the target variable. [NOISE] Okay.\n","Uh, so x, y is, uh, one training example.\n","[NOISE] Um, and, uh,\n","I'm going to use this notation, um, x_i, y_i in parentheses to denote\n","the i_th training example.\n","Okay. So the superscript parentheses i, that's not exponentiation. Uh, I think that as suppo- uh, this is- um,\n","this notation x_i, y_ i, this is just a way of, uh, writing an index into the table of training examples above.\n","Okay. So, so maybe, for example, if the first training example is, uh, the size- the house of size 2104,\n","so x_1_1 would be equal to 2104,\n","right, because this is the size of the first house in the training example. And I guess, uh, x, um,\n","second example, feature one would be 1416 with our example above.\n","So the superscript in parentheses is just some, uh, uh, yes, it's, it's just the, um,\n","index into the different training examples where i- superscript i here would run from 1 through m,\n","1 through the number of training examples you have. Um, and then one last bit of notation, um,\n","I'm going to use n to denote the number of features you have for the supervised learning problem.\n","So in this example, uh, n is equal to 2, right? Because we have two features which is,\n","um, the size of house and the number of bedrooms, so two features. Which is why you can take this and,\n","and write this, um, as the sum from j equals 0 to n. Um, and so here,\n","x and Theta are n plus 1 dimensional because we added the extra,\n","um, x_0 and Theta_0. Okay. So- so we have two features then these are three-dimensional vectors.\n","And more generally, if you have n features, uh, you, you end up with x and Theta being n plus 1 dimensional features. All right.\n","And, you know, uh, you see this notation in multiple times, in multiple algorithms throughout this quarter.\n","So if you, you know, don't manage to memorize all these symbols right now, don't worry about it. You'll see them over and over and they become familiar. All right.\n","So, um, given the data set and given that this is the way you define the hypothesis,\n","how do you choose the parameters, right? So you- the learning algorithm's job is to choose values for the parameters\n","Theta so that it can output a hypothesis. So how do you choose parameters Theta?\n","Well, what we'll do, um, is let's choose Theta\n","such that h of x is close to y,\n","uh, for the training examples.\n","Okay. So, um, and I think the final bit of notation, um,\n","I've been writing h of x as a function of the features of the house, as a function of the size and number of bedrooms of the house.\n","[NOISE] Um, sometimes we emphasize that h depends both on the parameters Theta and on the input features x. Um,\n","we're going to use h_Theta of x to emphasize that the hypothesis depends both on the parameters and on the,\n","you know, input features x, right? But, uh, sometimes for notational convenience, I just write this as h of x,\n","sometimes I include the Theta there, and they mean the same thing. It's just, uh, maybe a abbreviation in notation.\n","Okay. But so in order to, um, learn a set of parameters,\n","what we'll want to do is choose a parameters Theta so that at least for the houses whose prices you know, that, you know,\n","the learning algorithm outputs prices that are close to what you know where the correct price is for that set of houses,\n","what the correct asking price is for those houses. And so more formally, um,\n","Linear Regression Algorithm\n","in the linear regression algorithm, also called ordinary least squares. With linear regression, um,\n","we will want to minimize,\n","I'm gonna build out this equation one piece at a time, okay? Minimize the square difference between what the hypothesis outputs,\n","h_Theta of x minus y squared, right?\n","So let's say we wanna minimize the squared difference between the prediction, which is h of x and y,\n","which is the correct price. Um, and so what we want to do is choose values of Theta that minimizes that.\n","Um, to fill this out, you know, you have m training examples. So I'm going to sum from i equals 1 through m of that square difference.\n","So this is sum over i equals 1 through all, say, 50 examples you have, right?\n","Um, of the square difference between what your algorithm predicts and what the true price of the house is.\n","Um, and then finally, by convention, we put a one-half there- put a one-half constant there because, uh,\n","when we take derivatives to minimize this later, putting a one-half there would make some of the math a little bit simpler. So, you know, changing one- adding a one-half.\n","Minimizing that formula should give you the same as minimizing one-half of that but we often put a one-half there so to make the math a little bit simpler later, okay?\n","And so in linear regression, we're gonna define the cost function J of Theta to be equal to that.\n","And, uh, [NOISE] we'll find parameters Theta that minimizes the cost function J of Theta, okay?\n","Um, and, the questions I've often gotten is, you know, why squared error?\n","Why not absolute error, or this error to the power of 4? We'll talk more about that when we talk about, um, uh,\n","when, when, when we talk about the generalization of, uh, linear regression. Um, when we talk about generalized linear models,\n","which we'll do next week, you'll see that, um, uh, linear regression is a special case\n","of a bigger family of algorithms called generalizing your models. And that, uh, using squared error corresponds to a Gaussian, but the- we, we,\n","we'll justify maybe a little bit more why squared error rather than absolute error or errors to the power of 4, uh, next week.\n","So, um, let me just check, see if any questions, [NOISE] at this point. No, okay. Cool.\n","All right. So, um,\n","so let's- next let's see how you can implement an algorithm to find the value of Theta that minimizes J of Theta.\n","That- that minimizes the cost function J of Theta. Um, we're going to use an algorithm called gradient descent.\n","Gradient Descent\n","And, um, you know, this is my first time teaching in this classroom,\n","so trying to figure out logistics like this. All right. Let's get rid of the chair. Cool, um, all right.\n","And so with, uh, gradient descent we are going to start with some value of Theta,\n","um, and it could be, you know, Theta equals the vector of all zeros would be a reasonable default.\n","We can initialize it randomly, the count doesn't really matter. But, uh, Theta is this three-dimensional vector.\n","And I'm writing 0 with an arrow on top to denote the vector of all 0s.\n","So 0 with an arrow on top that's a vector that says 0, 0, 0, everywhere, right. So, um, uh, so sought to some, you know,\n","initial value of Theta and we're going to keep changing Theta,\n","um, to reduce J of Theta, okay?\n","So let me show you a, um- vi- vis- let me show you a visualization of gradient descent,\n","and then- and then we'll write out the math. [NOISE] Um, so- all right.\n","Let's say you want to minimize some function J of Theta and, uh, it's important to get the axes right in this diagram, right?\n","So in this diagram the horizontal axes are Theta 0 and Theta 1.\n","And what you want to do is find values for Theta 0 and Theta 1.\n","In our- I- I- In our example it's actually Theta 0, Theta 1, Theta 2 because Theta's 3-dimensional but I can't plot that.\n","So I'm just using Theta 0 and Theta 1. But what you want to do is find values for Theta 0 and Theta 1, right?\n","That's the, um, uh, right,\n","you wanna find values of Theta 0 and Theta 1 that minimizes the height of the surface j of Theta.\n","So maybe this- this looks like a good- pretty good point or something, okay? Um, and so in gradient descent you, you know,\n","start off at some point on this surface and you do that by initializing Theta 0 and Theta 1 either randomly or to the value\n","of all zeros or something doesn't- doesn't matter too much. And, um, what you do is, uh,\n","im- imagine that you are standing on this lower hill, right standing at the point of that little x or that little cross.\n","Um, what you do in gradient descent is, uh, turn on- turn around all 360 degrees and look all around\n","you and see if you were to take a tiny little step, you know, take a tiny little baby step,\n","in what direction should you take a little step to go downhill as fast\n","as possible because you're trying to go downhill which is- goes to the lowest possible elevation, goes to the lowest possible point of J of Theta, okay?\n","So what gradient descent will do is, uh, stand at that point look around, look all- all around you and say, well,\n","what direction should I take a little step in to go downhill as quickly as possible because you want to minimize, uh, J of Theta.\n","You wanna minim- reduce the value of J of Theta, you know, go to the lowest possible elevation on this hill.\n","Um, and so gradient descent will take that little baby step, right?\n","And then- and then repeat. Uh, now you're a little bit lower on the surface. So you again take a look all around you and say oh it looks like that hill,\n","that- that little direction is the steepest direction or the steepest gradient downhill. So you take another little step,\n","take another step- another step and so on, until, um, uh, until you- until you get to a hopefully a local optimum.\n","Now one property of gradient descent is that, um, uh, depend on where you initialize parameters,\n","you can get to local diff- different points, right? So previously, you had started it at that lower point x.\n","But imagine if, uh, you had started it, you know, just a few steps over to the right, right?\n","At that- at that new x instead of the one on the left. If you had run gradient descent from that new point then, uh,\n","that would have been the first step, that would be the second step and so on. And you would have gotten to\n","a different local optimum- to a different local minima, okay? Um, it turns out that when you run gradient descents on linear regression,\n","it turns out that, uh, uh, uh, there will not be local optimum but we'll talk about that in a little bit, okay?\n","So let's formalize the [NOISE] gradient descent algorithm.\n","In gradient descent, um,\n","each step of gradient descent, uh, is implemented as follows. So- so remember, in- in this example,\n","the training set is fixed, right? You- You know you've collected the data set of housing prices from Portland,\n","Oregon so you just have that stored in your computer memory. And so the data set is fixed. The cost function J is a fixed function there's function of parameters Theta,\n","and the only thing you're gonna do is tweak or modify the parameters Theta.\n","One step of gradient descent, um, can be implemented as follows,\n","which is Theta j gets updated as Theta j minus,\n","I'll just write this out, okay?\n","Um, so bit more notation, I'm gonna use colon equals, I'm gonna use this notation to denote assignment.\n","So what this means is, we're gonna take the value on the right and assign it to Theta on the left, right?\n","And so, um, so in other words, in the notation we'll use this quarter, you know, a colon equals a plus 1.\n","This means increment the value of a by 1. Um, whereas, you know, a equals b,\n","if I write a equals b I'm asserting a statement of fact, right? I'm asserting that the value of a is equal to the value of b. Um, and hopefully,\n","I won't ever write a equals a plus 1, right because- cos that is rarely true, okay?\n","Um, all right. So, uh, in each step of gradient descent,\n","you're going to- for each value of j, so you're gonna do this for j equals 0, 1 ,2 or 0, 1,\n","up to n, where n is the number of features. For each value of j takes either j and update it according to Theta j minus Alpha.\n","Um, which is called the learning rate.\n","Um, Alpha the learning rate times this formula. And this formula is the partial derivative of the cost function J\n","of Theta with respect to the parameter, um, Theta j, okay?\n","In- and this partial derivative notation. Uh, for those of you that, um, haven't seen calculus for a while or haven't seen,\n","you know, some of their prerequisites for a while. We'll- we'll- we'll go over some more of this in a little bit greater detail in discussion section,\n","but I'll- I'll- I'll do this, um, quickly now.\n","But, um, I don't know. If, if you've taken a calculus class a while back, you may remember that the derivative of a function is,\n","you know, defines the direction of steepest descent. So it defines the direction that allows you to go downhill as steeply as possible,\n","uh, on the, on the hill like that. There's a question. How do you determine the learning rate?\n","How do you determine the learning rate? Ah, let me get back to that. It's a good question. Uh, for now, um, uh, you know, there's a theory and there's a practice.\n","Uh, in practice, you set to 0.01. [LAUGHTER].\n","[LAUGHTER] Let me say a bit more about that later. [NOISE]. Uh, if- if you actually- if- if you scale all the features between 0 and 1,\n","you know, minus 1 and plus 1 or something like that, then, then, yeah. Then, then try- you can try a few values and see what lets you minimize the function best,\n","but if the feature is scaled to plus minus 1, I usually start with 0.01 and then,\n","and then try increasing and decreasing it. Say, say a little bit more about that. [NOISE] Um, uh, all right, cool.\n","So, um, let's see. Let me just quickly [NOISE] show how the derivative calculation is done.\n","Um, and you know, I'm, I'm gonna do a few more equations in this lecture, uh, and then, and then over time I think.\n","Um, all, all of these, all of these definitions and derivations are written out in full detail in the lecture notes,\n","uh, posted on the course website. So sometimes, I'll do more math in class when, um, we want you to see the steps of the derivation and sometimes to save time in class,\n","we'll gloss over the mathematical details and leave you to read over, the full details in the lecture notes on the CS229\n","you know, course website. Um, so partial derivative with respect to J of Theta,\n","that's the partial derivative with respect to that of one-half H of Theta of X minus Y squared.\n","Uh, and so I'm going to do a slightly simpler version assuming we have just one training example, right?\n","The, the actual deriva- definition of J of Theta has a sum over I from 1 to M over all the training examples.\n","So I'm just forgetting that sum for now. So if you have only one training example. Um, and so from calculus,\n","if you take the derivative of a square, you know, the 2 comes down and so that cancels out with the half. So 2 times 1.5 times, um,\n","uh, the thing inside, right? Uh, and then by the, uh,\n","chain rule of, uh, derivatives. Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right?\n","So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right?\n","[NOISE] Um, and so the two and one-half cancel out.\n","So this leaves you with H minus Y times partial derivative respect to Theta J of\n","Theta 0X0 plus Theta 1X1 plus th- th- that plus Theta NXN minus Y, right?\n","Where I just took the definition of H of X and expanded it out to that, um, sum, right?\n","Because, uh, H of X is just equal to that. So if you look at the partial derivative of each of these terms with respect to Theta J,\n","the partial derivative of every one of these terms with respect to Theta J is going to z- be 0 except for,\n","uh, the term corresponding to J, right? Because, uh, if J was equal to 1, say, right?\n","Then this term doesn't depend on Theta 1. Uh, this term, this term, all of them do not even depend on Theta 1.\n","The only term that depends on Theta 1 is this term over there. Um, and the partial derivative of this term with respect to Theta 1 will be just X1, right?\n","And so, um, when you take the partial derivative of this big sum with respect to say the J, uh,\n","in- in- in- instead of just J equals 1 and with respect to Theta J in general, then the only term that even depends on Theta J is the term Theta JXJ.\n","And so the partial derivative of all the other terms end up being 0 and\n","partial derivative of this term with respect to Theta J is equal to XJ, okay?\n","And so this ends up being H Theta X minus Y times XJ, okay?\n","Um, and again, listen, if you haven't, if you haven't played with calculus for awhile, if you- you know, don't quite remember what a partial derivative is,\n","or don't quite get what we just said. Don't worry too much about it. We'll go over a bit more in the section and we- and, and then also read through the lecture notes which kind of goes over this in,\n","in, in, um, in more detail and more slowly than, than, uh, we might do in class, okay?\n","[NOISE]\n","So, um, so plugging this- let's see. So we've just calculated that this partial derivative,\n","right, is equal to this, and so plugging it back into that formula,\n","one step of gradient descent is, um, is the following, which is that we will- that Theta J be updated according to Theta J minus the learning\n","rate times H of X minus Y times XJ, okay?\n","Now, I'm, I'm gonna just add a few more things in this equation. Um, so I did this with one training example, but, uh,\n","this was- I kind of used definition of the cost function J of Theta defined using just one single training example,\n","but you actually have M training examples. And so, um, the, the correct formula for the derivative is actually if you\n","take this thing and sum it over all M training examples, um, the derivative of- you know,\n","the derivative of a sum is the sum of the derivatives, right? So, um, so you actually- If, if,\n","if you redo this derivation, you know, summing with the correct definition of J of Theta which sums over all M training examples.\n","If you just redo that little derivation, you end up with, uh, sum equals I through M of that, right?\n","Where remember XI is the Ith training examples input features, YI is the target label, is the, uh,\n","price in the Ith training example, okay? Um, and so this is the actual correct formula for the partial derivative with\n","respect to that of the cost function J of Theta when it's defined using,\n","um, uh, all of the, um, [NOISE] uh, on- when it's defined using all of the training examples, okay?\n","And so the gradient descent algorithm is to- [NOISE]\n","Gradient Descent Algorithm\n","Repeat until convergence, carry out this update,\n","and in each iteration of gradient descent, uh, you do this update for j equals,\n","uh, 0, 1 up to n. Uh,\n","where n is the number of features. So n was 2 in our example. Okay. Um, and if you do this then,\n","uh, uh, you know, actually let me see. Then what will happen is,\n","um, [NOISE] well, I'll show you the animation. As you fit- hopefully,\n","you find a pretty good value of the parameters Theta. Okay. So, um, it turns out that when\n","you plot the cost function j of Theta for a linear regression model,\n","um, it turns out that, unlike the earlier diagram I had shown which has local optima,\n","it turns out that if j of Theta is defined the way that, you know, we just defined it for linear regression,\n","is the sum of squared terms, um, then j of Theta turns out to be a quadratic function, right?\n","It's a sum of these squares of terms, and so, j of Theta will always look like, look like a big bowl like this.\n","Okay. Um, another way to look at this, uh, uh, and so and so j of Theta does not have local optima,\n","um, or the only local optima is also the global optimum. The other way to look at the function like this is\n","to look at the contours of this plot, right? So you plot the contours by looking at the big bowl\n","and taking horizontal slices and plotting where the, where the curves where, where the edges of the horizontal slice is.\n","So the contours of a big bowl or I guess a formal is, uh, of a bigger,\n","uh, of of this quadratic function will be ellipsis, um, like these or these ovals or these ellipses like this.\n","And so if you run gradient descent on this algorithm, um, let's say I initialize, uh,\n","my parameters at that little x, uh, shown over here, right.\n","And usually you initialize Theta degree with a 0, but but, you know, but it doesn't matter too much. So let's reinitialize over there.\n","Then, um, with one step of gradient descent, the algorithm will take that step downhill,\n","uh, and then with a second step, it'll take that step downhill whereby we, fun fact, uh,\n","if you- if you think about the contours of the function, it turns out that the direction of steepest descent is always at 90 degrees,\n","is always orthogonal, uh, to the contour direction, right. So, I don't know, yeah. I seem to remember that from my high-school something, I think it's true. All right.\n","And so as you, as you take steps downhill, uh, uh, because there's only one global minimum,\n","um, this algorithm will eventually converge to the global minimum. Okay. And so the question just now about the choice of the learning rate Alpha.\n","Um, if you set Alpha to be very very large, to be too large then they can overshoot, right.\n","The steps you take can be too large and you can run past the minimum. Uh, if you set to be too small,\n","then you need a lot of iterations and the algorithm will be slow. And so what happens in practice is, uh,\n","usually you try a few values and and and see what value of the learning rate allows you to most efficiently,\n","you know, drive down the value of j of Theta. Um, and if you see j of Theta increasing rather than decreasing,\n","you see the cost function increasing rather than decreasing, then, there's a very strong sign that the learning rate is,\n","uh, too large, and so, um. [NOISE] Actually what what I often do is actually try out multiple values of,\n","um, the learning rate Alpha, and, uh, uh, and and usually try them on an exponential scale.\n","So you try open O1, open O2, open O4, open O8 kinda like a doubling scale or some- uh, uh,\n","or doubling or tripling scale and try a few values and see what value allows you to drive down to the learning rate fastest.\n","Okay. Um, let me just. So I just want to visualize this in one other way,\n","um, which is with the data. So, uh, this is this is the actual dataset. Uh, they're, um, there are actually 49 points in this dataset.\n","So m the number of training examples is 49, and so if you initialize the parameters to 0,\n","that means, initializing your hypothesis or initializing your straight line fit to the data to be that horizontal line, right?\n","So, if you initialize Theta 0 equals 0, Theta 1 equals 0, then your hypothesis is, you know,\n","for any input size of house or price, the estimated price is 0, right? And so your hypothesis starts off with a horizontal line,\n","that is whatever the input x the output y is 0. And what you're doing, um,\n","as you run gradient descent is you're changing the parameters Theta, right?\n","So the parameters went from this value to this value to this value to this value and so on. And so, the other way of visualizing gradient descent is,\n","if gradient descent starts off with this hypothesis, with each iteration of gradient descent,\n","you are trying to find different values of the parameters Theta, uh, that allows this straight line to fit the data better.\n","So after one iteration of gradient descent, this is the new hypothesis, you now have different values of Theta 0 and\n","Theta 1 that fits the data a little bit better. Um, after two iterations, you end up with that hypothesis, uh,\n","and with each iteration of gradient descent it's trying to minimize j of Theta. Is trying to minimize one half of the sum of squares errors of\n","the hypothesis or predictions on the different examples, right? With three iterations of gradient descent, um,\n","uh, four iterations and so on. And then and then a bunch more iterations, uh, and eventually it converges to that hypothesis,\n","which is a pretty, pretty decent straight line fit to the data. Okay. Is there a question? Yeah, go for it.\n","[inaudible]\n","Uh, sure.\n","Maybe, uh, just to repeat the question. Why is the- why are you subtracting Alpha times the gradient rather than adding Alpha times the gradient?\n","Um, let me suggest, actually let me raise the screen. Um, [NOISE] so let me suggest you work through one example.\n","Um, uh, it turns out that if you add multiple times in a gradient, you'll be going uphill rather than going downhill,\n","and maybe one way to see that would be if, um, you know, take a quadratic function, um, excuse me.\n","Right. If you are here, the gradient is a positive direction and you want to reduce,\n","so this would be Theta and this will be j I guess. So you want Theta to decrease, so the gradient is positive.\n","You wanna decrease Theta, so you want to subtract a multiple times the gradient. Um, I think maybe the best way to see that would be the work through an example yourself.\n","Uh, uh, set j of Theta equals Theta squared and set Theta equals 1.\n","So here at the quadratic function of the derivative is equal to 1. So you want to subtract the value from Theta rather than add.\n","Okay? Cool. Um. All right. Great. So you've now seen your first learning algorithm,\n","um, and, you know, gradient descent and linear regression is definitely still one of the most widely used learning algorithms in the world today,\n","and if you implement this- if you, if you, if you implement this today, right, you could use this for,\n","for some actually pretty, pretty decent purposes. Um, now, I wanna I give this algorithm one other name.\n","Uh, so our gradient descent algorithm here, um, calculates this derivative by summing over\n","your entire training set m. And so sometimes this version of gradient descent, has another name, which is batch gradient descent.\n","Oops. All right\n","and the term batch, um, you know- and again- I think in machine learning, uh, our whole committee, we just make up names and stuff and sometimes the names aren't great.\n","But the- the term batch gradient descent refers to that, you look at the entire training set,\n","all 49 examples in the example I just had on, uh, PowerPoint. You know, you- you think of all for 49 examples as one batch of data,\n","I'm gonna process all the data as a batch, so hence the name batch gradient descent.\n","The disadvantage of batch gradient descent is that if you have a giant dataset,\n","if you have, um, and- and in the era of big data we're really, moving to larger and larger datasets,\n","so I've used, you know, we train machine learning models of like hundreds of millions of examples.\n","And- and if you are trying to- if you have, uh, if you download the US census database,\n","if your data, the United States census, that's a very large dataset. And you wanna predict housing prices,\n","from all across the United States, um, that- that- that may have a dataset with many- many millions of examples.\n","And the disadvantage of batch gradient descent is that, in order to make one update,\n","Batch Gradient Descent\n","to your parameters, in order to even take a single step of gradient descent, you need to calculate, this sum.\n","And if m is say a million or 10 million or 100 million, you need to scan through your entire database,\n","scan through your entire dataset and calculate this for, you know, 100 million examples and sum it up.\n","And so every single step of gradient descent becomes very slow because you're scanning over, you're reading over, right,\n","like 100 million training examples, uh, uh, and uh, uh, before you can even, you know,\n","make one tiny little step of gradient descent. Okay, um, yeah, and by the way, I think- I feel like in today's era of\n","big data people start to lose intuitions about what's a big data-set. I think even by today's standards, like a hundred million examples is still very big, right,\n","I- I rarely- only rarely use a hundred million examples. Um, I don't know,\n","maybe in a few years we'll look back on a hundred million examples and say that was really small, but at least today. Uh, yeah.\n","So the main disadvantage of batch gradient descent is, every single step of gradient descent requires that you read through, you know,\n","your entire data-set, maybe terabytes of data-sets maybe- maybe- maybe, uh, tens or hundreds of terabytes of data,\n","uh, before you can even update the parameters just once. And if gradient descent needs, you know,\n","hundreds of iterations to converge, then you'll be scanning through your entire data-set hundreds of times.\n","Right, or-or and then sometimes we train, our algorithms with thousands or tens of thousands of iterations.\n","And so- so this- this gets expensive. So there's an alternative to batch gradient descent. Um, and\n","let me just write out the algorithm here that we can talk about it, which is going to repeatedly do this.\n","[NOISE] Oops, okay. Um, so this algorithm,\n","which is called stochastic gradient descent. [NOISE] Um, instead of scanning through\n","Stochastic Gradient Descent\n","all million examples before you update the parameters theta even a little bit,\n","in stochastic gradient descent, instead, in the inner loop of the algorithm, you loop through j equals 1 through m of taking a gradient descent step\n","using, the derivative of just one single example of just that, uh, one example, ah,\n","oh, excuse me it's through i, right. Yeah, so let i go from 1 to m,\n","and update theta j for every j. So you update this for j equals 1 through n, update theta j,\n","using this derivative that when now this derivative is taken just with respect to one training example- example I.\n","Okay, um, I'll- I'll just- alright and I guess you update this for every j.\n","Okay, and so, let me just draw a picture of what this algorithm is doing.\n","If um, this is the contour, like the one you saw just now.\n","So the axes are, uh, theta 0 and theta 1, and the height of the surface right, denote the contours as j of theta.\n","With stochastic gradient descent, what you do is you initialize the parameters somewhere. And then you will look at your first training example.\n","Hey, lets just look at one house, and see if you can predict that house as better, and you modify the parameters to\n","increase the accuracy where you predict the price of that one house. And because you're fitting the data just for one house, um, you know,\n","maybe you end up improving the parameters a little bit, but not quite going in the most direct direction downhill.\n","And you go look at the second house and say, hey, let's try to fit that house better. And then you update the parameters. And you look at third house, fourth house.\n","Right, and so as you run stochastic gradient descent, it takes a slightly noisy, slightly random path.\n","Uh, but on average, it's headed toward the global minimum, okay.\n","So as you run stochastic gradient descent- stochastic gradient descent will actually, never quite converge.\n","In- with- with batch gradient descent, it kind of went to the global minimum and stopped right,\n","uh, with stochastic gradient descent even as you won't run it, the parameters will oscillate and won't ever quite converge because you're always\n","running around looking at different houses and trying to do better than just that one house- and that one house- and that one house. Uh, but when you have a very large data-set,\n","stochastic gradient descent, allows your implementation- allows you algorithm to make much faster progress.\n","Uh, and so, um, uh, uh- and so when you have very large data-sets,\n","stochastic gradient descent is used much more in practice than batch gradient descent. [BACKGROUND]\n","Uh, yeah, is it possible to start with stochastic gradient descent and then switch over to batch gradient descent? Yes, it is.\n","So, uh, boy, something that wasn't talked about in this class, it's talked about in CS230 is Mini-batch gradient descent, where, um,\n","you don't- where you use say a hundred examples at a time rather than one example at a time. And so- uh, so that's another algorithm that I should use more often in practice.\n","I think people rarely- actually, so- so in practice, you know, when your dataset is large, we rarely,\n","ever switch to batch gradient descent, because batch gradient descent is just so slow, right.\n","So, I-I know I'm thinking through concrete examples of problems I've worked on.\n","And I think that what- maybe actually maybe- I think that uh,\n","for a lot of- for- for modern machine learning, where you have- if you have very- very large data sets, right so you know,\n","whether- if you're building a speech recognition system, you might have like a terabyte of data, right, and so, um,\n","it's so expensive to scan through a terabyte of data just reading it from disk, right it's so expensive that you would\n","probably never even run one iteration of batch gradient descent. Uh, and it turns out the- the-\n","there's one- one huge saving grace of stochastic gradient descent is, um, let's say you run stochastic gradient descent, right,\n","and, you know, you end up with this parameter and that's the parameter you use,\n","for your machine learning system, rather than the global optimum. It turns out that parameter is actually not that bad, right,\n","you- you probably make perfectly fine predictions even if you don't quite get to the like the global- global minimum.\n","So, uh, what you said I think it's a fine thing to do, no harm trying it.\n","Although in practice uh, uh, in practice we don't bother, I think in practice we usually use stochastic gradient descent.\n","The thing that actually is more common, is to slowly decrease the learning rate. So just keep using stochastic gradient descent,\n","but reduce the learning rate over time. So it takes smaller and smaller steps. So if you do that, then what happens is the size of the oscillations would decrease.\n","Uh, and so you end up oscillating or bouncing around the smaller region. So wherever you end up,\n","may not be the global- global minimum, but at least it'll- be it'll be closer to the global minimum. Yeah, so decreasing the learning rate is used much more often.\n","Cool. Question. Yeah. [BACKGROUND]\n","Oh sure, when do you stop with stochastic gradient descent? Uh, uh, plot to j of theta, uh, over time.\n","So j of theta is a cost function that you're trying to drag down. So monitor j of theta as,\n","you know, is going down over time, and then if it looks like it stopped going down, then you can say, \"Oh, it looks like it looks like it stopped going down,\" then you stop training.\n","Although- and then- ah, uh, you know, one nice thing about linear regression is that it has no local optimum and so,\n","um, uh, it- you run into these convergence debugging types of issues less often.\n","Where you're training highly non-linear things like neural networks, we should talk about later in CS229 as well.\n","Uh, these issues become more acute. Cool. Okay, great.\n","So, um, uh, yeah. [BACKGROUND].\n","Oh, would your learning rate be 1 over n times linear regressions then? Not really, it's usually much bigger than that.\n","Uh, uh, yeah, because if your learning rate was 1 over n times that of what you'd use with batch gradient descent\n","then it would end up being as slow as batch gradient descent, so it's usually much bigger. Okay. So, um, so that's stochastic gradient descent and- and- so I'll tell you what I do.\n","If- if you have a relatively small dataset, you know, if you have- if you have, I don't know, like hundreds of examples maybe thousands of examples where,\n","uh, it's computationally efficient to do batch gradient descent. If batch gradient descent doesn't cost too much,\n","I would almost always just use batch gradient descent because it's one less thing to fiddle with, right? It's just one less thing to have to worry about,\n","uh, the parameters oscillating, but your dataset is too large that batch gradient descent becomes prohibit- prohibitively slow,\n","then almost everyone will use, you know, stochastic gradient descent or whatever more like stochastic gradient descent, okay?\n","All right, so, um, gradient descent,\n","both batch gradient descent and stochastic gradient descent is an iterative algorithm meaning that you have to take multiple steps to get to,\n","you know, get near hopefully the global optimum. It turns out there is another algorithm,\n","uh, and- and, um, for many other algorithms we'll talk about in this course including generalized linear models and neural networks and a few other algorithms, uh,\n","you will have to use gradient descent and so- and so we'll see gradient descent, you know, as we develop multiple different algorithms later this quarter.\n","It turns out that for the special case of linear regression, uh, uh, and I mean linear regression but not the algorithm we'll talk about next Monday,\n","not the algorithm we'll talk about next Wednesday, but if the algorithm you're using is linear regression and exactly linear regression.\n","It turns out that there's a way to, uh, solve for the optimal value of the parameters theta to just jump in\n","one step to the global optimum without needing to use an iterative algorithm,\n","right, and this- this one I'm gonna present next is called the normal equation. It works only for linear regression,\n","doesn't work for any of the other algorithms I talk about later this quarter. But [NOISE] um, uh,\n","let me quickly show you the derivation of that. And, um, what I want to do is, uh,\n","give you a flavor of how to derive the normal equation and where you end up with is you know,\n","wha- what- what I hope to do is end up with a formula that lets you say theta equals some stuff where you just\n","set theta equals to that and in one step with a few matrix multiplications you end up with the optimal value of theta that lands you right at the global optimum,\n","right, just like that, just in one step. Okay. Um, and if you've taken, you know,\n","advanced linear algebra courses before or something, you may have seen, um, this formula for linear regression.\n","Wha- what a lot of linear algebra classes do is, what some linear algebra classes do is cover the board with,\n","you know, pages and pages of matrix derivatives. Um, what I wanna do is describe to you\n","a matrix derivative notation that allows you to derive the normal equation in roughly four lines of linear algebra,\n","uh, rather than some pages and pages of linear algebra and in the work I've done in machine learning you know,\n","sometimes notation really matters, right. If you have the right notation you can solve some problems much more easily and what I wanna do is,\n","um, uh, define this uh, matrix linear algebra notation and then I don't wanna do all the steps of the derivation,\n","I wanna give you- give you a sense of the flavor of what it looks like and then, um, I'll ask you to,\n","uh, uh, get a lot of details yourself, um, in the- in the lecture notes where we\n","work out everything in more detail than I want to do algebra in class. And, um, in problem set one you'll get to practice using this yourself to- to- to-,\n","you know, derive some additional things. I've- I've found this notation really convenient, right, for deriving learning algorithms.\n","Okay. So, um, I'm going to use the following notation.\n","Um, so J, right.\n","There's a function mapping from parameters to the real numbers. So I'm going to define this- this is the derivative of J of theta with respect to theta,\n","where- remember theta is a three-dimensional vector says R3,\n","or actually it's R n+1, right. If you have, uh, two features to the house if n=2,\n","then theta was 3 dimensional, it's n+1 dimensional so it's a vector. And so I'm gonna define the derivative with respect to theta of J of theta as follows.\n","Um, this is going to be itself a 3 by 1 vector [NOISE].\n","Okay, so I hope this notation is clear. So this is a three-dimensional vector with, uh, three components.\n","Alright so that's what I guess I'm.\n","So that's the first component is a vector, there's a second and there's a third. It's the partial derivative of J with respect to each of the three elements.\n","Um, and more generally,\n","in the notation we'll use, um, let me give you an example.\n","Um, uh, let's say that a is a matrix. So let's say that a is a two-by-two matrix.\n","Then, um, you can have a function, right, so let's say a is, you know,\n","A1-1, A1-2, A2-1 and A2-2, right.\n","So A is a two-by-two matrix. Then you might have some function um,\n","of a matrix A right, then that's a real number. So maybe f maps from A 2-by-2 to,\n","uh, excuse me, R 2-by-2, it's a real number. So, um, uh, and so for example,\n","if f of A equals A11 plus A12 squared, then f of, you know, 5,\n","6, 7, 8 would be equal to I guess 5 plus 6 squared, right.\n","So as we derive this, we'll be working a little bit with functions that map from matrices to real numbers and this is just one made up\n","example of a function that inputs a matrix and maps the matrix, maps the values of matrix to a real number.\n","And when you have a matrix function like this, I'm going to define the derivative with respect to A of f of A to be\n","equal to itself a matrix where the derivative of f of A with respect to the matrix A.\n","Uh, this itself will be a matrix with the same dimension of a and the elements of\n","this are the derivative with respect to the individual elements.\n","Actually, let me just write it like this. [NOISE]\n","Okay. So if A was a 2-by-2 matrix then the derivative of F of A with respect to A is itself\n","a 2-by-2 matrix and you compute this 2-by-2 matrix just by looking at F and taking,\n","uh, derivatives with respect to the different elements and plugging them into the different, the different elements of this matrix.\n","Okay. Um, and so in this particular example, I guess the derivative respect to A of F of A.\n","This would be, um, [NOISE] right,\n","it would be- it would be that. Ah and I got these four numbers by taking, um,\n","the definition of F and taking the derivative with respect to A_1, 1 and plugging that here.\n","Ah, taking the derivative with respect to A_1,2 and\n","plugging that here and taking the derivative with respect to the remaining elements and plugging them here which- which was 0.\n","Okay? So that's the definition of a matrix derivative. Yeah? [inaudible].\n","Oh, yes. We're just using the definition for a vector. Ah, N by 1 or N by 1 matrix. Yes. And in fact that definition and\n","this definition for the derivative of J with respect to Theta these are consistent. So if you apply that definition to a column vector,\n","treating a column vector as an N by 1 matrix or N, I guess it would be N plus 1 by 1 matrix\n","then that- that specializes to what we described here. [NOISE]\n","All right. So, um, let's see.\n","Okay. So, um, I want to leave the details of the lecture notes because there's\n","more lines of algebra which I won't do but it'll give you an overview [NOISE] of what the derivation of the normal equation looks like.\n","Um, so onto this definition of a derivative of a- of a matrix,\n","um, the broad outline of what we're going to do is we're going to take J of Theta.\n","Right. That's the cost function. Um, take the derivative with respect to Theta.\n","Right. Ah, since Theta is a vector so you want to take the derivative with respect to Theta and you know well,\n","how do you minimize a function? You take derivatives with [NOISE] respect to Theta and set it equal to 0.\n","And then you solve for the value of Theta so that the derivative is 0. Right. The- the minimum, you know, the maximum and minimum of a function is where the derivative is equal to 0.\n","So- so how you derive the normal equation is take this vector. Ah, so J of Theta maps from a vector to a real number.\n","So we'll, take the derivatives respect to Theta set it to 0,0 and solve for Theta and then we end up with a formula for Theta that lets you just,\n","um, ah, you know, immediately go to the global minimum of the- of the cost function J of Theta.\n","And, and a lot of the build up, a lot of this notation is, you know, is there- what does this mean and is there\n","an easy way to compute the derivative of J of Theta? Okay? So, um, ah,\n","so I hope you understand the lecture notes when hopefully you take a look at them, ah, just a couple other derivations.\n","Um, if A [NOISE] is a square matrix.\n","So let's say A is a [NOISE] N, N by N matrix. So number of rows equals number of columns.\n","Um, I'm going to denote the trace of A [NOISE] to be equal to [NOISE] the sum of the diagonal entries.\n","[NOISE] So sum of i of A_ii.\n","And this is pronounced the trace of A, um, and, ah, and- and you can- you can also write this as\n","trace operator like the trace function applied to A but by convention we often write trace of A without the parentheses.\n","And so this is called the trace of A. [NOISE] So trace just means sum of diagonal entries and,\n","um, some facts about the trace of a matrix. You know, trace of A is equal to\n","the trace of A transpose because if you transpose a matrix, right, you're just flipping it along the- the 45 degree axis.\n","And so the the diagonal entries actually stay the same when you transpose the matrix. So the trace of A is equal to trace of A transpose, um,\n","and then, ah, there-there are some other useful properties of, um, the trace operator.\n","Um, here's one that I don't want to prove but that you could go home and prove yourself with a-with a few with- with a little bit of work, maybe not,\n","not too much which is, ah, if you define, um, F of A [NOISE] equals trace of A times B.\n","So here if B is some fixed matrix, right, ah, and what F of A does is it multiplies A and\n","B and then it takes the sum of diagonal entries. Then it turns out that the derivative with respect to A of F of A is equal to,\n","um, B transpose [NOISE].\n","Um, and this is, ah, you could prove this yourself. For any matrix B, if F of A is defined this way,\n","the de- the derivative is equal to B transpose. Um, the trace function or the trace operator has other interesting properties.\n","The trace of AB is equal to the trace of BA. Ah, um, you could- you could prove this from past principles,\n","it's a little bit of work to prove, ah, ah, that- that you, if you expand out the definition of A and B it should prove\n","that [NOISE] and the trace of A times B times C is equal to [NOISE] the trace of C times A times B. Ah,\n","this is a cyclic permutation property. If you have a multiply, you know, multiply several matrices together you can always take one from\n","the end and move it to the front and the trace will remain the same. [NOISE] And,\n","um, another one that is a little bit harder to prove is that the trace,\n","excuse me, the derivative of A trans- of AA transpose C is [NOISE] Okay.\n","Yeah. So I think just as- just as for you know, ordinary, um, calculus we know the derivative of X squared is 2_X.\n","Right. And so we all figured out that very well. We just use it too much without- without having to re-derive it every time.\n","Ah, this is a little bit like that. The trace of A squared C is, you know, two times CA.\n","Right. It's a little bit like that but- but with- with matrix notation as there. So think of this as analogous to D,\n","DA of A squared C equals 2AC. Right. But that is like the matrix version of that.\n","[NOISE]\n","All right. So finally, um,\n","what I'd like to do is take J of Theta and express it in this,\n","uh, you know, matrix vector notation. So we can take derivatives with respect to Theta, and set the derivatives equal to 0,\n","and just solve for the value of Theta, right? And so, um, let me just write out the definition of J of Theta.\n","So J of Theta was one-half sum from i equals 1 through m\n","of h of x i minus y i squared.\n","Um, and it turns out that, um, all right.\n","It turns out that, um, if it is, if you define a matrix capital X as follows.\n","Which is, I'm going to take the matrix capital X and take the training examples we have,\n","you know, and stack them up in rows.\n","So we have m training examples, right? So so the X's were column vectors.\n","So I'm taking transpose to just stack up the training examples in, uh, in rows here.\n","So let me call this the design matrix. So the capital X called the design matrix. And, uh, it turns out that if you define X this way,\n","then X times Theta, there's this thing times Theta.\n","And the way a matrix vector multiplication works is your Theta is now a column vector, right?\n","So Theta is, you know, Theta_0, Theta_1, Theta_2. So the way that, um,\n","matrix-vector multiplication works is you multiply this column vector with each of these in in turn.\n","And so this ends up being X1 transpose Theta, X2 transpose Theta,\n","down to X_m transpose Theta,\n","which is of course just the vector of all of the predictions of the algorithm.\n","And so if, um, now let me also define a vector y to be taking all of the,\n","uh, labels from your training example,\n","and stacking them up into a big column vector, right. Let me define y that way.\n","Um, it turns out that, um,\n","J of Theta can then be written as one-half of\n","X Theta minus y transpose X Theta minus y.\n","Okay. Um, and let me see. Yeah. Let me just, uh, uh,\n","outline the proof, but I won't do this in great detail. So X Theta minus y is going to be,\n","right, so this is X Theta, this is y. So, you know, X Theta minus y is going to be this vector of h of x1 minus\n","y1 down to h of xm minus ym, right.\n","So it's just all the errors your learning algorithm is making on the m examples. It's the difference between predictions and the actual labels.\n","And if you you remember, so Z transpose Z is equal to sum over i Z squared, right.\n","A vector transpose itself is a sum of squares of elements. And so this vector transpose itself is the sum of squares of the elements, right.\n","So so which is why, uh, so so the cost function J of Theta is computed by taking the sum of squares of all of these elements,\n","of all of these errors, and and the way you do that is to take this vector, your X Theta minus y transpose itself,\n","is the sum of squares of these, which is exactly the error. So that's why you end up with a,\n","this as the sum of squares of the, those error terms.\n","Okay. And, um, if some of the steps don't quite make sense,\n","really don't worry about it. All this is written out more slowly and carefully in the lecture notes. But I wanted you to have a sense of the, uh,\n","broad arc of the of the big picture of their derivation before you go through them yourself in greater detail in the lecture notes elsewhere.\n","So finally, what we want to do is take the derivative with respect to Theta of J of Theta,\n","and set that to 0. And so this is going to be equal to the derivative of\n","one-half X Theta minus y transpose X Theta minus y. Um, and so I'm gonna,\n","I'm gonna do the steps really quickly, right. So the steps require some of the little properties of\n","traces and matrix derivatives I wrote down briefly just now. But so I'm gonna do these very quickly without getting into the details, but, uh,\n","so this is equal to one-half derivative of Theta of, um.\n","So take transposes of these things. So this becomes Theta transpose X transpose minus y transpose.\n","Right. Um, and then, uh, kind of like expanding out a quadratic function, right.\n","This is, you know, A minus B times C minus D. So you can just AC minus AD this and so on.\n","So I'll just write this out.\n","All right. And so, uh, what I just did here this is similar to how, you know,\n","ax minus b times ax minus b, is equal to a squared x squared minus axb minus bax plus b squared.\n","Is it's kind of, it's just expanding out a quadratic function. Um, and then the final step\n","is, yeah, go ahead. [BACKGROUND] Oh, is that right?\n","Oh yes, thank you. Thank you. Um, and then the final step is,\n","you know, for each of these four terms; first, second, third, and fourth terms, to take the derivative with respect to Theta.\n","And if you use some of the formulas I was alluding to over there, you find that the derivative, um,\n","which which I don't want to show the derivation of, but it turns out that the derivative is, um,\n","X transpose X Theta plus X transpose X Theta minus,\n","um, X transpose y minus X transpose y,\n","um, and we're going to set this derivative. Actually not, let me just do this.\n","And so this simplifies to X transpose X Theta minus X transpose y.\n","And so as as described earlier, I'm gonna set this derivative to 0.\n","And how to go from this step to that step is using the matrix derivatives, uh, explained in more detail in the lecture notes.\n","And so the final step is, you know, having set this to 0, this implies that X transpose X Theta equals X transpose y.\n","Uh, so this is called the normal equations.\n","And the optimum value for Theta is Theta equals\n","X transpose X inverse, X transpose y.\n","Okay. Um, and if you implement this, um,\n","then, you know, you can in basically one step, get the value of Theta that corresponds to the global minimum.\n","Okay. Um, and and and again,\n","common question I get is one is, well, what if X is non-invertible? Uh, what that usually means is you have have redundant features,\n","uh, that your features are linearly dependent. Uh, but if you use something called the pseudo inverse,\n","you kind of get the right answer if that's the case. Although I think the, uh, even more right answer is if you have linearly dependent features,\n","probably means you have the same feature repeated twice, and I will usually go and figure out what features are actually repeated,\n","leading to this problem. Okay. All right.\n","Uh, any last questions before- so that so that's the normal equations. Hope you read through the detailed derivations in the lecture notes.\n","Any last questions before we break? Okay. [BACKGROUND]\n","Oh, yeah. How do you choose a learning rate? It's, it's, it's quite empirical, I think. So most people would try different values,\n","and then just pick one. All right. I think let's let's break. If, if people have more questions,\n","when the TAs come up, we're going to keep taking questions. Well, let's break for the day. Thanks everyone.\n","\"\"\"\n","\n","lecture2_pages = [Document(page_content=texto)]\n","print(lecture2_pages[0].page_content[:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPqt8EYHfwKj","executionInfo":{"status":"ok","timestamp":1746986329636,"user_tz":-120,"elapsed":88,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"ab3c40eb-285c-4a67-a97c-2f70c12b297d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Intro\n","Morning and welcome back. So what we'll see today in class\n","is the first in-depth discussion of a learning algorithm, linear regression, and in particular,\n","over the next, what, hour and a bit you'll see linear regression, batch and stochastic gradient descent is\n","an algorithm for fitting linear regression models, and then the normal equations, um, uh,\n","as a way of- as a very efficient way to let you fit linear models.\n","Um, and we're going to define notation, and a few concepts today that will\n"]}]},{"cell_type":"markdown","source":["### Website Loading\n","\n","En este caso cargaremos un Markdown de Github de la biblioteca Pytorch. Su readme para poder extraer información: [Readme.md](https://github.com/pytorch/pytorch/blob/main/README.md)"],"metadata":{"id":"nJ_cSZYlZ_ew"}},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\"https://raw.githubusercontent.com/pytorch/pytorch/main/README.md\")\n","#loader = WebBaseLoader(\"https://www.codemancers.pro/\")"],"metadata":{"id":"sQytBYwqaMig","executionInfo":{"status":"ok","timestamp":1747854578500,"user_tz":-120,"elapsed":5,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["lecture3_pages = loader.load()"],"metadata":{"id":"Ggaxf6q1aVQF","executionInfo":{"status":"ok","timestamp":1747854579585,"user_tz":-120,"elapsed":282,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["print(lecture3_pages[0].page_content[:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4hZsmzXaZPo","executionInfo":{"status":"ok","timestamp":1747854580133,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"d8f6c80c-c9c3-4914-a6c0-a0a64ba5a72f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n","\n","--------------------------------------------------------------------------------\n","\n","PyTorch is a Python package that provides two high-level features:\n","- Tensor computation (like NumPy) with strong GPU acceleration\n","- Deep neural networks built on a tape-based autograd system\n","\n","You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n","\n","Our trunk\n"]}]},{"cell_type":"markdown","source":["## Document Chunking"],"metadata":{"id":"yd4JqL3qdRLm"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"],"metadata":{"id":"Dqxu41LedV9M","executionInfo":{"status":"ok","timestamp":1747854614161,"user_tz":-120,"elapsed":31,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["Configuraremos un tamaño de Chunk de 26, con un solapamiento de 4\n"],"metadata":{"id":"bObuIgMzdZ_h"}},{"cell_type":"markdown","source":["### Fixed-size"],"metadata":{"id":"zKCOsgLAeE0y"}},{"cell_type":"code","source":["chunk_size = 26\n","chunk_overlap = 4"],"metadata":{"id":"XULT7TIFdZSk","executionInfo":{"status":"ok","timestamp":1747854622469,"user_tz":-120,"elapsed":1,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=chunk_size,\n","    chunk_overlap=chunk_overlap\n",")\n","c_splitter = CharacterTextSplitter(\n","    chunk_size=chunk_size,\n","    chunk_overlap=chunk_overlap\n",")"],"metadata":{"id":"f8x9Liqbdfpz","executionInfo":{"status":"ok","timestamp":1747854626310,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["text1 = 'abcdefghijklmnopqrstuvwxyz'\n","r_splitter.split_text(text1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udWWGS05dl0B","executionInfo":{"status":"ok","timestamp":1747854647233,"user_tz":-120,"elapsed":5,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"8adf9ab2-4850-41bb-8a2f-1ae4900b1ade"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['abcdefghijklmnopqrstuvwxyz']"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["text2 = 'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz'\n","r_splitter.split_text(text2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9Ia9Bx9drQv","executionInfo":{"status":"ok","timestamp":1747854678975,"user_tz":-120,"elapsed":46,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"488e6a6b-1ec8-4cfa-fd94-957761720d19"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['abcdefghijklmnopqrstuvwxyz',\n"," 'wxyzabcdefghijklmnopqrstuv',\n"," 'stuvwxyzabcdefghijklmnopqr',\n"," 'opqrstuvwxyzabcdefghijklmn',\n"," 'klmnopqrstuvwxyzabcdefghij',\n"," 'ghijklmnopqrstuvwxyz']"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### Recursive Structure Aware Splitting"],"metadata":{"id":"bauMbM02eKly"}},{"cell_type":"code","source":["chapter2_lotr = \"\"\"2. Concerning Pipe-weed\n","\n","There is another astonishing thing about Hobbits of old that must be\n","mentioned, an astonishing habit: they imbibed or inhaled, through pipes of\n","clay or wood, the smoke of the burning leaves of a herb, which they called\n","pipe-weed or leaf, a variety probably of Nicotiana. A great deal of mystery\n","surrounds the origin of this peculiar custom, or 'art' as the Hobbits\n","preferred to call it. All that could be discovered about it in antiquity was\n","put together by Meriadoc Brandybuck (later Master of Buckland), and since he\n","and the tobacco of the Southfarthing play a part in the history that\n","follows, his remarks in the introduction to his Herblore of the Shire may be\n","quoted.\n","\n","’This,' he says, 'is the one art that we can certainly claim to be our\n","own invention. When Hobbits first began to smoke is not known, all the\n","legends and family histories take it for granted; for ages folk in the Shire\n","smoked various herbs, some fouler, some sweeter. But all accounts agree that\n","Tobold Hornblower of Longbottom in the Southfarthing first grew the true\n","pipe-weed in his gardens in the days of Isengrim the Second, about the year\n","1070 of Shire-reckoning. The best home-grown still comes from that district,\n","especially the varieties now known as Longbottom Leaf, Old Toby, and\n","Southern Star.\n","\n","'How Old Toby came by the plant is not recorded, for to his dying day\n","he would not tell. He knew much about herbs, but he was no traveller. It is\n","said that in his youth he went often to Bree, though he certainly never went\n","further from the Shire than that. It is thus quite possible that he learned\n","of this plant in Bree, where now, at any rate, it grows well on the south\n","slopes of the hill. The Bree-hobbits claim to have been the first actual\n","smokers of the pipe-weed. They claim, of course, to have done everything\n","before the people of the Shire, whom they refer to as \"colonists\"; but in\n","this case their claim is, I think, likely to be true. And certainly it was\n","from Bree that the art of smoking the genuine weed spread in the recent\n","centuries among Dwarves and such other folk, Rangers, Wizards, or wanderers,\n","as still passed to and fro through that ancient road-meeting. The home and\n","centre of the an is thus to be found in the old inn of Bree, The Prancing\n","\n","\n","Pony, that has been kept by the family of Butterbur from time beyond record.\n","\n","'All the same, observations that I have made on my own many journeys\n","south have convinced me that the weed itself is not native to our parts of\n","the world, but came northward from the lower Anduin, whither it was, I\n","suspect, originally brought over Sea by the Men of Westernesse. It grows\n","abundantly in Gondor, and there is richer and larger than in the North,\n","where it is never found wild, and flourishes only in warm sheltered places\n","like Longbottom. The Men of Gondor call it sweet galenas, and esteem it only\n","for the fragrance of its flowers. From that land it must have been carried\n","up the Greenway during the long centuries between the coming of Elendil and\n","our own day. But even the D®nedain of Gondor allow us this credit: Hobbits\n","first put it into pipes. Not even the Wizards first thought of that before\n","we did. Though one Wizard that I knew took up the art long ago, and became\n","as skilful in it as in all other things that he put his mind to.' \"\"\""],"metadata":{"id":"DZgm_MF0eUEu","executionInfo":{"status":"ok","timestamp":1747854775976,"user_tz":-120,"elapsed":13,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["len(chapter2_lotr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dm1v4CZeekZ5","executionInfo":{"status":"ok","timestamp":1747854788389,"user_tz":-120,"elapsed":7,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"2f6f5c05-50d3-4375-d27e-0168188f6721"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3306"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["c_splitter = CharacterTextSplitter(\n","    chunk_size=100,\n","    chunk_overlap=0,\n","    separator = ' '\n",")\n","r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=100,\n","    chunk_overlap=0,\n","    separators=[\".\", \",\", \":\"]\n",")"],"metadata":{"id":"pXU4xIdOeqQG","executionInfo":{"status":"ok","timestamp":1747854792011,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["c_splitter.split_text(chapter2_lotr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DflC2uVieskT","executionInfo":{"status":"ok","timestamp":1747854811137,"user_tz":-120,"elapsed":16,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"bf116708-083b-46d5-84e1-71ea8404d5d5"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2. Concerning Pipe-weed\\n\\nThere is another astonishing thing about Hobbits of old that must',\n"," 'be\\nmentioned, an astonishing habit: they imbibed or inhaled, through pipes of\\nclay or wood, the',\n"," 'smoke of the burning leaves of a herb, which they called\\npipe-weed or leaf, a variety probably of',\n"," \"Nicotiana. A great deal of mystery\\nsurrounds the origin of this peculiar custom, or 'art' as the\",\n"," 'Hobbits\\npreferred to call it. All that could be discovered about it in antiquity was\\nput together by',\n"," 'Meriadoc Brandybuck (later Master of Buckland), and since he\\nand the tobacco of the Southfarthing',\n"," 'play a part in the history that\\nfollows, his remarks in the introduction to his Herblore of the',\n"," \"Shire may be\\nquoted.\\n\\n’This,' he says, 'is the one art that we can certainly claim to be our\\nown\",\n"," 'invention. When Hobbits first began to smoke is not known, all the\\nlegends and family histories take',\n"," 'it for granted; for ages folk in the Shire\\nsmoked various herbs, some fouler, some sweeter. But all',\n"," 'accounts agree that\\nTobold Hornblower of Longbottom in the Southfarthing first grew the',\n"," 'true\\npipe-weed in his gardens in the days of Isengrim the Second, about the year\\n1070 of',\n"," 'Shire-reckoning. The best home-grown still comes from that district,\\nespecially the varieties now',\n"," \"known as Longbottom Leaf, Old Toby, and\\nSouthern Star.\\n\\n'How Old Toby came by the plant is not\",\n"," 'recorded, for to his dying day\\nhe would not tell. He knew much about herbs, but he was no traveller.',\n"," 'It is\\nsaid that in his youth he went often to Bree, though he certainly never went\\nfurther from the',\n"," 'Shire than that. It is thus quite possible that he learned\\nof this plant in Bree, where now, at any',\n"," 'rate, it grows well on the south\\nslopes of the hill. The Bree-hobbits claim to have been the first',\n"," 'actual\\nsmokers of the pipe-weed. They claim, of course, to have done everything\\nbefore the people of',\n"," 'the Shire, whom they refer to as \"colonists\"; but in\\nthis case their claim is, I think, likely to be',\n"," 'true. And certainly it was\\nfrom Bree that the art of smoking the genuine weed spread in the',\n"," 'recent\\ncenturies among Dwarves and such other folk, Rangers, Wizards, or wanderers,\\nas still passed',\n"," 'to and fro through that ancient road-meeting. The home and\\ncentre of the an is thus to be found in',\n"," 'the old inn of Bree, The Prancing\\n\\n\\nPony, that has been kept by the family of Butterbur from time',\n"," \"beyond record.\\n\\n'All the same, observations that I have made on my own many journeys\\nsouth have\",\n"," 'convinced me that the weed itself is not native to our parts of\\nthe world, but came northward from',\n"," 'the lower Anduin, whither it was, I\\nsuspect, originally brought over Sea by the Men of Westernesse.',\n"," 'It grows\\nabundantly in Gondor, and there is richer and larger than in the North,\\nwhere it is never',\n"," 'found wild, and flourishes only in warm sheltered places\\nlike Longbottom. The Men of Gondor call it',\n"," 'sweet galenas, and esteem it only\\nfor the fragrance of its flowers. From that land it must have been',\n"," 'carried\\nup the Greenway during the long centuries between the coming of Elendil and\\nour own day. But',\n"," 'even the D®nedain of Gondor allow us this credit: Hobbits\\nfirst put it into pipes. Not even the',\n"," 'Wizards first thought of that before\\nwe did. Though one Wizard that I knew took up the art long ago,',\n"," \"and became\\nas skilful in it as in all other things that he put his mind to.'\"]"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["r_splitter.split_text(chapter2_lotr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DmzSJd8evIU","executionInfo":{"status":"ok","timestamp":1747854834536,"user_tz":-120,"elapsed":13,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"730b0c33-a339-4a78-e9af-7e0668b21c11"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2',\n"," '. Concerning Pipe-weed\\n\\nThere is another astonishing thing about Hobbits of old that must be\\nmentioned',\n"," ', an astonishing habit: they imbibed or inhaled, through pipes of\\nclay or wood',\n"," ', the smoke of the burning leaves of a herb, which they called\\npipe-weed or leaf',\n"," ', a variety probably of Nicotiana',\n"," '. A great deal of mystery\\nsurrounds the origin of this peculiar custom',\n"," \", or 'art' as the Hobbits\\npreferred to call it\",\n"," '. All that could be discovered about it in antiquity was\\nput together by Meriadoc Brandybuck (later Master of Buckland)',\n"," ', and since he\\nand the tobacco of the Southfarthing play a part in the history that\\nfollows',\n"," ', his remarks in the introduction to his Herblore of the Shire may be\\nquoted',\n"," \".\\n\\n’This,' he says, 'is the one art that we can certainly claim to be our\\nown invention\",\n"," '. When Hobbits first began to smoke is not known',\n"," ', all the\\nlegends and family histories take it for granted; for ages folk in the Shire\\nsmoked various herbs',\n"," ', some fouler, some sweeter',\n"," '. But all accounts agree that\\nTobold Hornblower of Longbottom in the Southfarthing first grew the true\\npipe-weed in his gardens in the days of Isengrim the Second',\n"," ', about the year\\n1070 of Shire-reckoning',\n"," '. The best home-grown still comes from that district',\n"," ',\\nespecially the varieties now known as Longbottom Leaf, Old Toby, and\\nSouthern Star',\n"," \".\\n\\n'How Old Toby came by the plant is not recorded, for to his dying day\\nhe would not tell\",\n"," '. He knew much about herbs, but he was no traveller',\n"," '. It is\\nsaid that in his youth he went often to Bree',\n"," ', though he certainly never went\\nfurther from the Shire than that',\n"," '. It is thus quite possible that he learned\\nof this plant in Bree, where now, at any rate',\n"," ', it grows well on the south\\nslopes of the hill',\n"," '. The Bree-hobbits claim to have been the first actual\\nsmokers of the pipe-weed',\n"," '. They claim, of course, to have done everything\\nbefore the people of the Shire',\n"," ', whom they refer to as \"colonists\"; but in\\nthis case their claim is, I think, likely to be true',\n"," '. And certainly it was\\nfrom Bree that the art of smoking the genuine weed spread in the recent\\ncenturies among Dwarves and such other folk',\n"," ', Rangers, Wizards, or wanderers,\\nas still passed to and fro through that ancient road-meeting',\n"," '. The home and\\ncentre of the an is thus to be found in the old inn of Bree, The Prancing\\n\\n\\nPony',\n"," ', that has been kept by the family of Butterbur from time beyond record',\n"," \".\\n\\n'All the same\",\n"," ', observations that I have made on my own many journeys\\nsouth have convinced me that the weed itself is not native to our parts of\\nthe world',\n"," ', but came northward from the lower Anduin, whither it was, I\\nsuspect',\n"," ', originally brought over Sea by the Men of Westernesse',\n"," '. It grows\\nabundantly in Gondor, and there is richer and larger than in the North',\n"," ',\\nwhere it is never found wild, and flourishes only in warm sheltered places\\nlike Longbottom',\n"," '. The Men of Gondor call it sweet galenas, and esteem it only\\nfor the fragrance of its flowers',\n"," '. From that land it must have been carried\\nup the Greenway during the long centuries between the coming of Elendil and\\nour own day',\n"," '. But even the D®nedain of Gondor allow us this credit: Hobbits\\nfirst put it into pipes',\n"," '. Not even the Wizards first thought of that before\\nwe did',\n"," '. Though one Wizard that I knew took up the art long ago',\n"," ', and became\\nas skilful in it as in all other things that he put his mind to',\n"," \".'\"]"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","text_splitter = CharacterTextSplitter(\n","    separator=\"\\n\",\n","    chunk_size=1000,\n","    chunk_overlap=150,\n","    length_function=len\n",")"],"metadata":{"id":"4m-ee9Xx4G9k","executionInfo":{"status":"ok","timestamp":1747854919931,"user_tz":-120,"elapsed":48,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["docs = text_splitter.split_documents(lecture1_pages)"],"metadata":{"id":"cueIFo6x4MRW","executionInfo":{"status":"ok","timestamp":1747854921904,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3V2iPEo5Fg3","executionInfo":{"status":"ok","timestamp":1747854925908,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"1cc512cf-44a3-4cc6-e0ee-c90ef65b7cb5"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["len(lecture1_pages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCLwmYiG5H-v","executionInfo":{"status":"ok","timestamp":1747854961174,"user_tz":-120,"elapsed":44,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"c85487d1-fd26-4c68-d0d7-bf4cea0df81f"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["### Content-Aware Splitting\n"],"metadata":{"id":"yRpLCwU65ebS"}},{"cell_type":"code","source":["from langchain.text_splitter import MarkdownHeaderTextSplitter"],"metadata":{"id":"yTgGGI5c5oWW","executionInfo":{"status":"ok","timestamp":1747854982102,"user_tz":-120,"elapsed":42,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["headers_to_split_on = [\n","    (\"#\", \"Header 1\"),\n","    (\"##\", \"Header 2\"),\n","    (\"###\", \"Header 3\"),\n","]"],"metadata":{"id":"slQGVUHq5qS7","executionInfo":{"status":"ok","timestamp":1747854983003,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["markdown_splitter = MarkdownHeaderTextSplitter(\n","    headers_to_split_on=headers_to_split_on\n",")\n","md_header_splits = markdown_splitter.split_text(lecture3_pages[0].page_content)"],"metadata":{"id":"xpREWFti5tQH","executionInfo":{"status":"ok","timestamp":1747854984201,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["md_header_splits[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhJCVc-E5_0u","executionInfo":{"status":"ok","timestamp":1747854986335,"user_tz":-120,"elapsed":11,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"ff77f5a4-01c6-4705-e62a-57a8101aa617"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={}, page_content='![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)  \\n--------------------------------------------------------------------------------  \\nPyTorch is a Python package that provides two high-level features:\\n- Tensor computation (like NumPy) with strong GPU acceleration\\n- Deep neural networks built on a tape-based autograd system  \\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.  \\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).  \\n- [More About PyTorch](#more-about-pytorch)\\n- [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\\n- [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\\n- [Python First](#python-first)\\n- [Imperative Experiences](#imperative-experiences)\\n- [Fast and Lean](#fast-and-lean)\\n- [Extensions Without Pain](#extensions-without-pain)\\n- [Installation](#installation)\\n- [Binaries](#binaries)\\n- [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\\n- [From Source](#from-source)\\n- [Prerequisites](#prerequisites)\\n- [NVIDIA CUDA Support](#nvidia-cuda-support)\\n- [AMD ROCm Support](#amd-rocm-support)\\n- [Intel GPU Support](#intel-gpu-support)\\n- [Get the PyTorch Source](#get-the-pytorch-source)\\n- [Install Dependencies](#install-dependencies)\\n- [Install PyTorch](#install-pytorch)\\n- [Adjust Build Options (Optional)](#adjust-build-options-optional)\\n- [Docker Image](#docker-image)\\n- [Using pre-built images](#using-pre-built-images)\\n- [Building the image yourself](#building-the-image-yourself)\\n- [Building the Documentation](#building-the-documentation)\\n- [Building a PDF](#building-a-pdf)\\n- [Previous Versions](#previous-versions)\\n- [Getting Started](#getting-started)\\n- [Resources](#resources)\\n- [Communication](#communication)\\n- [Releases and Contributing](#releases-and-contributing)\\n- [The Team](#the-team)\\n- [License](#license)')"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["md_header_splits[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dw2b8Wf6CA3","executionInfo":{"status":"ok","timestamp":1747854988733,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"4b9ae0e3-f63a-47e1-86b3-2d96b95a8dfb"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'Header 2': 'More About PyTorch'}, page_content='[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)  \\nAt a granular level, PyTorch is a library that consists of the following components:  \\n| Component | Description |\\n| ---- | --- |\\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |  \\nUsually, PyTorch is used either as:  \\n- A replacement for NumPy to use the power of GPUs.\\n- A deep learning research platform that provides maximum flexibility and speed.  \\nElaborating Further:')"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["### NLP Chunking"],"metadata":{"id":"shJ9op9L61Tk"}},{"cell_type":"markdown","source":["Necesitamos un modelo de embedding para hacer el Splitting. En este ejemplo usaremos OpenAI.\n"],"metadata":{"id":"NmTR3FpI7tUY"}},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2QNNCxU65gj","executionInfo":{"status":"ok","timestamp":1747855140541,"user_tz":-120,"elapsed":7155,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"ebae79f8-8c16-400a-9721-1d3ba0582df6"},"execution_count":43,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API Key:··········\n"]}]},{"cell_type":"markdown","source":["¡Ahora implementamos el `SemanticChunker`!\n","\n","Hoy utilizaremos el umbral `percentile` como ejemplo - pero hay tres estrategias diferentes que podríais utilizar (descripciones proporcionadas por los [docs de LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker) sobre el Fragmentador Semántico):\n","\n","- `percentile` (por defecto) - En este método, se calculan todas las diferencias entre frases y luego cualquier diferencia mayor que el percentil X se divide.\n","\n","- `standard_deviation` - En este método, cualquier diferencia mayor que X desviaciones estándar se divide.\n","\n","- `interquartile` - En este método, la distancia intercuartílica se utiliza para dividir los fragmentos.\n","\n","La idea básica es la siguiente:\n","\n","1. Divide nuestro documento en frases (basado en `.`, `?`, y `!`)\n","2. Indexa cada frase basándose en la posición\n","3. Añade un `buffer_size` (`int`) de frases a cada lado de nuestra frase seleccionada\n","4. Calcula distancias entre grupos de frases\n","5. Fusiona grupos basándose en la similitud según los umbrales anteriores\n","\n","> NOTA: Este método es actualmente experimental y no está en una forma final estable - esperad actualizaciones y mejoras en los próximos meses\n"],"metadata":{"id":"x7Mc-vn27ivH"}},{"cell_type":"code","source":["from langchain_experimental.text_splitter import SemanticChunker\n","from langchain_openai.embeddings import OpenAIEmbeddings\n","\n","semantic_chunker = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\")"],"metadata":{"id":"9B8utnDI7G3r","executionInfo":{"status":"ok","timestamp":1747855149142,"user_tz":-120,"elapsed":3552,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","loader = PyPDFLoader(\"https://arxiv.org/pdf/2504.21801v1\")\n","lecture1_pages = loader.load()"],"metadata":{"id":"XmOSR2GO8o11","executionInfo":{"status":"ok","timestamp":1747855330783,"user_tz":-120,"elapsed":2439,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["content = []\n","\n","for page in lecture1_pages:\n","  content.append(page.page_content)\n","\n","semantic_chunks = semantic_chunker.create_documents(content)"],"metadata":{"id":"YB_atT0U8R-x","executionInfo":{"status":"ok","timestamp":1747855344168,"user_tz":-120,"elapsed":13382,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["len(semantic_chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7g6_CaIu9bXY","executionInfo":{"status":"ok","timestamp":1747855344173,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"fcfaa924-48d6-4dba-a3dc-441d4c964402"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["82"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["print(semantic_chunks[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbmP1Ijb9jyR","executionInfo":{"status":"ok","timestamp":1747855344194,"user_tz":-120,"elapsed":20,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"f5e07fac-a40e-4924-ae6e-04d9155965ef"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via\n","Reinforcement Learning for Subgoal Decomposition\n","Z.Z. Ren*, Zhihong Shao*, Junxiao Song*, Huajian Xin†, Haocheng Wang†, Wanjia Zhao†, Liyue Zhang, Zhe Fu\n","Qihao Zhu, Dejian Yang, Z.F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao\n","Daya Guo, Chong Ruan\n","DeepSeek-AI\n","https://github.com/deepseek-ai/DeepSeek-Prover-V2\n","Abstract\n","We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal\n","theorem proving in Lean 4, with initialization data collected through a recursive theorem prov-\n","ing pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting\n","DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved\n","subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3’s step-\n","by-step reasoning, to create an initial cold start for reinforcement learning. This process enables\n","us to integrate both informal and formal mathematical reasoning into a unified model. The\n","resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\n","theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658\n","problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench,\n","a collection of 325 formalized problems, to enrich our evaluation, including 15 selected prob-\n","lems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME\n","problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3\n","solves 8 of these problems using majority voting, highlighting that the gap between formal and\n","informal mathematical reasoning in large language models is substantially narrowing. MiniF2F-test\n","60\n","65\n","70\n","75\n","80\n","85\n","90Pass Rate (%)\n","88.9%\n","82.0%\n","80.7%\n","73.0%\n","67.6%\n","64.7%\n","PutnamBench\n","0\n","10\n","20\n","30\n","40\n","50Number of Problems Solved (out of 658)\n","49\n","23\n","8 7\n","ProverBench-AIME 24&25\n","0\n","2\n","4\n","6\n","8\n","10Number of Problems Solved (out of 15)\n","8\n","6\n","1\n","DeepSeek-Prover-V2-671B DeepSeek-Prover-V2-7B Kimina-Prover-Preview 72B BFS-Prover 7B STP 7B Geodel-Prover 7B DeepSeek-V3-0324 (Informal)\n","Figure 1 |Benchmark performance of DeepSeek-Prover-V2. On the AIME benchmark, DeepSeek-\n","V3 is evaluated using the standard find-answer task for natural-language reasoning, while\n","prover models generate Lean code to construct formal proofs for a given correct answer. *Core contributors †Work done during internship at DeepSeek-AI.'\n"]}]},{"cell_type":"code","source":["print(semantic_chunks[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCDvgQ8y-fr6","executionInfo":{"status":"ok","timestamp":1747855344198,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"6bf53795-4574-4fe9-d2c6-f8b03245ffbd"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='arXiv:2504.21801v1  [cs.CL]  30 Apr 2025'\n"]}]},{"cell_type":"code","source":["print(semantic_chunks[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSTc8H4nrIaC","executionInfo":{"status":"ok","timestamp":1747855344201,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"9ca2fc41-7130-4e84-bce3-10bc0c39eafc"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='1. Introduction\n","The emergence of reasoning capabilities in large language models (LLMs) has revolutionized\n","numerous areas of artificial intelligence, particularly in the domain of mathematical problem\n","solving (DeepSeek-AI, 2025). These advancements are largely enabled by the paradigm of\n","inference-time scaling, most notably through natural language chain-of-thought reasoning\n","(Jaech et al., 2024). Rather than relying solely on a single forward pass to arrive at an answer,\n","LLMs can reflect on intermediate reasoning steps, improving both accuracy and interpretability. Despite the success of natural language reasoning in solving competition-level mathematical\n","problems, its application to formal theorem proving remains fundamentally challenging. LLMs\n","perform natural language reasoning in an inherently informal manner, relying on heuristics, ap-\n","proximations, and data-driven guessing patterns that often lack the rigorous structure required\n","by formal verification systems. In contrast, proof assistants such as Lean (Moura and Ullrich,\n","2021), Isabelle (Paulson, 1994), and Coq (Barras et al., 1999) operate on strict logical foundations,\n","where every proof step must be explicitly constructed and formally verified. These systems\n","permit no ambiguity, implicit assumptions, or omission of details. Bridging the gap between\n","informal, high-level reasoning and the syntactic rigor of formal verification systems remains a\n","longstanding research challenge in neural theorem proving (Yang et al., 2024). To harness the strengths of informal mathematical reasoning in support of formal theorem\n","proving, a classical approach is to hierarchically decompose formal proofs based on the guidance\n","of natural-language proof sketches.'\n"]}]},{"cell_type":"code","source":["print(semantic_chunks[3])"],"metadata":{"id":"0yU0gQhArLIQ","executionInfo":{"status":"ok","timestamp":1747855344290,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"6298e2a6-39c2-4e21-bfd8-7d2325e1172d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='Jiang et al. (2023) proposed a framework, calledDraft, Sketch,\n","and Prove (DSP), that leverages a large language model to generate proof sketches in natural\n","language, which are subsequently translated into formal proof steps. This informal-to-formal\n","theorem proving paradigm closely mirrors the concept of subgoals in hierarchical reinforcement\n","learning (Barto and Mahadevan, 2003; Nachum et al., 2018; Eppe et al., 2022), where complex\n","tasks are broken down into a hierarchy of simpler subtasks that can be solved independently\n","to progressively achieve the overarching objective. In formal theorem proving, a subgoal\n","is typically an intermediate proposition or lemma that contributes to the proof of a larger\n","theorem (Zhao et al., 2023, 2024). This hierarchical decomposition aligns with human problem-\n","solving strategies and supports modularity, reusability, and more efficient proof search (Wang\n","et al., 2024b; Zheng et al., 2024).'\n"]}]}]}