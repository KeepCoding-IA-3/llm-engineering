{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1EWsq0BGumwOSrtJsnCcwG96YMBnEFUak","timestamp":1710591266809},{"file_id":"1usxOzWwLW9FDQxPetWi62PdRvx_JnMpJ","timestamp":1705021623211},{"file_id":"1FgUKk5TNcwTe2J4GJdTF04nVKFXKFlP5","timestamp":1705015141107}],"collapsed_sections":["eo_QP1ITFfX2"],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Unsupervised Pre-Training of GPT-Style Model\n","\n","En este laboratorio veremos un ejemplo de cómo crear un modelo tipo GPT utilizando el método de preentrenamiento no supervisado.\n","\n","Utilizaremos el modelo de código abierto de Andrej Karpathy [nanoGPT](https://github.com/karpathy/nanoGPT).\n","\n","El código del modelo está en el repositorio de nanoGPT, específicamente en [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py)"],"metadata":{"id":"UWiGVj6njoDn"}},{"cell_type":"markdown","source":["## Obtener el dataset\n","\n","Para entrenar estos tipos de modelos de Deep Learning se necesitan Corpus de texto muy grandes.\n","\n","> NOTA: El entrenamiento de GPT-2 está explicado en este paper: [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Recordemos los desafíos computacionales, para alcanzar los mismos resultados de GPT-2, necesitaríamos 8xA100s por unos 4 o 5 días utilizando la técnica de DDP en el corpus de [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)\n","\n","Nosotros utilizaremos un corpus más pequeño que se usa a menudo en tareas de entrenamiento de RNNs: `tinyshakespeare`\n"],"metadata":{"id":"eHi04aEnkKEZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMRsEQZy6tgc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369499584,"user_tz":-60,"elapsed":2196,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"53fe2502-7a1c-49d8-9668-7b24f04ee86e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nanoGPT'...\n","remote: Enumerating objects: 686, done.\u001b[K\n","remote: Counting objects: 100% (2/2), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 684 (from 2)\u001b[K\n","Receiving objects: 100% (686/686), 959.50 KiB | 4.05 MiB/s, done.\n","Resolving deltas: 100% (385/385), done.\n"]}],"source":["!git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"code","source":["!pip install tiktoken requests cohere openai -qU"],"metadata":{"id":"d_gepPv1Qdj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369513085,"user_tz":-60,"elapsed":13513,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"9efe28ec-e059-4799-9c35-067160dda7cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Descargamos el dataset y lo separamos en dos; `train_data` y `val_data`"],"metadata":{"id":"70hSjXmZmCt3"}},{"cell_type":"code","source":["import os\n","import requests\n","import tiktoken\n","import numpy as np\n","\n","# Definir la ruta donde se guardarán los datos de Shakespeare.\n","current_path = \"/data/shakespeare\"\n","# URL desde donde se descargará el conjunto de datos \"Tiny Shakespeare\".\n","data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","\n","# Comprobar si la carpeta existe, si no, la creamos\n","if not os.path.exists(current_path):\n","    os.makedirs(current_path)\n","\n","# Descargar el dataset\n","input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n","if not os.path.exists(input_file_path):\n","\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","\n","# Calcular la longitud de los datos y dividirlos en datos de entrenamiento y validación.\n","n = len(data)\n","train_data = data[:int(n*0.9)]  #90%\n","val_data = data[int(n*0.9):]    #10%\n"],"metadata":{"id":"T7qRWArUNiZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tokenizers -qU"],"metadata":{"id":"gFnrwKpQPsYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-tokenización y BPE\n","\n","El Byte-Pair Encoding (BPE) es una técnica de tokenización que combina pares de caracteres o palabras frecuentes para formar un vocabulario eficiente. Se inicia con la pre-tokenización y se repite el proceso de combinar los pares más frecuentes hasta alcanzar un tamaño de vocabulario deseado. Este método permite manejar de manera equilibrada palabras comunes y raras, adaptándose dinámicamente al texto.\n","\n","En el caso de GPT-2, BPE se utilizó dividiendo el texto en espacios al principio, tratando cada palabra como una unidad y luego aplicando BPE para formar tokens más complejos a partir de combinaciones frecuentes de caracteres o subpalabras. Esto permitió al GPT-2 manejar eficazmente un amplio rango de vocabulario con un número limitado de tokens, equilibrando la cobertura del vocabulario y la eficiencia del modelo.\n"],"metadata":{"id":"GLecDiHbogvX"}},{"cell_type":"code","source":["input_text = \"\"\"\n","Many that live deserve death. And some that die deserve life. Can you give it to them? Then do not be too eager to deal out death in judgement. for even the very wise cannot see all ends\n","\"\"\"\n","\n","# Separamos el texto en dos\n","naive_word_list = input_text.split()"],"metadata":{"id":"m34NDAGCpiz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Contamos la frequencia de cada palabra"],"metadata":{"id":"hR8k-2bopqjy"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","vocab_and_frequencies = defaultdict(int)\n","\n","# Para cada palabra en la lista de palabras \"naive_word_list\"\n","for word in naive_word_list:\n","  # Incrementamos el contador de frecuencia de cada palabra, separando las letras con espacios\n","  vocab_and_frequencies[\" \".join(list(word))] += 1\n","\n","# Ordenamos los elementos del diccionario por su frecuencia en orden descendente y tomamos los primeros 5\n","sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)"],"metadata":{"id":"VCAk__xQviDU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369671522,"user_tz":-60,"elapsed":5,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"1230f958-e230-4774-a61a-7eb9c6b0d547"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('t h a t', 2),\n"," ('d e s e r v e', 2),\n"," ('t o', 2),\n"," ('M a n y', 1),\n"," ('l i v e', 1),\n"," ('d e a t h .', 1),\n"," ('A n d', 1),\n"," ('s o m e', 1),\n"," ('d i e', 1),\n"," ('l i f e .', 1),\n"," ('C a n', 1),\n"," ('y o u', 1),\n"," ('g i v e', 1),\n"," ('i t', 1),\n"," ('t h e m ?', 1),\n"," ('T h e n', 1),\n"," ('d o', 1),\n"," ('n o t', 1),\n"," ('b e', 1),\n"," ('t o o', 1),\n"," ('e a g e r', 1),\n"," ('d e a l', 1),\n"," ('o u t', 1),\n"," ('d e a t h', 1),\n"," ('i n', 1),\n"," ('j u d g e m e n t .', 1),\n"," ('f o r', 1),\n"," ('e v e n', 1),\n"," ('t h e', 1),\n"," ('v e r y', 1),\n"," ('w i s e', 1),\n"," ('c a n n o t', 1),\n"," ('s e e', 1),\n"," ('a l l', 1),\n"," ('e n d s', 1)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Contamos cuantas palabras tenemos de vocabulario\n"],"metadata":{"id":"NckufSxxp-w5"}},{"cell_type":"code","source":["from typing import Dict, Tuple, List, Set\n","\n","def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n","  vocab = set()\n","\n","  for word in current_vocab.keys():\n","    # Separamos la palabra en subpalabras si es necesario y las añadimos al conjunto\n","    for subword in word.split():\n","      vocab.add(subword)\n","\n","  # Retornamos el tamaño del conjunto, que es el número de palabras únicas\n","  return len(vocab)"],"metadata":{"id":"BNcjzjDvvKjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_vocabulary_size(vocab_and_frequencies)"],"metadata":{"id":"pf3kCf-WvdBL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369732815,"user_tz":-60,"elapsed":398,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"a7cc822b-eb3b-45a4-9836-8e76296e68d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Ahora ya podemos empezar a construir nuestros \"pares\". Consideraremos su frecuencia dentro del corpus."],"metadata":{"id":"OGxrHYmftDTr"}},{"cell_type":"code","source":["def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  pairs = {}\n","\n","  # Iteramos sobre cada palabra y su frecuencia en el vocabulario actual\n","  for word, frequency in current_vocab.items():\n","    symbols = word.split()  # Separamos la palabra en símbolos (palabras) usando espacios\n","\n","    # Creamos pares de símbolos consecutivos y actualizamos su frecuencia\n","    for i in range(len(symbols) - 1):  # Se resta 1 para evitar un error de índice fuera de rango\n","      pair = (symbols[i], symbols[i + 1])  # Formamos el par actual\n","      current_frequency = pairs.get(pair, 0)  # Obtenemos la frecuencia actual del par, 0 si no existe\n","      pairs[pair] = current_frequency + frequency  # Actualizamos la frecuencia del par con la frecuencia de la palabra\n","\n","  return pairs\n"],"metadata":{"id":"sTwvfTAErQN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"],"metadata":{"id":"FudOaKmYv9-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"id":"UxpIfDKXvmGA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369839107,"user_tz":-60,"elapsed":383,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"df02f7f5-544b-4530-943a-33f5b59e6086"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('t', 'h'), 6),\n"," (('v', 'e'), 6),\n"," (('d', 'e'), 5),\n"," (('a', 't'), 4),\n"," (('s', 'e'), 4)]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["¡Ahora que ya tenemos los pares, podemos añadirlos al vocabulario!"],"metadata":{"id":"OqORqdzwsZ6s"}},{"cell_type":"code","source":["import re\n","\n","def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  vocab_out = {}\n","\n","  # Creamos una expresión regular para identificar el par más común dentro de una cadena, escapando los caracteres especiales si los hay.\n","  pattern = re.escape(' '.join(most_common_pair))\n","  # El reemplazo será la concatenación del par sin espacios.\n","  replacement = ''.join(most_common_pair)\n","\n","  for word_in in current_vocab:\n","      # Reemplazamos el par más común encontrado en cada palabra por su versión concatenada.\n","      word_out = re.sub(pattern, replacement, word_in)\n","      # Conservamos la frecuencia original de la palabra en el nuevo vocabulario con la palabra ya procesada.\n","      vocab_out[word_out] = current_vocab[word_in]\n","\n","  return vocab_out\n"],"metadata":{"id":"L7ohHm2kshoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_vocab_and_frequencies = merge_vocab(\n","    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n","    vocab_and_frequencies\n",")"],"metadata":{"id":"Ab760KKuwzZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)"],"metadata":{"id":"zx6Oi7kLvqOx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716839937661,"user_tz":-120,"elapsed":211,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"b814c6a9-57b1-48e2-d7f7-77b680d75956"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('th a t', 2),\n"," ('d e s e r v e', 2),\n"," ('t o', 2),\n"," ('M a n y', 1),\n"," ('l i v e', 1),\n"," ('d e a th .', 1),\n"," ('A n d', 1),\n"," ('s o m e', 1),\n"," ('d i e', 1),\n"," ('l i f e .', 1),\n"," ('C a n', 1),\n"," ('y o u', 1),\n"," ('g i v e', 1),\n"," ('i t', 1),\n"," ('th e m ?', 1),\n"," ('T h e n', 1),\n"," ('d o', 1),\n"," ('n o t', 1),\n"," ('b e', 1),\n"," ('t o o', 1),\n"," ('e a g e r', 1),\n"," ('d e a l', 1),\n"," ('o u t', 1),\n"," ('d e a th', 1),\n"," ('i n', 1),\n"," ('j u d g e m e n t .', 1),\n"," ('f o r', 1),\n"," ('e v e n', 1),\n"," ('th e', 1),\n"," ('v e r y', 1),\n"," ('w i s e', 1),\n"," ('c a n n o t', 1),\n"," ('s e e', 1),\n"," ('a l l', 1),\n"," ('e n d s', 1)]"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Vemos que `t h` se ha convertido en `th`!\n","\n","¿Cómo impacta en nuestro vocabulario?"],"metadata":{"id":"9DPkBzj2u-me"}},{"cell_type":"code","source":["find_vocabulary_size(new_vocab_and_frequencies)"],"metadata":{"id":"bO_xegCtxjQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736369879453,"user_tz":-60,"elapsed":403,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"aead24ef-1524-4f92-a824-cdc7461e9bff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Nuestro vocabulario ha aumentado en 1.\n","\n","La idea es que BPE seguirá haciendo este proceso hasta encontrar un vocabulario de la longitud que le pedimos."],"metadata":{"id":"o3M13D60xzZi"}},{"cell_type":"markdown","source":["## Entrenar el Tokenizador\n","\n","Ahora que ya hemos visto cómo funciona `BPE`, ¡podemos entrenar nuestro propio Tokenizador!\n","\n","Utilizaremos la librería `Tokenizer` de Hugging Face con el modelo BPE inicializando el token \"Desconocido\"\n","\n","Añadiremos un normalizador de código Unicode. Puedes leer más sobre esto en: [link](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n","\n","También añadiremos un codificador y un decodificador a nivel de bytes.\n","\n","  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n","  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n","  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n","  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n","  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)\n"],"metadata":{"id":"BePYCbHly02H"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.normalizers import NFD, Sequence\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n","tokenizer.normalizer = Sequence([NFD()])\n","tokenizer.pre_tokenizer = ByteLevel()\n","tokenizer.decoder = ByteLevelDecoder()"],"metadata":{"id":"OrztE09OPosB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Necesitamos añadir algunos tokens especiales a nuestro tokenizador para asegurarnos de que tenga acceso a patrones de tokens comunes.\n","\n","Usamos los siguientes:\n","\n","- `\"<s>\"`    : bos_token - token de inicio de secuencia\n","- `\"</s>\"`   : eos_token - token de final de secuencia\n","- `\"<pad>\"`  : padding_token - token utilizado para rellenar secuencias\n","- `\"<unk>\"`  : unk_token - token utilizado para representar tokens desconocidos\n","- `\"<mask>\"` : mask_token - token utilizado para enmascarar partes de nuestra secuencia\n","\n","También estableceremos un vocabulario objetivo de 50,000 tokens."],"metadata":{"id":"dDqkNNdM1KsD"}},{"cell_type":"code","source":["trainer = BpeTrainer(\n","    vocab_size=50000,\n","    show_progress=True,\n","    special_tokens=[\n","      \"<s>\",\n","      \"<pad>\",\n","      \"</s>\",\n","      \"<unk>\",\n","      \"<mask>\"\n","    ]\n",")"],"metadata":{"id":"x9iQVhN3P3RN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ya solo queda el entrenamiento. Usaremos la función `train()` que encontraréis en la documentación en: [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"],"metadata":{"id":"yQ8X9vZe2Fyw"}},{"cell_type":"code","source":["tokenizer.train(files=[input_file_path], trainer=trainer)"],"metadata":{"id":"LinLHotSP7gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ya tenemos el tokenizer preparado, y podemos cargarlo como si fuera el Tokenizer de GPT2 😀"],"metadata":{"id":"V2JNYiqB2qKV"}},{"cell_type":"code","source":["save_path = '/content/tokenizer'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","tokenizer.model.save(save_path)"],"metadata":{"id":"Jk6QjDGHQy2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370041573,"user_tz":-60,"elapsed":442,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"d7113aca-5775-43d6-f8a9-df2404e8fa92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["!pip install transformers -qU"],"metadata":{"id":"cOOlbggdRFrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"],"metadata":{"id":"us1vofdhQ45C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_sentence = \"Many that live deserve death. And some that die deserve life. Can you give it to them? Then do not be too eager to deal out death in judgement. for even the very wise cannot see all ends\""],"metadata":{"id":"dnYnFa3fTRLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_sentence = tokenizer.tokenize(input_sentence)\n","tokenized_sentence"],"metadata":{"id":"bn7Yygw_vw7z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370110214,"user_tz":-60,"elapsed":401,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"3a4096e5-a147-4da5-d9a0-9a5b6547e176"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['M',\n"," 'any',\n"," 'Ġthat',\n"," 'Ġlive',\n"," 'Ġdeserve',\n"," 'Ġdeath',\n"," '.',\n"," 'ĠAnd',\n"," 'Ġsome',\n"," 'Ġthat',\n"," 'Ġdie',\n"," 'Ġdeserve',\n"," 'Ġlife',\n"," '.',\n"," 'ĠCan',\n"," 'Ġyou',\n"," 'Ġgive',\n"," 'Ġit',\n"," 'Ġto',\n"," 'Ġthem',\n"," '?',\n"," 'ĠThen',\n"," 'Ġdo',\n"," 'Ġnot',\n"," 'Ġbe',\n"," 'Ġtoo',\n"," 'Ġeager',\n"," 'Ġto',\n"," 'Ġdeal',\n"," 'Ġout',\n"," 'Ġdeath',\n"," 'Ġin',\n"," 'Ġjudgement',\n"," '.',\n"," 'Ġfor',\n"," 'Ġeven',\n"," 'Ġthe',\n"," 'Ġvery',\n"," 'Ġwise',\n"," 'Ġcannot',\n"," 'Ġsee',\n"," 'Ġall',\n"," 'Ġends']"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","encoded_tokens"],"metadata":{"id":"uVM2U7m9vzWz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370128161,"user_tz":-60,"elapsed":442,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"253ca61c-ac63-48f9-dce6-a7d9e82bbebe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[28,\n"," 1445,\n"," 144,\n"," 769,\n"," 3437,\n"," 426,\n"," 11,\n"," 148,\n"," 463,\n"," 144,\n"," 781,\n"," 3437,\n"," 520,\n"," 11,\n"," 1523,\n"," 104,\n"," 507,\n"," 160,\n"," 103,\n"," 311,\n"," 15,\n"," 711,\n"," 214,\n"," 140,\n"," 121,\n"," 420,\n"," 6695,\n"," 103,\n"," 3144,\n"," 437,\n"," 426,\n"," 125,\n"," 16910,\n"," 11,\n"," 149,\n"," 1278,\n"," 80,\n"," 681,\n"," 2105,\n"," 632,\n"," 400,\n"," 227,\n"," 3952]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n","decoded_tokens"],"metadata":{"id":"oS6lE-NLRnzk","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1736370140595,"user_tz":-60,"elapsed":2797,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"fcf3a646-0490-40b1-c680-e186674486bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Many that live deserve death. And some that die deserve life. Can you give it to them? Then do not be too eager to deal out death in judgement. for even the very wise cannot see all ends'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Tokenizamos el dataset\n","\n","Necesitamos tokenizar el dataset de Shakespeare para poder utilizarlo con `nanoGPT`.\n","\n","Codificaremos el dataset de entrenamiento y validación, los guardaremos en binario para utilizarlos más tarde.\n"],"metadata":{"id":"ji3sF-rA21YH"}},{"cell_type":"code","source":["print(train_data[0:100])\n","train_ids = tokenizer.encode(train_data)\n","val_ids = tokenizer.encode(val_data)\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")"],"metadata":{"id":"RSOQAVThv9ZO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370153434,"user_tz":-60,"elapsed":2688,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"b7c4a9a3-d461-4589-9668-f211439fe049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","train has 291,284 tokens\n","val has 34,223 tokens\n"]}]},{"cell_type":"code","source":["# export to bin files\n","data_path = \"/data/shakespeare/\"\n","\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n","val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"],"metadata":{"id":"nKJ1KqiiPkRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["100 primeros registros..."],"metadata":{"id":"DFbbvIi7xsgr"}},{"cell_type":"code","source":["train_ids[:100]"],"metadata":{"id":"AbHhLjtrwALN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370178334,"user_tz":-60,"elapsed":383,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"74283ce4-1c49-4810-adc9-5860cd61a52a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n","          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n","         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n","         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n","       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n","         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n","        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n","       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n","          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n","         68], dtype=uint16)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["tokenizer.decode(train_ids[:100], clean_up_tokenization_spaces=False)"],"metadata":{"id":"xduulkdGwCjW","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1736370181780,"user_tz":-60,"elapsed":506,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"43affb35-2719-4d53-fd80-eb37ecd4e82c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Entrenando el modelo\n","\n","Usaremos `nanoGPT`. Si quieres ver cómo han construido la arquitectura, entra al repositorio y mira el archivo `model.py`."],"metadata":{"id":"c0I3VrRC3XIO"}},{"cell_type":"code","source":["%cd nanoGPT"],"metadata":{"id":"NUU2jaalUdqm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370206527,"user_tz":-60,"elapsed":469,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"30f583ad-dbb7-4682-8bd1-37d8613b3ab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanoGPT\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","\n","# from the local repo\n","from model import GPTConfig, GPT"],"metadata":{"id":"weNR37BwUYNg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hiperparametros"],"metadata":{"id":"kY_vWZG-3uM-"}},{"cell_type":"markdown","source":["#### I/O\n","\n","- `out_dir` - directorio de salida donde se guardan nuestros puntos de control\n"],"metadata":{"id":"OykCjVQK5EX-"}},{"cell_type":"code","source":["out_dir = 'out'"],"metadata":{"id":"viM3qlWt5PVS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inicialización\n","\n","Como estamos entrenando desde cero, utilizaremos init_from = 'scratch'.\n"],"metadata":{"id":"A5iwwrNL5H4C"}},{"cell_type":"code","source":["init_from = 'scratch'"],"metadata":{"id":"OK1z2m3C312T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluación y Registro\n","\n","- `eval_interval` - número de pasos entre las etapas de evaluación, alrededor de 250 está bien.\n","- `log_interval` - frecuencia con la que se registrará nuestro progreso de entrenamiento. Puedes establecer esto a aproximadamente 10.\n","- `eval_iters` - cuántas iteraciones queremos evaluar.\n","- `eval_only` - evaluación de nuestro modelo.\n","- `always_save_checkpoint` - guardará nuestro punto de control más reciente, independientemente de las métricas."],"metadata":{"id":"2YlolKOj4_dE"}},{"cell_type":"code","source":["eval_interval = 250\n","eval_iters = 200\n","log_interval = 10\n","eval_only = False\n","always_save_checkpoint = True"],"metadata":{"id":"MbFN5Ltq4_mo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Conjunto de Datos\n","\n","Podemos establecer nuestro conjunto de datos aquí - ¡usaremos el que creamos anteriormente!\n"],"metadata":{"id":"a488zaF_4zQk"}},{"cell_type":"code","source":["dataset = 'shakespeare'"],"metadata":{"id":"_QC7vWXC40Hp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Parámetros Hiper-Típicos\n","\n","- `gradient_accumulation_steps` - podemos utilizar la acumulación de gradientes para \"simular\" tamaños de lote más grandes combinando varios pasos de optimización diferentes juntos, sin necesitar la memoria adicional para tamaños de lote grandes. No hay que preocuparse tanto por esto ya que nuestro entrenamiento es corto, pero este hiperparámetro se puede configurar para ejecuciones de entrenamiento más grandes. Aquí hay una buena lectura sobre el tema. [Lectura](https://lightning.ai/blog/gradient-accumulation/)\n","- `batch_size` - Tamaño típico del lote - cuanto mayor mejor (hasta cierto punto) utilizaremos 16 para asegurarnos de que no superamos el límite de memoria de nuestra GPU.\n","- `block_size` - esto se puede considerar como otro término para la ventana de contexto de nuestro modelo. Ya que nuestro modelo no puede tomar entradas de longitud variable - utilizamos esto para establecer todas las entradas al tamaño deseado. Utilizaremos un valor de 512 para asegurar un entrenamiento rápido.\n"],"metadata":{"id":"XP9rBgGc426Q"}},{"cell_type":"code","source":["gradient_accumulation_steps = 1\n","batch_size = 16\n","block_size = 512"],"metadata":{"id":"EM_ybLPP43Pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Arquitectura del Modelo\n","\n","- `n_layer` - Este es el número de capas de decodificador que usaremos en nuestro modelo. Más sería considerado mejor (hasta cierto punto) y el documento original de GPT-2 utiliza `12`, pero nosotros estaremos usando un truncado `6` por facilidad y velocidad de entrenamiento.\n","- `n_head` - Este es el número de cabezas de atención en cada capa de decodificador.\n","- `n_embd` - Esta es la dimensión de embedding de nuestro modelo, esto es análogo a nuestro `model_d` del cuaderno anterior.\n","- `dropout` - Esto establece nuestro valor de dropout, ya que nuestro modelo es pequeño y va a ser extremadamente propenso a sobreajustarse, considera establecer esto en un nivel bastante agresivo (`0.2` se usó en el entrenamiento de ejemplo encontrado en el cuaderno).\n","- `bias` - Si usar o no sesgo dentro de las capas LayerNorm/Lineal.\n","\n","> NOTA: Necesitas asegurarte de que tu `n_embd` se divida limpiamente por tu `n_head`. Es decir:\n",">\n","> `n_embd % n_head == 0`.\n"],"metadata":{"id":"UZ-8bDIY45GS"}},{"cell_type":"code","source":["n_layer = 6\n","n_head = 6\n","n_embd = 516\n","dropout = 0.2\n","bias = False"],"metadata":{"id":"gMyyDBxB6k4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####❓Pregunta:\n","\n","¿Cuántas cabezas de atención (total) tendrá nuestra red final?\n"],"metadata":{"id":"-ynYYq-V_zwa"}},{"cell_type":"markdown","source":["#### Hiperparámetros del Optimizador\n","\n","Hiperparámetros Básicos del Optimizador:\n","\n","- `learning_rate` - ¡es nuestra tasa de aprendizaje! Alta cuando el dataset es pequeño, baja cuando el dataset es grande.\n","- `max_iters` - cuántas iteraciones entrenamos. Más iteraciones significan más tiempo de entrenamiento.\n","\n","Configuración de Decaimiento de la Tasa de Aprendizaje:\n","\n","- `decay_lr` - bandera de decaimiento\n","- `weight_Decay` - Cuánto puede ser la pérdida\n","- `lr_decay_iters` - debería establecerse en ~max_iters.\n","- `min_lr` - la tasa de aprendizaje mínima, debería ser ~ lr / 10\n","\n","Recorte y Calentamiento:\n","\n","- `grad_clip` - valor para recortar los gradientes. Útil para prevenir gradientes que desaparecen.\n","- `warmup_iters` - cuántas iteraciones para el calentamiento. El calentamiento es útil para permitir que tu entrenamiento se caliente lentamente. Utilizará una tasa de aprendizaje baja por un número de pasos para evitar cualquier pico inicial masivo. Ya que estamos entrenando un modelo muy pequeño - podemos evitar usar muchos pasos de calentamiento.\n","\n","> TIP: Elegir bien estos parámetros no es fácil. Tienen mucho que ver con la escala Chinchilla. Mirar el [Chinchilla Paper](https://arxiv.org/pdf/2203.15556.pdf) siempre es una buena opción...\n"],"metadata":{"id":"3NWDTaAz7gwh"}},{"cell_type":"code","source":["# adamw optimizer\n","learning_rate = 1e-3\n","max_iters = 5_000\n","beta1 = 0.9\n","beta2 = 0.99\n","\n","# lr decay settings\n","decay_lr = True\n","weight_decay = 1e-1\n","lr_decay_iters = 5_000\n","min_lr = 1e-4\n","\n","# clipping and warmup\n","grad_clip = 1.0\n","warmup_iters = 100"],"metadata":{"id":"qe-669jwUptI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Estos hiperparámetros son necesarios para establecer dada la tarea que estamos entrenando y el entorno en el cual nos encontramos entrenando."],"metadata":{"id":"ucldc4mz9yeT"}},{"cell_type":"code","source":["backend = 'nccl'\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n","compile = True\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","# -----------------------------------------------------------------------------\n","master_process = True\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","os.makedirs(out_dir, exist_ok=True)"],"metadata":{"id":"xHiGlMOp8Nux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370417089,"user_tz":-60,"elapsed":408,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"bf9f6ec8-7d0d-4e57-d32d-40dee0409818"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tokens per iteration will be: 8,192\n"]}]},{"cell_type":"markdown","source":["### Torch Settings\n","\n","Necesitamos establecer algunos ajustes de PyTorch, incluyendo la semilla, para permitirnos entrenar correctamente en nuestra GPU.\n","\n","No es necesario entender mucho aquí - estas son solo líneas de código necesarias. Plantilla."],"metadata":{"id":"eKmdfbye-BNf"}},{"cell_type":"code","source":["torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"],"metadata":{"id":"yh34QGD6VARU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataloader\n","\n","Este bloque:\n","\n","1. Establecerá la ruta de los datos.\n","2. Cargará el conjunto de datos que tokenizamos anteriormente desde el .bin que guardamos.\n","3. Definirá una función get_batch que nos retornará una sección aleatoria de nuestros datos así como la etiqueta correspondiente para esos datos y los moverá al GPU para un uso fácil dentro de nuestro bucle de entrenamiento."],"metadata":{"id":"gKeNwYaZ-Zoc"}},{"cell_type":"code","source":["data_dir = os.path.join('/data', dataset)\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y"],"metadata":{"id":"tOjaPyJpVEgx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Echemos un vistazo a cómo sería un ejemplo de nuestros lotes.\n","\n","Para recordar:\n","\n","- `train_data` - tiene ~2,9 millones de entradas\n","- `block_size` - es 512\n","- `batch_size` - es 16"],"metadata":{"id":"1Z7vMU34yRbq"}},{"cell_type":"code","source":["ix = torch.randint(len(train_data) - block_size, (batch_size,))\n","x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n","y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"],"metadata":{"id":"9_-Y5RZ-yX2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Our randomly selected indices were: {ix}\")\n","print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")\n","print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"],"metadata":{"id":"V1OceqT2wNjn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370438025,"user_tz":-60,"elapsed":2,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"60079231-48c8-4b24-cb3e-dc807b574fc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n","        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n","The first 10 elements of `x` at the first randomly selected index is:\n","tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n","The first 10 elements of `y` at the first randomly selected index is:\n","tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"]}]},{"cell_type":"markdown","source":["### Inicialización Sencilla del Modelo\n","\n","Aquí iniciamos nuestro número de iteraciones en 0 y nuestra mejor pérdida de valor como un número muy alto.\n"],"metadata":{"id":"EbDlW-68_atH"}},{"cell_type":"code","source":["iter_num = 0\n","best_val_loss = 1e9"],"metadata":{"id":"6hsepdVBVzQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Necesitamos el tamaño del vocabulario de nuestro tokenizador.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"A4Uj9qBI_vXc"}},{"cell_type":"code","source":["meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = tokenizer.vocab_size\n","meta_vocab_size"],"metadata":{"id":"m53DcCdFV0_a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370447275,"user_tz":-60,"elapsed":402,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"93ae9bb6-29d2-4388-b2ac-be5a71c95d5e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20099"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout)"],"metadata":{"id":"JfIWEbanV7ZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Iniciamos el modelo a partir de los argumentos previos.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"2WWcbkiCAUI2"}},{"cell_type":"code","source":["if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)"],"metadata":{"id":"1Xly4iA0V-vF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370463763,"user_tz":-60,"elapsed":1462,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"45b21f60-1353-48d6-e071-c76db5d3c666"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing a new model from scratch\n","number of parameters: 29.55M\n"]}]},{"cell_type":"markdown","source":["Si han utilizado los valores predeterminados, deberían tener un modelo con 29,55 millones de parámetros.\n","\n","Establecemos nuestro block_size al tamaño correcto tal como se determina en nuestros pasos de configuración."],"metadata":{"id":"BpViOsxLAl6p"}},{"cell_type":"code","source":["if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size"],"metadata":{"id":"TrEawNxdWRhm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora ya podemos ver el Modelo en su totalidad:\n","\n","\n","\n","\n","\n"],"metadata":{"id":"eRgguPLKAuZ5"}},{"cell_type":"code","source":["model.to(device)"],"metadata":{"id":"Ih7ZQDd6wU9Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370482474,"user_tz":-60,"elapsed":385,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"7dd997fa-7c70-42a6-deac-f3e65f9264c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(20099, 516)\n","    (wpe): Embedding(512, 516)\n","    (drop): Dropout(p=0.2, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln_1): LayerNorm()\n","        (attn): CausalSelfAttention(\n","          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n","          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n","          (attn_dropout): Dropout(p=0.2, inplace=False)\n","          (resid_dropout): Dropout(p=0.2, inplace=False)\n","        )\n","        (ln_2): LayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm()\n","  )\n","  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",")"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["Vamos a nombrar algunas cosas que hemos visto:\n","\n","- Normalización de Capa: `ln_f`\n","- Proyección Lineal para Salida: `lm_head`\n","- Mecanismo de Atención: `attn`\n","- Codificación Posicional: `wpe`\n","- Incrustaciones: `wte`\n","- Red de Avance Lineal (Feed Forward Network): `mlp`\n"],"metadata":{"id":"oHVw3KF3uvIA"}},{"cell_type":"markdown","source":["Configuraremos nuestro GradScaler: más información sobre este proceso [aquí](https://pytorch.org/docs/stable/amp.html#gradient-scaling).\n"],"metadata":{"id":"LzoEY6gcBOSp"}},{"cell_type":"code","source":["scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"],"metadata":{"id":"BNUThRt4WT5H","executionInfo":{"status":"ok","timestamp":1736370527084,"user_tz":-60,"elapsed":420,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa6d0098-a75a-4808-ed36-51c5286d76c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-51-6801aec47c83>:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"]}]},{"cell_type":"markdown","source":["Configuraremos nuestro optimizador a continuación. Asegúrate de incluir los valores correctos. Puedes consultar el archivo `model.py` para obtener más información sobre lo que se espera en el método `configure_optimizers` [aquí](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08#L26363).\n"],"metadata":{"id":"6Zs5Hcf9BBUD"}},{"cell_type":"code","source":["optimizer = model.configure_optimizers(\n","    weight_decay,\n","    learning_rate,\n","    (beta1, beta2),\n","    device_type\n",")\n","\n","checkpoint = None"],"metadata":{"id":"YesGeUnoWViL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370542668,"user_tz":-60,"elapsed":395,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"ee5a1885-9d24-4178-8f27-3fddf2eb1fbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 26, with 29,805,708 parameters\n","num non-decayed parameter tensors: 13, with 6,708 parameters\n","using fused AdamW: True\n"]}]},{"cell_type":"markdown","source":["Ahora podemos compilar nuestro modelo!\n","\n","Si utilizas la instancia T4 o V100 de Colab, esto no proporcionará una aceleración significativa, pero si usas la arquitectura Ampere (A100), deberías notar una diferencia significativa entre el modelo compilado y no compilado.\n","\n","Lee más sobre `torch.compile()` [aquí](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).\n"],"metadata":{"id":"ZF5YWJoKB4og"}},{"cell_type":"code","source":["if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0"],"metadata":{"id":"v0FNU0T0WXdI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370581438,"user_tz":-60,"elapsed":2175,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"9a5b0c76-f65f-4e4d-9440-7712f20fd083"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["compiling the model... (takes a ~minute)\n"]}]},{"cell_type":"markdown","source":["Aquí configuraremos nuestra función de estimación de pérdidas, que nos ayudará a estimar una pérdida arbitrariamente precisa sobre los datos de entrenamiento o de validación mediante el uso de muchos lotes.\n","\n","Notaréis que rápidamente convertimos el modelo al modelo `.eval()` y luego volvemos al modo `.train()`.\n"],"metadata":{"id":"p6lRcVsZCXRO"}},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"lUB5zVLVWbhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creando nuestro Planificador de LR\n","\n","Más allá de reducir lentamente nuestra tasa de aprendizaje a lo largo del tiempo, podemos usar un planificador de LR que nos permita mover nuestro aprendizaje según un patrón deseado.\n","\n","Utilizaremos un planificador \"warmup cosines\" y nuestra tasa de aprendizaje será la siguiente:\n","\n","![img](https://i.imgur.com/KoFEl0b.png)\n","\n","Hay muchos planificadores diferentes y muchas maneras diferentes de gestionar la tasa de aprendizaje, más info [aquí](https://d2l.ai/chapter_optimization/lr-scheduler.html)!\n"],"metadata":{"id":"fLsOpaACDDkF"}},{"cell_type":"code","source":["def get_lr(it):\n","    # 1) Calentamiento lineal durante `warmup_iters` pasos.\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters  # Calcula un learning rate proporcional al número de iteraciones.\n","    # 2) Si `it` es mayor que `lr_decay_iters`, retorna el learning rate mínimo.\n","    if it > lr_decay_iters:\n","        return min_lr  # Retorna el valor mínimo del learning rate directamente.\n","    # 3) Entre el período de calentamiento y el punto de decaimiento, utiliza el decaimiento cósmico hasta el learning rate mínimo.\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)  # Calcula la proporción del decaimiento.\n","    assert 0 <= decay_ratio <= 1  # Asegura que el ratio de decaimiento esté dentro del rango válido [0, 1].\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # El coeficiente varía entre 0 y 1\n","    return min_lr + coeff * (learning_rate - min_lr)  # Calcula el learning rate actual basándose en el coeficiente y la diferencia entre el learning rate máximo y mínimo.\n"],"metadata":{"id":"7-mNpWBSWdHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###❓Pregunta:\n","\n","¿Qué ventajas tiene un planificador de aprendizaje respecto a una tasa de aprendizaje estática?\n","\n","Conceptualmente, un planificador de aprendizaje comienza con grandes ajustes para comprender conceptos básicos y, a medida que se produce la mejora, los ajustes se hacen más pequeños y más precisos. Esto es similar a aprender a tocar un instrumento musical, donde inicialmente se avanza significativamente en los aspectos más amplios y, a medida que adquiere más conocimientos, comienza a perfeccionar y afinar tus habilidades.\n","\n","[Using theoretically computed adaptive learning rates for fast convergence](https://arxiv.org/pdf/1902.07399.pdf)\n"],"metadata":{"id":"UV0qN0fDwdOF"}},{"cell_type":"markdown","source":["Necesitamos establecer unos valores específicos en nuestro entorno para permitir el entrenamiento en Colab.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"cqFePCZmE1Lq"}},{"cell_type":"code","source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia"],"metadata":{"id":"CI14YAB6wbmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736370697936,"user_tz":-60,"elapsed":3337,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"6c522f60-2472-487c-f432-cbbdca0086ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"markdown","source":["## The Training Loop (30 min aprox)\n"],"metadata":{"id":"Nhqmxeo0Eg0Z"}},{"cell_type":"code","source":["import torch._dynamo\n","\n","# Enable verbose logging\n","os.environ['TORCH_LOGS'] = '+dynamo'\n","os.environ['TORCHDYNAMO_VERBOSE'] = '1'\n","\n","# Enable suppress_errors to fall back to eager mode\n","torch._dynamo.config.suppress_errors = True"],"metadata":{"id":"nMLykaVrxDzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, Y = get_batch('train')  # Obtiene un lote de datos de entrenamiento.\n","t0 = time.time()  # Marca el inicio del tiempo para esta iteración.\n","local_iter_num = 0  # Inicializa el contador de iteraciones locales.\n","raw_model = model  # Asigna el modelo a una variable para un uso futuro.\n","running_mfu = -1.0  # Inicializa la utilización de FLOPS del modelo a un valor negativo como marcador.\n","\n","while True:\n","    # Determina y establece el learning rate para esta iteración.\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr  # Actualiza el learning rate para cada grupo de parámetros.\n","\n","    # Evalúa la pérdida en los conjuntos de entrenamiento/validación y escribe checkpoints.\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()  # Estima la pérdida.\n","        print(f\"paso {iter_num}: pérdida en entrenamiento {losses['train']:.4f}, pérdida en validación {losses['val']:.4f}\")\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"guardando checkpoint en {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))  # Guarda el checkpoint.\n","    if iter_num == 0 and eval_only:\n","        break  # Salida anticipada si solo se está evaluando.\n","\n","    # Actualización hacia adelante y hacia atrás, con acumulación de gradiente opcional para simular un tamaño de lote más grande.\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:  # Contextualiza la ejecución para el tipo de datos específico (por ejemplo, fp16).\n","            logits, loss = model(X, Y)  # Calcula la pérdida.\n","            loss = loss / gradient_accumulation_steps  # Escala la pérdida para la acumulación de gradiente.\n","        X, Y = get_batch('train')  # Pre-carga del siguiente lote.\n","        scaler.scale(loss).backward()  # Retropropagación con escala de gradiente si es necesario.\n","\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)  # Desescala los gradientes para el clipping.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # Recorte del gradiente.\n","\n","    scaler.step(optimizer)  # Actualiza los parámetros.\n","    scaler.update()  # Actualiza la escala para la próxima iteración.\n","    optimizer.zero_grad(set_to_none=True)  # Limpia los gradientes.\n","\n","    # Temporización y registro.\n","    t1 = time.time()\n","    dt = t1 - t0  # Calcula el tiempo transcurrido.\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        lossf = loss.item() * gradient_accumulation_steps  # Obtiene la pérdida como flotante.\n","        if local_iter_num >= 5:  # Espera a que el bucle de entrenamiento se estabilice un poco.\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu  # Actualiza el MFU corriente.\n","        print(f\"iter {iter_num}: pérdida {lossf:.4f}, tiempo {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","    # Condiciones de terminación.\n","    if iter_num > max_iters:\n","        break  # Finaliza el bucle si se alcanza el número máximo de iteraciones.\n"],"metadata":{"id":"v01E4QA2weFe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23ce8cd8-d427-4ed5-ebaf-fa3c92071267","executionInfo":{"status":"ok","timestamp":1736376946005,"user_tz":-60,"elapsed":6101484,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /content/nanoGPT/model.py line 103 \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] due to: \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _compile(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     super().run()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     while self.step():\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _compile(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     super().run()\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     while self.step():\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n","W0108 21:14:04.868000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /content/nanoGPT/model.py line 26 \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] due to: \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _compile(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     super().run()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     while self.step():\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _compile(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     super().run()\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     while self.step():\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n","W0108 21:14:04.948000 918 torch/_dynamo/convert_frame.py:1125] \n","/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["paso 0: pérdida en entrenamiento 9.9353, pérdida en validación 9.9273\n","iter 0: pérdida 9.9507, tiempo 98354.34ms, mfu -100.00%\n","iter 10: pérdida 8.3544, tiempo 733.11ms, mfu 0.70%\n","iter 20: pérdida 7.4410, tiempo 737.33ms, mfu 0.70%\n","iter 30: pérdida 6.2612, tiempo 739.14ms, mfu 0.70%\n","iter 40: pérdida 5.7856, tiempo 740.43ms, mfu 0.70%\n","iter 50: pérdida 5.7032, tiempo 743.72ms, mfu 0.70%\n","iter 60: pérdida 5.3030, tiempo 744.71ms, mfu 0.70%\n","iter 70: pérdida 5.1779, tiempo 747.92ms, mfu 0.70%\n","iter 80: pérdida 5.1690, tiempo 753.05ms, mfu 0.70%\n","iter 90: pérdida 4.9334, tiempo 749.53ms, mfu 0.70%\n","iter 100: pérdida 4.6415, tiempo 751.52ms, mfu 0.70%\n","iter 110: pérdida 4.8009, tiempo 756.98ms, mfu 0.69%\n","iter 120: pérdida 4.6763, tiempo 757.31ms, mfu 0.69%\n","iter 130: pérdida 4.5699, tiempo 754.77ms, mfu 0.69%\n","iter 140: pérdida 4.5511, tiempo 758.66ms, mfu 0.69%\n","iter 150: pérdida 4.2360, tiempo 752.79ms, mfu 0.69%\n","iter 160: pérdida 4.2941, tiempo 765.37ms, mfu 0.69%\n","iter 170: pérdida 4.3835, tiempo 766.10ms, mfu 0.69%\n","iter 180: pérdida 4.2541, tiempo 768.40ms, mfu 0.69%\n","iter 190: pérdida 4.2651, tiempo 765.96ms, mfu 0.68%\n","iter 200: pérdida 4.1875, tiempo 770.09ms, mfu 0.68%\n","iter 210: pérdida 4.1763, tiempo 767.98ms, mfu 0.68%\n","iter 220: pérdida 4.0762, tiempo 767.32ms, mfu 0.68%\n","iter 230: pérdida 4.0618, tiempo 766.45ms, mfu 0.68%\n","iter 240: pérdida 4.0213, tiempo 769.00ms, mfu 0.68%\n","paso 250: pérdida en entrenamiento 3.9815, pérdida en validación 4.9462\n","guardando checkpoint en out\n","iter 250: pérdida 4.0755, tiempo 109419.02ms, mfu 0.61%\n","iter 260: pérdida 3.9759, tiempo 757.33ms, mfu 0.62%\n","iter 270: pérdida 3.9265, tiempo 766.80ms, mfu 0.62%\n","iter 280: pérdida 3.8915, tiempo 767.16ms, mfu 0.63%\n","iter 290: pérdida 3.9673, tiempo 767.38ms, mfu 0.63%\n","iter 300: pérdida 3.7176, tiempo 770.05ms, mfu 0.64%\n","iter 310: pérdida 3.6808, tiempo 767.56ms, mfu 0.64%\n","iter 320: pérdida 3.8407, tiempo 766.48ms, mfu 0.64%\n","iter 330: pérdida 3.7724, tiempo 773.30ms, mfu 0.65%\n","iter 340: pérdida 3.5678, tiempo 769.09ms, mfu 0.65%\n","iter 350: pérdida 3.6676, tiempo 770.11ms, mfu 0.65%\n","iter 360: pérdida 3.5800, tiempo 771.63ms, mfu 0.65%\n","iter 370: pérdida 3.7492, tiempo 774.10ms, mfu 0.65%\n","iter 380: pérdida 3.5569, tiempo 774.87ms, mfu 0.65%\n","iter 390: pérdida 3.6397, tiempo 771.78ms, mfu 0.66%\n","iter 400: pérdida 3.4350, tiempo 771.50ms, mfu 0.66%\n","iter 410: pérdida 3.5110, tiempo 778.13ms, mfu 0.66%\n","iter 420: pérdida 3.7205, tiempo 768.91ms, mfu 0.66%\n","iter 430: pérdida 3.5473, tiempo 771.23ms, mfu 0.66%\n","iter 440: pérdida 3.5918, tiempo 763.81ms, mfu 0.66%\n","iter 450: pérdida 3.5044, tiempo 763.75ms, mfu 0.66%\n","iter 460: pérdida 3.6102, tiempo 767.53ms, mfu 0.66%\n","iter 470: pérdida 3.4869, tiempo 768.46ms, mfu 0.66%\n","iter 480: pérdida 3.4775, tiempo 767.53ms, mfu 0.67%\n","iter 490: pérdida 3.3372, tiempo 769.37ms, mfu 0.67%\n","paso 500: pérdida en entrenamiento 3.3492, pérdida en validación 5.2430\n","guardando checkpoint en out\n","iter 500: pérdida 3.5201, tiempo 110336.33ms, mfu 0.60%\n","iter 510: pérdida 3.3578, tiempo 763.84ms, mfu 0.61%\n","iter 520: pérdida 3.5376, tiempo 765.57ms, mfu 0.61%\n","iter 530: pérdida 3.3224, tiempo 767.32ms, mfu 0.62%\n","iter 540: pérdida 3.1672, tiempo 769.04ms, mfu 0.62%\n","iter 550: pérdida 3.2885, tiempo 768.30ms, mfu 0.63%\n","iter 560: pérdida 3.3043, tiempo 771.21ms, mfu 0.63%\n","iter 570: pérdida 3.3314, tiempo 774.41ms, mfu 0.64%\n","iter 580: pérdida 3.2636, tiempo 770.35ms, mfu 0.64%\n","iter 590: pérdida 3.1794, tiempo 772.44ms, mfu 0.64%\n","iter 600: pérdida 3.2875, tiempo 768.39ms, mfu 0.65%\n","iter 610: pérdida 3.2710, tiempo 772.86ms, mfu 0.65%\n","iter 620: pérdida 3.2116, tiempo 776.99ms, mfu 0.65%\n","iter 630: pérdida 3.2677, tiempo 773.78ms, mfu 0.65%\n","iter 640: pérdida 3.0836, tiempo 769.36ms, mfu 0.65%\n","iter 650: pérdida 3.0768, tiempo 767.98ms, mfu 0.65%\n","iter 660: pérdida 3.0349, tiempo 774.84ms, mfu 0.66%\n","iter 670: pérdida 3.0096, tiempo 767.41ms, mfu 0.66%\n","iter 680: pérdida 3.0479, tiempo 772.75ms, mfu 0.66%\n","iter 690: pérdida 2.9847, tiempo 767.23ms, mfu 0.66%\n","iter 700: pérdida 3.0030, tiempo 769.56ms, mfu 0.66%\n","iter 710: pérdida 3.0094, tiempo 770.62ms, mfu 0.66%\n","iter 720: pérdida 3.0884, tiempo 772.45ms, mfu 0.66%\n","iter 730: pérdida 2.8115, tiempo 771.46ms, mfu 0.66%\n","iter 740: pérdida 2.8488, tiempo 771.76ms, mfu 0.66%\n","paso 750: pérdida en entrenamiento 2.6679, pérdida en validación 5.4549\n","guardando checkpoint en out\n","iter 750: pérdida 2.7850, tiempo 110115.62ms, mfu 0.60%\n","iter 760: pérdida 2.9470, tiempo 767.66ms, mfu 0.60%\n","iter 770: pérdida 2.8528, tiempo 766.91ms, mfu 0.61%\n","iter 780: pérdida 2.8046, tiempo 767.14ms, mfu 0.62%\n","iter 790: pérdida 2.6926, tiempo 768.15ms, mfu 0.62%\n","iter 800: pérdida 2.6908, tiempo 763.02ms, mfu 0.63%\n","iter 810: pérdida 2.5156, tiempo 771.91ms, mfu 0.63%\n","iter 820: pérdida 2.7948, tiempo 770.62ms, mfu 0.64%\n","iter 830: pérdida 2.6958, tiempo 771.48ms, mfu 0.64%\n","iter 840: pérdida 2.7401, tiempo 772.29ms, mfu 0.64%\n","iter 850: pérdida 2.5545, tiempo 775.06ms, mfu 0.64%\n","iter 860: pérdida 2.4970, tiempo 770.85ms, mfu 0.65%\n","iter 870: pérdida 2.4604, tiempo 769.84ms, mfu 0.65%\n","iter 880: pérdida 2.5487, tiempo 768.08ms, mfu 0.65%\n","iter 890: pérdida 2.4439, tiempo 772.41ms, mfu 0.65%\n","iter 900: pérdida 2.5804, tiempo 768.34ms, mfu 0.65%\n","iter 910: pérdida 2.3829, tiempo 769.89ms, mfu 0.66%\n","iter 920: pérdida 2.5849, tiempo 767.29ms, mfu 0.66%\n","iter 930: pérdida 2.4102, tiempo 770.96ms, mfu 0.66%\n","iter 940: pérdida 2.4630, tiempo 773.36ms, mfu 0.66%\n","iter 950: pérdida 2.3123, tiempo 765.65ms, mfu 0.66%\n","iter 960: pérdida 2.3993, tiempo 769.66ms, mfu 0.66%\n","iter 970: pérdida 2.3031, tiempo 771.72ms, mfu 0.66%\n","iter 980: pérdida 2.4205, tiempo 767.27ms, mfu 0.66%\n","iter 990: pérdida 2.2283, tiempo 772.01ms, mfu 0.66%\n","paso 1000: pérdida en entrenamiento 1.8995, pérdida en validación 5.9439\n","guardando checkpoint en out\n","iter 1000: pérdida 2.1695, tiempo 110128.15ms, mfu 0.60%\n","iter 1010: pérdida 2.2094, tiempo 762.80ms, mfu 0.61%\n","iter 1020: pérdida 2.1370, tiempo 767.44ms, mfu 0.61%\n","iter 1030: pérdida 2.1517, tiempo 779.30ms, mfu 0.62%\n","iter 1040: pérdida 2.1157, tiempo 780.25ms, mfu 0.62%\n","iter 1050: pérdida 1.9222, tiempo 775.51ms, mfu 0.63%\n","iter 1060: pérdida 2.1062, tiempo 776.81ms, mfu 0.63%\n","iter 1070: pérdida 2.1415, tiempo 772.31ms, mfu 0.63%\n","iter 1080: pérdida 2.0473, tiempo 766.33ms, mfu 0.64%\n","iter 1090: pérdida 1.9650, tiempo 769.97ms, mfu 0.64%\n","iter 1100: pérdida 1.8369, tiempo 769.51ms, mfu 0.64%\n","iter 1110: pérdida 2.0363, tiempo 769.13ms, mfu 0.65%\n","iter 1120: pérdida 1.9946, tiempo 767.39ms, mfu 0.65%\n","iter 1130: pérdida 2.0091, tiempo 770.89ms, mfu 0.65%\n","iter 1140: pérdida 1.7873, tiempo 773.90ms, mfu 0.65%\n","iter 1150: pérdida 1.9368, tiempo 772.20ms, mfu 0.65%\n","iter 1160: pérdida 1.9583, tiempo 771.75ms, mfu 0.66%\n","iter 1170: pérdida 1.7657, tiempo 772.01ms, mfu 0.66%\n","iter 1180: pérdida 1.7096, tiempo 773.39ms, mfu 0.66%\n","iter 1190: pérdida 1.8873, tiempo 771.78ms, mfu 0.66%\n","iter 1200: pérdida 1.7627, tiempo 770.19ms, mfu 0.66%\n","iter 1210: pérdida 1.7499, tiempo 774.58ms, mfu 0.66%\n","iter 1220: pérdida 1.7780, tiempo 770.37ms, mfu 0.66%\n","iter 1230: pérdida 1.6783, tiempo 770.04ms, mfu 0.66%\n","iter 1240: pérdida 1.8536, tiempo 774.52ms, mfu 0.66%\n","paso 1250: pérdida en entrenamiento 1.3130, pérdida en validación 6.3860\n","guardando checkpoint en out\n","iter 1250: pérdida 1.5950, tiempo 110333.76ms, mfu 0.60%\n","iter 1260: pérdida 1.6670, tiempo 762.94ms, mfu 0.60%\n","iter 1270: pérdida 1.7416, tiempo 772.84ms, mfu 0.61%\n","iter 1280: pérdida 1.7383, tiempo 774.46ms, mfu 0.62%\n","iter 1290: pérdida 1.6528, tiempo 781.27ms, mfu 0.62%\n","iter 1300: pérdida 1.6487, tiempo 777.45ms, mfu 0.62%\n","iter 1310: pérdida 1.5979, tiempo 775.35ms, mfu 0.63%\n","iter 1320: pérdida 1.4665, tiempo 776.89ms, mfu 0.63%\n","iter 1330: pérdida 1.7331, tiempo 773.19ms, mfu 0.64%\n","iter 1340: pérdida 1.7227, tiempo 767.39ms, mfu 0.64%\n","iter 1350: pérdida 1.5862, tiempo 765.76ms, mfu 0.64%\n","iter 1360: pérdida 1.5350, tiempo 766.65ms, mfu 0.65%\n","iter 1370: pérdida 1.5200, tiempo 769.76ms, mfu 0.65%\n","iter 1380: pérdida 1.5777, tiempo 769.99ms, mfu 0.65%\n","iter 1390: pérdida 1.5280, tiempo 770.99ms, mfu 0.65%\n","iter 1400: pérdida 1.6643, tiempo 771.74ms, mfu 0.65%\n","iter 1410: pérdida 1.4642, tiempo 767.97ms, mfu 0.66%\n","iter 1420: pérdida 1.4199, tiempo 770.82ms, mfu 0.66%\n","iter 1430: pérdida 1.2825, tiempo 775.82ms, mfu 0.66%\n","iter 1440: pérdida 1.3948, tiempo 773.32ms, mfu 0.66%\n","iter 1450: pérdida 1.3645, tiempo 772.87ms, mfu 0.66%\n","iter 1460: pérdida 1.3295, tiempo 774.63ms, mfu 0.66%\n","iter 1470: pérdida 1.3838, tiempo 772.40ms, mfu 0.66%\n","iter 1480: pérdida 1.2722, tiempo 770.88ms, mfu 0.66%\n","iter 1490: pérdida 1.4459, tiempo 773.30ms, mfu 0.66%\n","paso 1500: pérdida en entrenamiento 0.9317, pérdida en validación 6.8680\n","guardando checkpoint en out\n","iter 1500: pérdida 1.4100, tiempo 109550.39ms, mfu 0.60%\n","iter 1510: pérdida 1.3261, tiempo 766.61ms, mfu 0.60%\n","iter 1520: pérdida 1.4087, tiempo 773.23ms, mfu 0.61%\n","iter 1530: pérdida 1.1590, tiempo 777.05ms, mfu 0.62%\n","iter 1540: pérdida 1.3178, tiempo 780.38ms, mfu 0.62%\n","iter 1550: pérdida 1.2941, tiempo 779.15ms, mfu 0.62%\n","iter 1560: pérdida 1.3583, tiempo 773.55ms, mfu 0.63%\n","iter 1570: pérdida 1.1946, tiempo 774.74ms, mfu 0.63%\n","iter 1580: pérdida 1.3539, tiempo 771.72ms, mfu 0.64%\n","iter 1590: pérdida 1.2860, tiempo 767.95ms, mfu 0.64%\n","iter 1600: pérdida 1.3738, tiempo 768.80ms, mfu 0.64%\n","iter 1610: pérdida 1.1724, tiempo 767.47ms, mfu 0.65%\n","iter 1620: pérdida 1.3385, tiempo 770.16ms, mfu 0.65%\n","iter 1630: pérdida 1.0923, tiempo 770.13ms, mfu 0.65%\n","iter 1640: pérdida 1.2493, tiempo 768.03ms, mfu 0.65%\n","iter 1650: pérdida 1.1605, tiempo 770.76ms, mfu 0.65%\n","iter 1660: pérdida 1.1451, tiempo 772.36ms, mfu 0.65%\n","iter 1670: pérdida 1.2009, tiempo 769.98ms, mfu 0.66%\n","iter 1680: pérdida 1.1317, tiempo 771.86ms, mfu 0.66%\n","iter 1690: pérdida 1.1450, tiempo 771.98ms, mfu 0.66%\n","iter 1700: pérdida 1.1473, tiempo 772.40ms, mfu 0.66%\n","iter 1710: pérdida 1.1438, tiempo 773.75ms, mfu 0.66%\n","iter 1720: pérdida 1.2078, tiempo 774.97ms, mfu 0.66%\n","iter 1730: pérdida 1.1132, tiempo 774.10ms, mfu 0.66%\n","iter 1740: pérdida 1.1110, tiempo 771.28ms, mfu 0.66%\n","paso 1750: pérdida en entrenamiento 0.6785, pérdida en validación 7.2575\n","guardando checkpoint en out\n","iter 1750: pérdida 1.1134, tiempo 107507.55ms, mfu 0.60%\n","iter 1760: pérdida 1.0771, tiempo 769.08ms, mfu 0.60%\n","iter 1770: pérdida 1.0537, tiempo 772.58ms, mfu 0.61%\n","iter 1780: pérdida 1.1059, tiempo 770.71ms, mfu 0.62%\n","iter 1790: pérdida 1.0413, tiempo 775.01ms, mfu 0.62%\n","iter 1800: pérdida 0.9790, tiempo 776.21ms, mfu 0.63%\n","iter 1810: pérdida 0.9950, tiempo 774.55ms, mfu 0.63%\n","iter 1820: pérdida 1.0118, tiempo 768.10ms, mfu 0.63%\n","iter 1830: pérdida 0.9423, tiempo 770.13ms, mfu 0.64%\n","iter 1840: pérdida 1.0046, tiempo 766.75ms, mfu 0.64%\n","iter 1850: pérdida 0.9838, tiempo 769.24ms, mfu 0.64%\n","iter 1860: pérdida 0.9208, tiempo 766.32ms, mfu 0.65%\n","iter 1870: pérdida 0.9991, tiempo 769.63ms, mfu 0.65%\n","iter 1880: pérdida 1.0409, tiempo 769.38ms, mfu 0.65%\n","iter 1890: pérdida 0.9416, tiempo 768.20ms, mfu 0.65%\n","iter 1900: pérdida 0.9432, tiempo 774.11ms, mfu 0.65%\n","iter 1910: pérdida 0.9897, tiempo 774.21ms, mfu 0.66%\n","iter 1920: pérdida 0.9847, tiempo 772.09ms, mfu 0.66%\n","iter 1930: pérdida 0.8689, tiempo 769.97ms, mfu 0.66%\n","iter 1940: pérdida 0.9378, tiempo 773.91ms, mfu 0.66%\n","iter 1950: pérdida 0.9491, tiempo 772.44ms, mfu 0.66%\n","iter 1960: pérdida 0.8806, tiempo 772.95ms, mfu 0.66%\n","iter 1970: pérdida 0.8581, tiempo 774.48ms, mfu 0.66%\n","iter 1980: pérdida 0.8686, tiempo 771.28ms, mfu 0.66%\n","iter 1990: pérdida 1.0071, tiempo 775.97ms, mfu 0.66%\n","paso 2000: pérdida en entrenamiento 0.5187, pérdida en validación 7.5944\n","guardando checkpoint en out\n","iter 2000: pérdida 0.8835, tiempo 108025.11ms, mfu 0.60%\n","iter 2010: pérdida 0.8231, tiempo 767.96ms, mfu 0.60%\n","iter 2020: pérdida 0.9595, tiempo 770.11ms, mfu 0.61%\n","iter 2030: pérdida 0.8487, tiempo 779.34ms, mfu 0.62%\n","iter 2040: pérdida 0.8683, tiempo 774.59ms, mfu 0.62%\n","iter 2050: pérdida 0.8467, tiempo 777.15ms, mfu 0.62%\n","iter 2060: pérdida 0.8208, tiempo 771.84ms, mfu 0.63%\n","iter 2070: pérdida 0.8842, tiempo 771.88ms, mfu 0.63%\n","iter 2080: pérdida 0.9353, tiempo 772.37ms, mfu 0.64%\n","iter 2090: pérdida 0.9147, tiempo 768.02ms, mfu 0.64%\n","iter 2100: pérdida 0.8942, tiempo 770.73ms, mfu 0.64%\n","iter 2110: pérdida 0.8050, tiempo 767.93ms, mfu 0.65%\n","iter 2120: pérdida 0.8618, tiempo 770.25ms, mfu 0.65%\n","iter 2130: pérdida 0.8490, tiempo 769.30ms, mfu 0.65%\n","iter 2140: pérdida 0.7803, tiempo 768.53ms, mfu 0.65%\n","iter 2150: pérdida 0.8020, tiempo 766.70ms, mfu 0.65%\n","iter 2160: pérdida 0.7589, tiempo 768.16ms, mfu 0.66%\n","iter 2170: pérdida 0.8761, tiempo 768.84ms, mfu 0.66%\n","iter 2180: pérdida 0.7361, tiempo 768.29ms, mfu 0.66%\n","iter 2190: pérdida 0.7293, tiempo 774.47ms, mfu 0.66%\n","iter 2200: pérdida 0.7768, tiempo 777.28ms, mfu 0.66%\n","iter 2210: pérdida 0.7525, tiempo 777.29ms, mfu 0.66%\n","iter 2220: pérdida 0.7824, tiempo 776.76ms, mfu 0.66%\n","iter 2230: pérdida 0.7735, tiempo 773.18ms, mfu 0.66%\n","iter 2240: pérdida 0.7001, tiempo 774.60ms, mfu 0.66%\n","paso 2250: pérdida en entrenamiento 0.3927, pérdida en validación 7.9250\n","guardando checkpoint en out\n","iter 2250: pérdida 0.7196, tiempo 109607.03ms, mfu 0.60%\n","iter 2260: pérdida 0.7456, tiempo 767.93ms, mfu 0.60%\n","iter 2270: pérdida 0.7118, tiempo 770.03ms, mfu 0.61%\n","iter 2280: pérdida 0.6910, tiempo 779.15ms, mfu 0.62%\n","iter 2290: pérdida 0.6934, tiempo 776.56ms, mfu 0.62%\n","iter 2300: pérdida 0.7626, tiempo 776.55ms, mfu 0.62%\n","iter 2310: pérdida 0.6971, tiempo 777.70ms, mfu 0.63%\n","iter 2320: pérdida 0.7250, tiempo 770.67ms, mfu 0.63%\n","iter 2330: pérdida 0.7338, tiempo 771.46ms, mfu 0.64%\n","iter 2340: pérdida 0.6641, tiempo 770.62ms, mfu 0.64%\n","iter 2350: pérdida 0.6861, tiempo 768.05ms, mfu 0.64%\n","iter 2360: pérdida 0.6848, tiempo 768.58ms, mfu 0.65%\n","iter 2370: pérdida 0.7232, tiempo 776.62ms, mfu 0.65%\n","iter 2380: pérdida 0.7185, tiempo 774.09ms, mfu 0.65%\n","iter 2390: pérdida 0.7050, tiempo 773.89ms, mfu 0.65%\n","iter 2400: pérdida 0.6666, tiempo 770.10ms, mfu 0.65%\n","iter 2410: pérdida 0.7241, tiempo 768.32ms, mfu 0.65%\n","iter 2420: pérdida 0.6859, tiempo 770.72ms, mfu 0.66%\n","iter 2430: pérdida 0.6965, tiempo 774.78ms, mfu 0.66%\n","iter 2440: pérdida 0.6307, tiempo 770.51ms, mfu 0.66%\n","iter 2450: pérdida 0.6322, tiempo 772.12ms, mfu 0.66%\n","iter 2460: pérdida 0.6361, tiempo 775.22ms, mfu 0.66%\n","iter 2470: pérdida 0.6423, tiempo 774.10ms, mfu 0.66%\n","iter 2480: pérdida 0.6194, tiempo 773.44ms, mfu 0.66%\n","iter 2490: pérdida 0.7043, tiempo 772.54ms, mfu 0.66%\n","paso 2500: pérdida en entrenamiento 0.3140, pérdida en validación 8.1879\n","guardando checkpoint en out\n","iter 2500: pérdida 0.5978, tiempo 107430.35ms, mfu 0.60%\n","iter 2510: pérdida 0.6295, tiempo 770.49ms, mfu 0.60%\n","iter 2520: pérdida 0.6320, tiempo 772.33ms, mfu 0.61%\n","iter 2530: pérdida 0.6702, tiempo 774.42ms, mfu 0.62%\n","iter 2540: pérdida 0.5968, tiempo 772.27ms, mfu 0.62%\n","iter 2550: pérdida 0.6105, tiempo 779.07ms, mfu 0.62%\n","iter 2560: pérdida 0.5759, tiempo 772.56ms, mfu 0.63%\n","iter 2570: pérdida 0.6260, tiempo 771.66ms, mfu 0.63%\n","iter 2580: pérdida 0.5884, tiempo 771.94ms, mfu 0.64%\n","iter 2590: pérdida 0.7120, tiempo 773.40ms, mfu 0.64%\n","iter 2600: pérdida 0.5683, tiempo 773.72ms, mfu 0.64%\n","iter 2610: pérdida 0.5557, tiempo 767.27ms, mfu 0.64%\n","iter 2620: pérdida 0.6144, tiempo 770.06ms, mfu 0.65%\n","iter 2630: pérdida 0.5900, tiempo 767.92ms, mfu 0.65%\n","iter 2640: pérdida 0.5967, tiempo 769.03ms, mfu 0.65%\n","iter 2650: pérdida 0.6052, tiempo 768.36ms, mfu 0.65%\n","iter 2660: pérdida 0.6048, tiempo 769.57ms, mfu 0.66%\n","iter 2670: pérdida 0.5657, tiempo 772.52ms, mfu 0.66%\n","iter 2680: pérdida 0.5798, tiempo 768.36ms, mfu 0.66%\n","iter 2690: pérdida 0.5397, tiempo 775.62ms, mfu 0.66%\n","iter 2700: pérdida 0.5304, tiempo 772.61ms, mfu 0.66%\n","iter 2710: pérdida 0.5646, tiempo 771.53ms, mfu 0.66%\n","iter 2720: pérdida 0.5654, tiempo 773.30ms, mfu 0.66%\n","iter 2730: pérdida 0.5584, tiempo 777.22ms, mfu 0.66%\n","iter 2740: pérdida 0.5376, tiempo 773.99ms, mfu 0.66%\n","paso 2750: pérdida en entrenamiento 0.2380, pérdida en validación 8.3852\n","guardando checkpoint en out\n","iter 2750: pérdida 0.5217, tiempo 109153.45ms, mfu 0.60%\n","iter 2760: pérdida 0.5042, tiempo 767.74ms, mfu 0.60%\n","iter 2770: pérdida 0.5314, tiempo 772.38ms, mfu 0.61%\n","iter 2780: pérdida 0.5463, tiempo 776.54ms, mfu 0.62%\n","iter 2790: pérdida 0.5438, tiempo 781.08ms, mfu 0.62%\n","iter 2800: pérdida 0.5295, tiempo 775.44ms, mfu 0.62%\n","iter 2810: pérdida 0.5214, tiempo 775.11ms, mfu 0.63%\n","iter 2820: pérdida 0.5157, tiempo 768.43ms, mfu 0.63%\n","iter 2830: pérdida 0.5314, tiempo 769.41ms, mfu 0.64%\n","iter 2840: pérdida 0.4672, tiempo 771.09ms, mfu 0.64%\n","iter 2850: pérdida 0.4832, tiempo 770.54ms, mfu 0.64%\n","iter 2860: pérdida 0.5266, tiempo 767.19ms, mfu 0.65%\n","iter 2870: pérdida 0.5242, tiempo 769.94ms, mfu 0.65%\n","iter 2880: pérdida 0.4753, tiempo 769.75ms, mfu 0.65%\n","iter 2890: pérdida 0.4871, tiempo 769.94ms, mfu 0.65%\n","iter 2900: pérdida 0.5150, tiempo 768.79ms, mfu 0.65%\n","iter 2910: pérdida 0.4377, tiempo 771.79ms, mfu 0.66%\n","iter 2920: pérdida 0.4534, tiempo 768.29ms, mfu 0.66%\n","iter 2930: pérdida 0.4738, tiempo 773.07ms, mfu 0.66%\n","iter 2940: pérdida 0.4200, tiempo 772.14ms, mfu 0.66%\n","iter 2950: pérdida 0.4448, tiempo 778.45ms, mfu 0.66%\n","iter 2960: pérdida 0.5065, tiempo 770.78ms, mfu 0.66%\n","iter 2970: pérdida 0.4885, tiempo 771.03ms, mfu 0.66%\n","iter 2980: pérdida 0.4703, tiempo 772.72ms, mfu 0.66%\n","iter 2990: pérdida 0.4815, tiempo 774.52ms, mfu 0.66%\n","paso 3000: pérdida en entrenamiento 0.1903, pérdida en validación 8.5669\n","guardando checkpoint en out\n","iter 3000: pérdida 0.4519, tiempo 107799.37ms, mfu 0.60%\n","iter 3010: pérdida 0.4302, tiempo 769.43ms, mfu 0.60%\n","iter 3020: pérdida 0.4598, tiempo 767.84ms, mfu 0.61%\n","iter 3030: pérdida 0.4335, tiempo 773.99ms, mfu 0.62%\n","iter 3040: pérdida 0.4481, tiempo 777.06ms, mfu 0.62%\n","iter 3050: pérdida 0.4412, tiempo 776.28ms, mfu 0.62%\n","iter 3060: pérdida 0.4197, tiempo 775.68ms, mfu 0.63%\n","iter 3070: pérdida 0.4571, tiempo 776.50ms, mfu 0.63%\n","iter 3080: pérdida 0.3951, tiempo 768.78ms, mfu 0.64%\n","iter 3090: pérdida 0.4193, tiempo 774.28ms, mfu 0.64%\n","iter 3100: pérdida 0.5165, tiempo 763.90ms, mfu 0.64%\n","iter 3110: pérdida 0.4377, tiempo 767.50ms, mfu 0.65%\n","iter 3120: pérdida 0.4198, tiempo 771.36ms, mfu 0.65%\n","iter 3130: pérdida 0.3757, tiempo 772.61ms, mfu 0.65%\n","iter 3140: pérdida 0.3990, tiempo 775.11ms, mfu 0.65%\n","iter 3150: pérdida 0.4205, tiempo 769.98ms, mfu 0.65%\n","iter 3160: pérdida 0.4065, tiempo 774.95ms, mfu 0.65%\n","iter 3170: pérdida 0.4062, tiempo 774.63ms, mfu 0.66%\n","iter 3180: pérdida 0.3942, tiempo 773.49ms, mfu 0.66%\n","iter 3190: pérdida 0.3632, tiempo 778.69ms, mfu 0.66%\n","iter 3200: pérdida 0.3841, tiempo 772.62ms, mfu 0.66%\n","iter 3210: pérdida 0.4311, tiempo 773.49ms, mfu 0.66%\n","iter 3220: pérdida 0.4299, tiempo 773.39ms, mfu 0.66%\n","iter 3230: pérdida 0.3748, tiempo 771.82ms, mfu 0.66%\n","iter 3240: pérdida 0.3849, tiempo 772.72ms, mfu 0.66%\n","paso 3250: pérdida en entrenamiento 0.1546, pérdida en validación 8.6888\n","guardando checkpoint en out\n","iter 3250: pérdida 0.3829, tiempo 109394.90ms, mfu 0.60%\n","iter 3260: pérdida 0.3779, tiempo 764.84ms, mfu 0.60%\n","iter 3270: pérdida 0.3852, tiempo 773.87ms, mfu 0.61%\n","iter 3280: pérdida 0.4016, tiempo 780.40ms, mfu 0.61%\n","iter 3290: pérdida 0.3941, tiempo 778.39ms, mfu 0.62%\n","iter 3300: pérdida 0.3825, tiempo 775.65ms, mfu 0.62%\n","iter 3310: pérdida 0.3499, tiempo 774.70ms, mfu 0.63%\n","iter 3320: pérdida 0.3631, tiempo 772.63ms, mfu 0.63%\n","iter 3330: pérdida 0.3470, tiempo 771.23ms, mfu 0.64%\n","iter 3340: pérdida 0.4120, tiempo 770.80ms, mfu 0.64%\n","iter 3350: pérdida 0.3881, tiempo 767.57ms, mfu 0.64%\n","iter 3360: pérdida 0.3682, tiempo 766.92ms, mfu 0.65%\n","iter 3370: pérdida 0.3450, tiempo 769.84ms, mfu 0.65%\n","iter 3380: pérdida 0.3273, tiempo 766.98ms, mfu 0.65%\n","iter 3390: pérdida 0.3410, tiempo 773.61ms, mfu 0.65%\n","iter 3400: pérdida 0.3538, tiempo 771.76ms, mfu 0.65%\n","iter 3410: pérdida 0.3493, tiempo 772.02ms, mfu 0.65%\n","iter 3420: pérdida 0.3624, tiempo 769.20ms, mfu 0.66%\n","iter 3430: pérdida 0.3625, tiempo 770.30ms, mfu 0.66%\n","iter 3440: pérdida 0.3413, tiempo 773.76ms, mfu 0.66%\n","iter 3450: pérdida 0.3840, tiempo 774.27ms, mfu 0.66%\n","iter 3460: pérdida 0.3393, tiempo 776.32ms, mfu 0.66%\n","iter 3470: pérdida 0.3623, tiempo 769.39ms, mfu 0.66%\n","iter 3480: pérdida 0.3290, tiempo 770.88ms, mfu 0.66%\n","iter 3490: pérdida 0.3343, tiempo 775.98ms, mfu 0.66%\n","paso 3500: pérdida en entrenamiento 0.1306, pérdida en validación 8.8512\n","guardando checkpoint en out\n","iter 3500: pérdida 0.3351, tiempo 107657.21ms, mfu 0.60%\n","iter 3510: pérdida 0.3501, tiempo 773.03ms, mfu 0.60%\n","iter 3520: pérdida 0.3183, tiempo 774.13ms, mfu 0.61%\n","iter 3530: pérdida 0.3294, tiempo 777.75ms, mfu 0.61%\n","iter 3540: pérdida 0.3537, tiempo 774.34ms, mfu 0.62%\n","iter 3550: pérdida 0.3460, tiempo 774.20ms, mfu 0.62%\n","iter 3560: pérdida 0.3012, tiempo 774.56ms, mfu 0.63%\n","iter 3570: pérdida 0.3087, tiempo 773.15ms, mfu 0.63%\n","iter 3580: pérdida 0.3299, tiempo 773.85ms, mfu 0.64%\n","iter 3590: pérdida 0.3007, tiempo 775.94ms, mfu 0.64%\n","iter 3600: pérdida 0.3090, tiempo 765.99ms, mfu 0.64%\n","iter 3610: pérdida 0.3204, tiempo 767.92ms, mfu 0.64%\n","iter 3620: pérdida 0.2874, tiempo 767.86ms, mfu 0.65%\n","iter 3630: pérdida 0.3215, tiempo 769.49ms, mfu 0.65%\n","iter 3640: pérdida 0.3052, tiempo 768.52ms, mfu 0.65%\n","iter 3650: pérdida 0.2891, tiempo 769.34ms, mfu 0.65%\n","iter 3660: pérdida 0.2941, tiempo 765.92ms, mfu 0.66%\n","iter 3670: pérdida 0.3027, tiempo 771.73ms, mfu 0.66%\n","iter 3680: pérdida 0.3196, tiempo 770.00ms, mfu 0.66%\n","iter 3690: pérdida 0.3083, tiempo 771.86ms, mfu 0.66%\n","iter 3700: pérdida 0.3159, tiempo 773.56ms, mfu 0.66%\n","iter 3710: pérdida 0.3127, tiempo 770.60ms, mfu 0.66%\n","iter 3720: pérdida 0.2870, tiempo 772.72ms, mfu 0.66%\n","iter 3730: pérdida 0.3019, tiempo 777.12ms, mfu 0.66%\n","iter 3740: pérdida 0.3029, tiempo 770.92ms, mfu 0.66%\n","paso 3750: pérdida en entrenamiento 0.1106, pérdida en validación 8.8415\n","guardando checkpoint en out\n","iter 3750: pérdida 0.2855, tiempo 107933.81ms, mfu 0.60%\n","iter 3760: pérdida 0.2801, tiempo 770.01ms, mfu 0.60%\n","iter 3770: pérdida 0.2857, tiempo 776.35ms, mfu 0.61%\n","iter 3780: pérdida 0.2822, tiempo 774.30ms, mfu 0.62%\n","iter 3790: pérdida 0.3038, tiempo 772.78ms, mfu 0.62%\n","iter 3800: pérdida 0.2997, tiempo 771.79ms, mfu 0.63%\n","iter 3810: pérdida 0.2959, tiempo 771.88ms, mfu 0.63%\n","iter 3820: pérdida 0.2762, tiempo 772.24ms, mfu 0.63%\n","iter 3830: pérdida 0.2780, tiempo 772.52ms, mfu 0.64%\n","iter 3840: pérdida 0.2670, tiempo 770.03ms, mfu 0.64%\n","iter 3850: pérdida 0.2873, tiempo 772.74ms, mfu 0.64%\n","iter 3860: pérdida 0.2527, tiempo 768.50ms, mfu 0.65%\n","iter 3870: pérdida 0.2739, tiempo 767.90ms, mfu 0.65%\n","iter 3880: pérdida 0.2896, tiempo 769.73ms, mfu 0.65%\n","iter 3890: pérdida 0.2874, tiempo 768.74ms, mfu 0.65%\n","iter 3900: pérdida 0.2554, tiempo 768.36ms, mfu 0.65%\n","iter 3910: pérdida 0.2751, tiempo 767.91ms, mfu 0.66%\n","iter 3920: pérdida 0.2832, tiempo 770.67ms, mfu 0.66%\n","iter 3930: pérdida 0.2772, tiempo 770.06ms, mfu 0.66%\n","iter 3940: pérdida 0.2880, tiempo 772.26ms, mfu 0.66%\n","iter 3950: pérdida 0.2706, tiempo 770.16ms, mfu 0.66%\n","iter 3960: pérdida 0.2681, tiempo 770.76ms, mfu 0.66%\n","iter 3970: pérdida 0.2533, tiempo 769.27ms, mfu 0.66%\n","iter 3980: pérdida 0.2566, tiempo 770.92ms, mfu 0.66%\n","iter 3990: pérdida 0.2845, tiempo 770.55ms, mfu 0.66%\n","paso 4000: pérdida en entrenamiento 0.0987, pérdida en validación 8.9715\n","guardando checkpoint en out\n","iter 4000: pérdida 0.2325, tiempo 109746.23ms, mfu 0.60%\n","iter 4010: pérdida 0.2536, tiempo 769.00ms, mfu 0.60%\n","iter 4020: pérdida 0.2700, tiempo 774.13ms, mfu 0.61%\n","iter 4030: pérdida 0.2596, tiempo 776.74ms, mfu 0.62%\n","iter 4040: pérdida 0.2707, tiempo 776.58ms, mfu 0.62%\n","iter 4050: pérdida 0.2648, tiempo 777.48ms, mfu 0.63%\n","iter 4060: pérdida 0.2630, tiempo 775.46ms, mfu 0.63%\n","iter 4070: pérdida 0.2537, tiempo 776.02ms, mfu 0.63%\n","iter 4080: pérdida 0.2568, tiempo 777.97ms, mfu 0.64%\n","iter 4090: pérdida 0.2422, tiempo 770.23ms, mfu 0.64%\n","iter 4100: pérdida 0.2473, tiempo 776.20ms, mfu 0.64%\n","iter 4110: pérdida 0.2686, tiempo 767.27ms, mfu 0.64%\n","iter 4120: pérdida 0.2612, tiempo 774.03ms, mfu 0.65%\n","iter 4130: pérdida 0.2194, tiempo 768.98ms, mfu 0.65%\n","iter 4140: pérdida 0.2123, tiempo 771.36ms, mfu 0.65%\n","iter 4150: pérdida 0.2417, tiempo 772.90ms, mfu 0.65%\n","iter 4160: pérdida 0.2349, tiempo 767.25ms, mfu 0.65%\n","iter 4170: pérdida 0.2350, tiempo 776.49ms, mfu 0.66%\n","iter 4180: pérdida 0.2603, tiempo 768.64ms, mfu 0.66%\n","iter 4190: pérdida 0.2425, tiempo 769.08ms, mfu 0.66%\n","iter 4200: pérdida 0.2470, tiempo 772.20ms, mfu 0.66%\n","iter 4210: pérdida 0.2541, tiempo 776.38ms, mfu 0.66%\n","iter 4220: pérdida 0.2512, tiempo 772.87ms, mfu 0.66%\n","iter 4230: pérdida 0.2412, tiempo 775.39ms, mfu 0.66%\n","iter 4240: pérdida 0.2700, tiempo 775.96ms, mfu 0.66%\n","paso 4250: pérdida en entrenamiento 0.0900, pérdida en validación 9.0995\n","guardando checkpoint en out\n","iter 4250: pérdida 0.2457, tiempo 107132.32ms, mfu 0.60%\n","iter 4260: pérdida 0.2195, tiempo 769.50ms, mfu 0.60%\n","iter 4270: pérdida 0.2269, tiempo 771.17ms, mfu 0.61%\n","iter 4280: pérdida 0.2344, tiempo 776.92ms, mfu 0.61%\n","iter 4290: pérdida 0.2726, tiempo 776.12ms, mfu 0.62%\n","iter 4300: pérdida 0.2441, tiempo 771.93ms, mfu 0.62%\n","iter 4310: pérdida 0.2024, tiempo 776.32ms, mfu 0.63%\n","iter 4320: pérdida 0.2281, tiempo 769.00ms, mfu 0.63%\n","iter 4330: pérdida 0.2322, tiempo 771.46ms, mfu 0.64%\n","iter 4340: pérdida 0.2291, tiempo 770.14ms, mfu 0.64%\n","iter 4350: pérdida 0.2337, tiempo 765.30ms, mfu 0.64%\n","iter 4360: pérdida 0.2261, tiempo 769.65ms, mfu 0.65%\n","iter 4370: pérdida 0.2254, tiempo 771.33ms, mfu 0.65%\n","iter 4380: pérdida 0.2239, tiempo 769.36ms, mfu 0.65%\n","iter 4390: pérdida 0.2386, tiempo 769.27ms, mfu 0.65%\n","iter 4400: pérdida 0.2432, tiempo 771.02ms, mfu 0.65%\n","iter 4410: pérdida 0.2265, tiempo 768.02ms, mfu 0.66%\n","iter 4420: pérdida 0.2486, tiempo 771.65ms, mfu 0.66%\n","iter 4430: pérdida 0.2263, tiempo 773.18ms, mfu 0.66%\n","iter 4440: pérdida 0.2356, tiempo 768.68ms, mfu 0.66%\n","iter 4450: pérdida 0.2171, tiempo 772.52ms, mfu 0.66%\n","iter 4460: pérdida 0.2319, tiempo 778.69ms, mfu 0.66%\n","iter 4470: pérdida 0.2206, tiempo 771.91ms, mfu 0.66%\n","iter 4480: pérdida 0.2316, tiempo 768.76ms, mfu 0.66%\n","iter 4490: pérdida 0.2108, tiempo 771.74ms, mfu 0.66%\n","paso 4500: pérdida en entrenamiento 0.0841, pérdida en validación 9.1421\n","guardando checkpoint en out\n","iter 4500: pérdida 0.2095, tiempo 109693.31ms, mfu 0.60%\n","iter 4510: pérdida 0.2255, tiempo 767.82ms, mfu 0.60%\n","iter 4520: pérdida 0.2080, tiempo 770.71ms, mfu 0.61%\n","iter 4530: pérdida 0.2073, tiempo 778.96ms, mfu 0.62%\n","iter 4540: pérdida 0.2226, tiempo 778.41ms, mfu 0.62%\n","iter 4550: pérdida 0.2238, tiempo 773.73ms, mfu 0.62%\n","iter 4560: pérdida 0.2253, tiempo 775.18ms, mfu 0.63%\n","iter 4570: pérdida 0.2114, tiempo 775.89ms, mfu 0.63%\n","iter 4580: pérdida 0.2076, tiempo 770.63ms, mfu 0.64%\n","iter 4590: pérdida 0.1958, tiempo 766.25ms, mfu 0.64%\n","iter 4600: pérdida 0.1956, tiempo 767.63ms, mfu 0.64%\n","iter 4610: pérdida 0.2040, tiempo 766.15ms, mfu 0.65%\n","iter 4620: pérdida 0.2021, tiempo 770.94ms, mfu 0.65%\n","iter 4630: pérdida 0.2239, tiempo 772.35ms, mfu 0.65%\n","iter 4640: pérdida 0.1914, tiempo 774.84ms, mfu 0.65%\n","iter 4650: pérdida 0.2363, tiempo 772.89ms, mfu 0.65%\n","iter 4660: pérdida 0.2027, tiempo 773.80ms, mfu 0.65%\n","iter 4670: pérdida 0.1869, tiempo 770.99ms, mfu 0.66%\n","iter 4680: pérdida 0.2126, tiempo 772.52ms, mfu 0.66%\n","iter 4690: pérdida 0.1900, tiempo 772.16ms, mfu 0.66%\n","iter 4700: pérdida 0.1929, tiempo 774.54ms, mfu 0.66%\n","iter 4710: pérdida 0.2106, tiempo 772.79ms, mfu 0.66%\n","iter 4720: pérdida 0.2079, tiempo 771.62ms, mfu 0.66%\n","iter 4730: pérdida 0.2217, tiempo 773.58ms, mfu 0.66%\n","iter 4740: pérdida 0.1758, tiempo 775.08ms, mfu 0.66%\n","paso 4750: pérdida en entrenamiento 0.0801, pérdida en validación 9.1948\n","guardando checkpoint en out\n","iter 4750: pérdida 0.1975, tiempo 107765.40ms, mfu 0.60%\n","iter 4760: pérdida 0.2290, tiempo 766.80ms, mfu 0.60%\n","iter 4770: pérdida 0.2065, tiempo 771.53ms, mfu 0.61%\n","iter 4780: pérdida 0.2237, tiempo 774.17ms, mfu 0.62%\n","iter 4790: pérdida 0.2001, tiempo 778.87ms, mfu 0.62%\n","iter 4800: pérdida 0.1923, tiempo 778.35ms, mfu 0.62%\n","iter 4810: pérdida 0.2116, tiempo 776.82ms, mfu 0.63%\n","iter 4820: pérdida 0.1932, tiempo 772.12ms, mfu 0.63%\n","iter 4830: pérdida 0.2070, tiempo 771.36ms, mfu 0.64%\n","iter 4840: pérdida 0.1932, tiempo 771.18ms, mfu 0.64%\n","iter 4850: pérdida 0.2084, tiempo 773.38ms, mfu 0.64%\n","iter 4860: pérdida 0.2060, tiempo 771.89ms, mfu 0.64%\n","iter 4870: pérdida 0.1952, tiempo 769.40ms, mfu 0.65%\n","iter 4880: pérdida 0.1974, tiempo 768.09ms, mfu 0.65%\n","iter 4890: pérdida 0.1995, tiempo 770.46ms, mfu 0.65%\n","iter 4900: pérdida 0.2191, tiempo 775.93ms, mfu 0.65%\n","iter 4910: pérdida 0.2146, tiempo 776.58ms, mfu 0.65%\n","iter 4920: pérdida 0.1892, tiempo 772.26ms, mfu 0.66%\n","iter 4930: pérdida 0.1931, tiempo 775.18ms, mfu 0.66%\n","iter 4940: pérdida 0.2006, tiempo 774.02ms, mfu 0.66%\n","iter 4950: pérdida 0.1895, tiempo 773.13ms, mfu 0.66%\n","iter 4960: pérdida 0.1948, tiempo 764.95ms, mfu 0.66%\n","iter 4970: pérdida 0.1885, tiempo 774.26ms, mfu 0.66%\n","iter 4980: pérdida 0.2079, tiempo 776.54ms, mfu 0.66%\n","iter 4990: pérdida 0.1916, tiempo 767.78ms, mfu 0.66%\n","paso 5000: pérdida en entrenamiento 0.0763, pérdida en validación 9.3194\n","guardando checkpoint en out\n","iter 5000: pérdida 0.2080, tiempo 107895.24ms, mfu 0.60%\n"]}]},{"cell_type":"markdown","source":["## Generación de texto!\n","\n","Para generar texto podemos usar `sample.py`, configurarlo con nuestro nuevo modelo y así podremos probar:\n"],"metadata":{"id":"L2J5JlRxFJOM"}},{"cell_type":"markdown","source":["### Cargar modelo y setup"],"metadata":{"id":"eo_QP1ITFfX2"}},{"cell_type":"code","source":["import os\n","import pickle\n","from contextlib import nullcontext\n","import torch\n","import tiktoken\n","from model import GPTConfig, GPT\n","\n","# Configuración inicial\n","init_from = 'resume'  # Especifica si el modelo se inicializa desde un estado guardado o desde una variante de GPT2.\n","out_dir = 'out'  # Directorio de salida para guardar estados, ignorado si `init_from` no es 'resume'.\n","start = \"\\n\"  # Cadena inicial para la generación de texto. Puede ser modificada para especificar un archivo.\n","num_samples = 10  # Número de muestras a generar.\n","max_new_tokens = 500  # Máximo número de tokens a generar por muestra.\n","temperature = 0.8  # Ajusta la aleatoriedad de las predicciones. Menos de 1.0 hace que sea menos aleatorio.\n","top_k = 200  # Limita la selección a los `top_k` tokens más probables, los demás tienen probabilidad 0.\n","seed = 1337  # Semilla para la generación aleatoria, para la reproducibilidad.\n","device = 'cuda'  # Dispositivo de cómputo utilizado, como 'cpu' o 'cuda'.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'  # Determina el tipo de datos basado en la disponibilidad y soporte de CUDA.\n","compile = False  # Determina si el modelo debe compilarse con PyTorch 2.0 para mejorar el rendimiento.\n","\n","# Configuración de PyTorch\n","torch.manual_seed(seed)  # Establece la semilla para la generación aleatoria en PyTorch.\n","torch.cuda.manual_seed(seed)  # Establece la semilla para la generación aleatoria en CUDA.\n","torch.backends.cuda.matmul.allow_tf32 = True  # Permite cálculos TF32 en multiplicaciones de matrices con CUDA.\n","torch.backends.cudnn.allow_tf32 = True  # Permite cálculos TF32 en cuDNN.\n","\n","# Preparación para la generación\n","device_type = 'cuda' if 'cuda' in device else 'cpu'  # Determina el tipo de dispositivo basado en `device`.\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]  # Mapea `dtype` a un tipo de datos de PyTorch.\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)  # Configura el contexto de ejecución con autocasting si es necesario.\n"],"metadata":{"id":"-vftqU9LheEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Modelo\n","if init_from == 'resume':\n","    # Inicializa desde un modelo guardado en un directorio específico.\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')  # Construye la ruta al archivo de checkpoint.\n","    checkpoint = torch.load(ckpt_path, map_location=device)  # Carga el checkpoint en el dispositivo especificado.\n","\n","    # Crea una configuración de GPT a partir de los argumentos guardados en el checkpoint.\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","\n","    # Inicializa el modelo con la configuración obtenida.\n","    model = GPT(gptconf)\n","\n","    # Carga el estado del modelo desde el checkpoint.\n","    state_dict = checkpoint['model']\n","\n","    # Limpia los nombres de las claves del estado del modelo si comienzan con un prefijo no deseado.\n","    unwanted_prefix = '_orig_mod.'  # Prefijo a buscar y eliminar de los nombres de las claves.\n","    for k, v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            # Si la clave comienza con el prefijo no deseado, elimina el prefijo y actualiza el nombre de la clave.\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","\n","    # Carga el estado limpiado del modelo en el modelo actual.\n","    model.load_state_dict(state_dict)\n"],"metadata":{"id":"FQRB3j7iiNkl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736376946897,"user_tz":-60,"elapsed":894,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"456121a4-224c-4276-bfea-6c9bded15470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-61-4764ead93e82>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(ckpt_path, map_location=device)  # Carga el checkpoint en el dispositivo especificado.\n"]},{"output_type":"stream","name":"stdout","text":["number of parameters: 29.55M\n"]}]},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model)"],"metadata":{"id":"N1YAy8DriVZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc = tokenizer\n","encode = lambda s: enc.encode(s)\n","decode = lambda l: enc.decode(l)"],"metadata":{"id":"KoB-5ZuLicAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generation!"],"metadata":{"id":"mkTQ9wo7FjYU"}},{"cell_type":"code","source":["# Codifica el inicio del prompt.\n","if start.startswith('FILE:'):\n","    # Si el inicio del prompt indica que se debe utilizar un archivo, lee el contenido de este archivo como prompt.\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()  # Lee el contenido del archivo especificado como prompt inicial.\n","\n","# Codifica el texto inicial en una secuencia de identificadores utilizable por el modelo.\n","start_ids = encode(start)  # Esta función `encode` debe estar definida en alguna otra parte del código o biblioteca.\n","\n","# Prepara el tensor de entrada para el modelo con los identificadores codificados.\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])  # Añade una dimensión de lote al tensor.\n","\n","# Ejecución de la generación de texto.\n","with torch.no_grad():  # Desactiva el cálculo de gradientes para optimizar la generación.\n","    with ctx:  # Utiliza el contexto definido anteriormente para gestionar la precisión numérica.\n","        for k in range(num_samples):  # Genera el número especificado de muestras.\n","            # Utiliza el modelo para generar una secuencia de texto basada en la entrada y los parámetros especificados.\n","            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            # Decodifica la secuencia de identificadores generada a texto legible.\n","            print(decode(y[0].tolist()))\n","            print('---------------')  # Imprime un separador después de cada muestra generada.\n"],"metadata":{"id":"hQlFkfOtwnTN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736376988712,"user_tz":-60,"elapsed":41816,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"6fb81092-cbe5-4972-8adf-f2a4854c8374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","To the crown, in our golden crown:\n","Yet, taking may you, that you\n","But since you have you crown, your crown,\n","Your cares you deny, in your private life,\n","And your answer you answer.\n","\n","KING HENRY VI:\n","Sh, O happy woman,\n","Have left me, would say nothing;\n","For all the father of you would you would not remember'd?\n","\n","Second Keeper:\n","A crown'd I cannot now remain.\n","\n","KING HENRY VI:\n","Why, am I dead? do I not breathe a man?\n","Ah, simple men, you know not what you swear!\n","Look, as I blow this feather from my face,\n","And as the air blows it to me again,\n","Obeying with my wind when I do blow,\n","And yielding to another when it blows,\n","Commanded always by the greater gust;\n","Such is the lightness of you common men.\n","But do not break your oaths; for of that sin\n","My mild entreaty shall not make you guilty.\n","Go where you will, the king shall be commanded;\n","And be you kings, and I'll obey.\n","\n","First Keeper:\n","We are true subjects to the king, King Edward.\n","\n","KING HENRY VI:\n","So would you be again to Henry,\n","If he were seated as King Edward is.\n","\n","First Keeper:\n","We charge you, in God's name, and the king's,\n","To go with us unto the officers.\n","\n","KING HENRY VI:\n","In God's name, lead; your king's name be obey'd:\n","And what God will, that let your king perform;\n","And what he will, I humbly yield unto.\n","3 KING HENRY VI\n","\n","KING EDWARD IV:\n","Brother of Gloucester, at Saint Alban's field\n","This lady's husband, Sir Richard Grey, was slain, was slain,\n","His lands then seized on by the conqueror:\n","Her suit is now to repossess those lands;\n","Which we in justice cannot well deny,\n","Because in quarrel of the house of York\n","The worthy gentleman did lose his life.\n","\n","GLOUCESTER:\n","Your highness shall do well to grant her suit;\n","It were dishonour to deny it her.\n","\n","KING EDWARD IV:\n","It were no less\n","---------------\n","\n","\n","KING RICHARD III:\n","Why, Buckingham, I king, my brother there to be king,\n","To wail the foe, as it is done cannot greatly good.\n","\n","DUCHESS OF YORK:\n","As I be so.\n","\n","KING RICHARD III:\n","You shall you told me as I had as you.\n","\n","DUCHESS OF YORK:\n","As I had been remember'd to?\n","As I had been tedious days,\n","And yet the day,\n","Thou camest as I was thy husband'st the triumph in thy birth,\n","Who sues to make thee now, and cries 'God save thee now, now,'\n","For joyful mother, now what I, to make thee:\n","Dost thou wilt make me, good, not thy wife;\n","For one word is meant half my distress.\n","\n","KING RICHARD III:\n","Andeed, by your wife, I have been the kingdom;\n","Or I'll prove you in the kingdom of Margaret\n","Which you would have been\n","Than that he that raised me to the crown'd the crown.\n","\n","QUEEN ELIZABETH:\n","A murderous villain, and so still thou art.\n","\n","KING RICHARD III:\n","Why, madam?\n","\n","QUEEN ELIZABETH:\n","How canst thou woo her?\n","\n","KING RICHARD III:\n","That would I learn of you,\n","As one that are best acquainted with her humour.\n","\n","QUEEN ELIZABETH:\n","And wilt thou learn of me?\n","\n","KING RICHARD III:\n","Madam, with all my heart.\n","\n","QUEEN ELIZABETH:\n","Send to her, by the man that slew her brothers,\n","A pair of bleeding-hearts; thereon engrave\n","Edward and York; then haply she will weep:\n","Therefore present to her--as sometime Margaret\n","Did to thy father, steep'd in Rutland's blood,--\n","A handkerchief; which, say to her, say to her, did drain\n","The purple sap from her sweet brother's body\n","And bid her dry her weeping eyes therewith.\n","If this inducement force her not to love,\n","Send her a story of thy noble acts;\n","Tell her thou madest away her uncle Clarence,\n","Her uncle Rivers; yea, and, for her sake,\n","Madest quick conveyance with her good aunt Anne.\n","\n","KING RICHARD III:\n","Come, come, you\n","---------------\n","\n","Ind soon, my heaven she do this, my lord.\n","\n","GLOUCESTER:\n","\n","She may command me more fitter for that place than earth.\n","\n","LADY ANNE:\n","All men, if you will have power than earth.\n","\n","GLOUCESTER:\n","Say, madam, let me have power in place.\n","\n","LADY ANNE:\n","Well, set down the corse.\n","\n","GLOUCESTER:\n","Vouchsafe to wear this ring.\n","\n","LADY ANNE:\n","To take is not to give.\n","\n","GLOUCESTER:\n","Look, how this ring encompasseth finger.\n","Even so thy breast encloseth my poor heart;\n","Wear both of them are thine.\n","And if thy poor devoted suppliant may\n","But beg one favour at thy gracious hand,\n","Thou dost confirm his happiness for ever.\n","\n","LADY ANNE:\n","What is it?\n","\n","GLOUCESTER:\n","That it would please thee leave these sad designs\n","To him that hath more cause to be a mourner,\n","At Chertsey monastery this noble king,\n","And wet his grave with my repentant tears,\n","I will with all expedient duty see you:\n","For divers unknown reasons. I beseech you,\n","Grant me this boon.\n","\n","LADY ANNE:\n","With all my heart; and much it joys me too,\n","To see you are become so penitent.\n","Tressel and Berkeley, go along with me.\n","\n","GLOUCESTER:\n","Bid me farewell.\n","\n","LADY ANNE:\n","'Tis more than you deserve;\n","But since you teach me how to flatter you,\n","Imagine I have said farewell already.\n","\n","GLOUCESTER:\n","Sirs, take up the corse.\n","\n","GENTLEMEN:\n","Towards Chertsey, noble lord?\n","\n","GLOUCESTER:\n","No, to White-Friars; there attend my coining.\n","Was ever woman in this humour woo'd?\n","Was ever woman in this humour won?\n","I'll have her; but I will not keep her long.\n","What! I, that kill'd her husband and his father,\n","To take her in her heart's extremest hate,\n","With curses in her mouth, tears in her eyes,\n","The bleeding witness of her hatred by;\n","\n","---------------\n","\n","\n","CORIOLANUS:\n","Pray you,\n","Fare many as many odd,\n","That I have seen the many millions.\n","\n","SICINIUS:\n","'Tis not a voice, sir.\n","\n","CORIOLANUS:\n","Was made my reasons, sir; and services, I have\n","such a rod to\n","begged.\n","\n","MENENIUS:\n","But this is something odd.\n","\n","Second Senator:\n","An 'tis no matter.\n","\n","CORIOLANUS:\n","Pray you now, if it may stand with the\n","voices that I may be consul, I have here the\n","customary gown.\n","\n","Fourth Citizen:\n","You have deserved nobly of your country, and you\n","have not deserved nobly.\n","\n","CORIOLANUS:\n","Your enigma?\n","\n","Fourth Citizen:\n","You have been a scourge to her enemies, you have\n","been a rod to her friends; you have not indeed loved\n","the common people.\n","\n","CORIOLANUS:\n","You should account me the more virtuous that I have\n","not been common in my love. I will, sir, flatter my\n","sworn brother, the people, to earn a dearer\n","estimation of them; 'tis a condition they account\n","rather to have my hat than my heart, I will practise\n","the insinuating nod and be off to them most\n","counterfeitly; that is, sir, I will counterfeit the\n","bewitchment of some popular man and give it\n","bountiful to the desirers.\n","I may be consul.\n","\n","Fifth Citizen:\n","We hope to find you our friend; and therefore give\n","you our voices heartily.\n","\n","Fourth Citizen:\n","You have received many wounds for your country.\n","\n","Fourth Citizen:\n","You have received many wounds for your country.\n","\n","CORIOLANUS:\n","I will not seal your knowledge with showing them. I\n","will make much of your voices, and so trouble you no further.\n","\n","Both Citizens:\n","The gods give you joy, sir, heartily!\n","\n","CORIOLANUS:\n","Most sweet voices!\n","Better it is to die, better to starve,\n","Than crave the hire which first we do deserve.\n","Why in this woolvish toge should I stand here,\n","To beg of\n","---------------\n","\n","\n","MAMILLIUS:\n","Ay, good lord.\n","\n","LEONTES:\n","Why, Mamillius; thou'rt an honest man.\n","Camillo, this great sir will yet stay longer.\n","\n","CAMILLO:\n","You had much ado to make his anchor hold:\n","When you cast out, it still came home.\n","\n","LEONTES:\n","Didst note it?\n","\n","CAMILLO:\n","He would not stay at your petitions: made\n","His business more material.\n","\n","LEONTES:\n","Didst perceive it?\n","They're here with me already, whispering, rounding\n","'Sicilia is a so-forth:' 'tis far gone,\n","When I shall gust it last. How came't, Camillo,\n","That he did stay?\n","\n","CAMILLO:\n","At the good queen's entreaty.\n","\n","LEONTES:\n","At the queen's be't: 'good' should be pertinent\n","But, so it is, it is not. Was this taken\n","By any understanding pate but thine?\n","For thy conceit is soaking, will draw in\n","More than the common blocks: not noted, is't, is't,\n","But of the finer natures? by some severals\n","Of head-piece extraordinary? lower messes\n","Perchance are to this business purblind? say.\n","\n","CAMILLO:\n","Business, my lord! I think most understand\n","Bohemia stays here longer.\n","\n","LEONTES:\n","Ha!\n","\n","CAMILLO:\n","Stays here longer.\n","\n","LEONTES:\n","Ay, but why?\n","\n","CAMILLO:\n","To satisfy your highness and the entreaties\n","Of our most gracious mistress.\n","\n","LEONTES:\n","Satisfy!\n","The entreaties of your mistress! satisfy!\n","Let that suffice. I have trusted thee, Camillo,\n","With all the nearest things to my heart, as well\n","My chamber-councils, wherein, thou\n","Hast cleansed my bosom, I from thee departed\n","Thy penitent reform'd: but we have been\n","Deceived in thy integrity, deceived\n","In that which seems so.\n","\n","CAMILLO:\n","Be it forbid, my lord!\n","\n","LEONTES:\n","To bide upon't, thou art not honest, or,\n","If thou inclinest that way, thou art a coward,\n","Which hoxes\n","---------------\n","\n","If you should have been so ill time 'twere well.\n","\n","KING RICHARD III:\n","Why, so indeed! am I king,\n","YRREL:\n","The chaplain of the Tower hath buried them;\n","But how or in what place I do not know.\n","\n","KING RICHARD III:\n","Come to me, Tyrrel, soon at after supper,\n","And thou shalt tell the process of their death.\n","Meantime, but think how I may do thee good,\n","And be inheritor of thy desire.\n","Farewell till soon.\n","The son of Clarence have I pent up close;\n","His daughter meanly have I match'd in marriage;\n","The sons of Edward sleep in Abraham's bosom,\n","And Anne my wife hath bid the world good night.\n","Now, for I know the Breton Richmond aims\n","At young Elizabeth, my brother's daughter,\n","And, by that knot, looks proudly o'er the crown,\n","To her I go, a jolly thriving wooer.\n","\n","CATESBY:\n","My lord!\n","\n","KING RICHARD III:\n","Good news or bad, that thou comest in so bluntly?\n","\n","CATESBY:\n","Bad news, my lord: Ely is fled to Richmond;\n","And Buckingham, back'd with the hardy Welshmen,\n","Is in the field, and still his power increaseth.\n","\n","KING RICHARD III:\n","Ely with Richmond troubles me more near\n","Than Buckingham and his rash-levied army.\n","Come, I have heard that fearful commenting\n","Is leaden servitor to dull delay;\n","Delay leads impotent and snail-paced beggary\n","Then fiery expedition be my wing,\n","Jove's Mercury, and herald for a king!\n","Come, muster men: my counsel is my shield;\n","We must be brief when traitors brave the field.\n","\n","QUEEN MARGARET:\n","So, now prosperity begins to mellow\n","And drop into the rotten mouth of death.\n","Here in these confines slily have I lurk'd,\n","To watch the waning of mine adversaries.\n","A dire induction am I witness to,\n","And will to France, hoping the consequence\n","Will prove as bitter, black, and tragical.\n","Withdraw thee, wretched Margaret: who comes here?\n","\n","QUEEN ELIZABETH:\n","Ah, my young princes! ah, my tender babes!\n","My unblown flowers, new-appearing sweets!\n","If\n","---------------\n","\n","The slave of the field\n","May hence, and hear him.\n","\n","HASTINGS:\n","But I shall be well-month hence,\n","That they who brought me in my master drew\n","Into stop the boar did expect him here\n","And then scorn to look upon their tragedy.\n","I tell thee, Catesby--\n","\n","CATESBY:\n","What, my lord?\n","\n","HASTINGS:\n","Ere a fortnight make me elder,\n","I'll send some packing that yet think not on it.\n","\n","CATESBY:\n","'Tis a vile thing to die, my gracious lord,\n","When men are unprepared and look not for it.\n","\n","HASTINGS:\n","O monstrous! and so falls it out\n","With Rivers, Vaughan, Grey: and so 'twill do\n","With some men else, who think themselves as safe\n","As thou and I; who, as thou and I; who, are dear\n","To princely Richard and to Buckingham.\n","\n","CATESBY:\n","The princes both make high account his head upon the bridge.\n","\n","HASTINGS:\n","I know they do; and I have well deserved it.\n","Come on, come on; where is your boar-spear, man?\n","Fear you the boar, and go so unprovided?\n","STANLEY:\n","My lord, good morrow; good morrow, Catesby:\n","You may jest on, but, but, but, but, by the holy rood,\n","I do not like these several councils, I.\n","\n","HASTINGS:\n","My lord,\n","I hold my life as dear as you do yours;\n","And never in my life, I do protest,\n","Was it more precious to me than 'tis now:\n","Think you, but that I know our state secure,\n","I would be so triumphant as I am?\n","STANLEY:\n","The lords at Pomfret, when they rode from London,\n","Were jocund, and supposed their state was sure,\n","And they indeed had no cause to mistrust;\n","But yet, you see how soon the day o'ercast.\n","This sudden stag of rancour I misdoubt:\n","Pray God, I say, I say, I say, I prove a needless coward!\n","What, shall we toward the Tower? the day is spent.\n","\n","HASTINGS:\n","Come, come, have with you. Wot you what\n","---------------\n","\n","Thus far our side that, they stay, 'tis in a happy in sign of fear.\n","\n","BUSHY:\n","You peers, my lord, you not so,\n","Of you three blasts of,\n","Despair not the king.\n","\n","GREEN:\n","The wind that Bolingbroke.\n","\n","QUEEN:\n","Who shall hinder me?\n","I'll have an enemy,\n","And I no measure in me.\n","\n","GREEN:\n","No, no sorrow to keep him here in woe.\n","\n","QUEEN:\n","O, so, so that my husband!\n","\n","Can heaven will revenge! but a shame\n","\n","Griefs sick,\n","If thou art thyself in my breast,\n","But one in my breast, ere I see thee:\n","If thou dostop thy cheeks,\n","To teach me how to curse mine enemies!\n","\n","QUEEN:\n","O, banishment more slander of banishment\n","Than death can find more pain;\n","Than death can find me how to curse mine.\n","\n","GLOUCESTER:\n","Speak it again, and; for thy rage:\n","And for charity is too sharp,\n","O, let the damned son, and soul\n","In thy scorns drew'st rivers from our blood,\n","And then, to to the duke a clout\n","Steep'd in the faultless blood of pretty Rutland--\n","His curses, then from bitterness of soul\n","Denounced against thee, are all fall'n upon thee;\n","And God, not we, hath plagued thy bloody deed.\n","\n","QUEEN ELIZABETH:\n","So just is God, to right the innocent.\n","\n","HASTINGS:\n","O, 'twas the foulest deed to slay that babe,\n","And the most merciless that e'er was heard of!\n","\n","RIVERS:\n","Tyrants themselves wept when it was reported.\n","\n","DORSET:\n","No man but prophesied revenge for it.\n","\n","BUCKINGHAM:\n","Northumberland, then present, wept to see it.\n","\n","QUEEN MARGARET:\n","What were you snarling all before I came,\n","Ready to catch each other by the throat,\n","And turn you all your hatred now on me?\n","Did York's dread curse prevail so much with heaven?\n","That Henry's death, my lovely Edward's death,\n","\n","---------------\n","\n","LEONTES:\n","Once more, Paulina.\n","\n","PAULINA:\n","Most think I have done, sir, these forced\n","You have look upon Hermione as good light upon me\n","What I'ld shriek, that even your ears\n","Give way.\n","\n","LEONTES:\n","But O wife!\n","\n","PAULINA:\n","Were it is an old proverb to your charge\n","And, 'tis the worse. Behold, my lords,\n","Although the print be little, the whole matter\n","And copy of the father, eye, eye,\n","The trick of's frown, his forehead, nay, the valley,\n","The pretty dimples of his chin and cheek,\n","His smiles,\n","The very mould and frame of hand,\n","And thou, nail, good goddess Nature, which hast made it\n","So like to him that got it, if thou hast\n","The ordering of the mind too, 'mongst all colours\n","No yellow in't, lest she suspect, as he does,\n","Her children not her husband's!\n","\n","LEONTES:\n","A gross hag\n","And, lozel, thou art worthy to be hang'd,\n","That wilt not stay her tongue.\n","\n","ANTIGONUS:\n","Hang all the husbands\n","That cannot do that feat, you'll leave yourself\n","Hardly one subject.\n","\n","LEONTES:\n","Once more, take her hence.\n","\n","PAULINA:\n","A most unworthy and unnatural lord\n","Can do no more.\n","\n","LEONTES:\n","I'll ha' thee burnt.\n","\n","PAULINA:\n","I care not:\n","It is an heretic that makes the fire,\n","Not she which burns in't. I'll not call you tyrant;\n","But this most cruel usage of your queen,\n","Not able to produce more accusation\n","Than your own weak-hinged fancy, something savours\n","Of tyranny and will ignoble make you,\n","Yea, scandalous to the world.\n","\n","LEONTES:\n","On your allegiance,\n","Out of the chamber with her! Were I a tyrant,\n","Where were her life? she durst not call me so,\n","If she did know me one. Away with her!\n","\n","PAULINA:\n","I pray you, do not push me; I'll be gone.\n","Look to your babe, my lord; 'tis yours:\n","\n","---------------\n","\n","Not in this gracious lady.\n","\n","LEONTES:\n","Although I'll go not be, yet:\n","Thou wilt not be the fool,\n","If I receive thee burnt. Prithee, bring me to the\n","One grave and queen and queen and father;\n","And see good Paulina more.' And thou, bring me\n","Thy wife Paulina more shalt see\n","We can behold the statue of our queen: your gallery\n","Have we pass'd through, not without much content\n","In many goodly things now not see, not see,\n","Myself, nor Nature of her, she was as it.\n","\n","PAULINA:\n","As she lived peerless,\n","So her dead likeness, I do well believe,\n","Excels whatever yet you look'd upon\n","Or hand of man hath done; therefore I keep it\n","Lonely, apart. But here it is: prepare\n","To see the life as lively mock'd as ever\n","Still sleep mock'd death: behold, and say 'tis well.\n","I like your silence, it the more shows off\n","Your wonder: but yet speak; first, you, my liege,\n","Comes it not something near?\n","\n","LEONTES:\n","Her natural posture!\n","Chide me, dear stone, that I may say indeed\n","Thou art Hermione; or rather, thou art she\n","In thy not chiding, for she was as tender\n","As infancy and grace. But yet, Paulina,\n","Hermione was not so much wrinkled, nothing\n","So aged as this seems.\n","\n","POLIXENES:\n","O, not by much.\n","\n","PAULINA:\n","So much the more our carver's excellence;\n","Which lets go by some sixteen years and makes her\n","As she lived now.\n","\n","LEONTES:\n","As now she might have done,\n","So much to my good comfort, as it is\n","Now piercing to my soul. O, thus she stood,\n","Even with such life of majesty, warm life,\n","As now it coldly stands, when first I woo'd her!\n","I am ashamed: does not the stone rebuke me\n","For being more stone than it? O royal piece,\n","There's magic in thy majesty, which has\n","My evils conjured to remembrance and\n","From thy admiring daughter took the spirits,\n","Standing like stone with thee.\n","\n","\n","---------------\n"]}]}]}