{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFOWBeT9gKlZ1wlatld3Ej"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Agentic RAG"],"metadata":{"id":"-dZrB4pnbED5"}},{"cell_type":"markdown","source":["Crearemos un Agente capaz de realizar búsquedas en internet con DuckduckGo y también añadiremos RAG de artículos científicos de Arxiv\n"],"metadata":{"id":"pqjpTTmqbL3g"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9X_GU4AWaztX","executionInfo":{"status":"ok","timestamp":1747941948372,"user_tz":-120,"elapsed":39652,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"5fc8abd8-8e77-4397-8b0c-c9c0239f94d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-cloud-bigquery 3.32.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -qU langchain langchain_openai langchain_community langgraph==0.0.20 arxiv duckduckgo-search"]},{"cell_type":"code","source":["!pip install -qU faiss-cpu pymupdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBex21r7bafH","outputId":"72270f2b-8534-4ca9-ec89-cf7b537fc797","executionInfo":{"status":"ok","timestamp":1747941963577,"user_tz":-120,"elapsed":15203,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## Configuración de la API de OpenAI\n","Este código importa las bibliotecas necesarias para trabajar con la API de OpenAI, carga las variables de entorno desde un archivo .env y configura la clave de la API de OpenAI para ser utilizada en las solicitudes a la API.\n"],"metadata":{"id":"rWPwOIE9b_nP"}},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJUKuyXucCtp","executionInfo":{"status":"ok","timestamp":1747941970244,"user_tz":-120,"elapsed":6662,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"a817d1f9-7fba-4670-b3b8-d2f55373a206"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["OpenAI API Key:··········\n"]}]},{"cell_type":"markdown","source":["### Retrieval\n","\n","Primero, configuraremos un sistema local de recuperación sencillo que busca artículos de Arxiv sobre el tema de RAG.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"nuZN0-_Dcyqn"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import ArxivLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import OpenAIEmbeddings\n","\n","docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=350, chunk_overlap=50\n",")\n","\n","chunked_documents = text_splitter.split_documents(docs)\n","\n","faiss_vectorstore = FAISS.from_documents(\n","    documents=chunked_documents,\n","    embedding=OpenAIEmbeddings(),\n",")\n","\n","retriever = faiss_vectorstore.as_retriever()"],"metadata":{"id":"R_ydm_1vc8cz","executionInfo":{"status":"ok","timestamp":1747942036731,"user_tz":-120,"elapsed":22842,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Augmented\n","\n","¡Ahora que tenemos nuestro sistema de recuperación, podemos crear nuestro prompt para hacer RAG!\n"],"metadata":{"id":"d6MgNzGtdQ45"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","RAG_PROMPT = \"\"\"\\\n","Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n","\n","Question:\n","{question}\n","\n","Context:\n","{context}\n","\"\"\"\n","\n","rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"],"metadata":{"id":"y-nGYtl8XVTi","executionInfo":{"status":"ok","timestamp":1747942041625,"user_tz":-120,"elapsed":8,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Generation\n","\n","Añadiremos lo que conseguimos del Retrieval al Prompt como contexto\n"],"metadata":{"id":"v93CGocBXXws"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","openai_chat_model = ChatOpenAI(model=\"gpt-4.1-mini\")"],"metadata":{"id":"NdZiLwefXfxs","executionInfo":{"status":"ok","timestamp":1747942049931,"user_tz":-120,"elapsed":120,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Ahora crearemos el chain\n"],"metadata":{"id":"GhyP7J13qZh5"}},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain.schema.output_parser import StrOutputParser\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","retrieval_augmented_generation_chain = (\n","    # INVOCAR LA CADENA CON: {\"question\" : \"<<PREGUNTA DEL USUARIO>>\"}\n","    # \"question\" : el valor de la clave \"question\"\n","    # \"context\"  : el valor de la clave \"question\" y encadenandolo con el base_retriever\n","    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n","    # \"context\"  : se asigna a un objeto RunnablePassthrough (no será llamado ni considerado en el siguiente paso)\n","    #              obteniendo el valor de la clave \"context\" del paso anterior\n","    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n","    # \"response\" : Los valores de \"context\" y \"question\" se utilizan para formatear nuestro objeto de prompt y luego se encadenan\n","    #              en el LLM y se almacenan en una clave llamada \"response\"\n","    # \"context\"  : Poblada obteniendo el valor de la clave \"context\" del paso anterior\n","    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",")\n"],"metadata":{"id":"NX46ehKAqoIs","executionInfo":{"status":"ok","timestamp":1747942073867,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"Que es RAG?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9u9VKy5mqtS6","executionInfo":{"status":"ok","timestamp":1747942081134,"user_tz":-120,"elapsed":3356,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"4a5309d6-9e52-4839-85f9-4f0b2fb0ef60"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'response': AIMessage(content='RAG (Retrieval-Augmented Generation) es un enfoque que mejora los modelos de lenguaje al incorporar información relevante obtenida de bases de conocimiento externas. Funciona bajo el paradigma \"recuperar y luego leer\": primero, se utiliza la pregunta de entrada como consulta para recuperar documentos relevantes mediante un módulo de recuperación; luego, estos documentos recuperados junto con la pregunta se combinan como entrada completa para que el modelo genere la respuesta final. Este método ayuda a reducir errores factuales en tareas que requieren mucho conocimiento al proporcionar datos actualizados o específicos que el modelo por sí solo no tendría.', response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 2546, 'total_tokens': 2664, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini', 'system_fingerprint': 'fp_71b9d4b387', 'finish_reason': 'stop', 'logprobs': None}, id='run-784f548f-6716-427c-9aba-7ae3fec7e281-0'),\n"," 'context': [Document(page_content='3\\nRelated Work\\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from\\nvarious external knowledge bases has been shown to significantly improve the performance of various NLP tasks,\\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023; Lin et al., 2023) and open domain\\nquestion answering (Izacard et al., 2022; Zhang et al., 2024). RAG mainly adopts the \"retrieve then read\" paradigm.\\nSpecifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from\\nthe external knowledge base, and finally the retrieved documents and questions are merged into a complete input\\nto generate final output. For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on\\nrelevant documents through chunked cross-attention, thereby introducing new parameters to the model. REPLUG (Shi\\net al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever. RAFT (Zhang et al., 2024)\\nproposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train\\nlanguage models for domain-specific open-book settings.\\nFinetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the\\nLLM or retriever in the RAG framework. For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}),\n","  Document(page_content='However, traditional RAG suffers from the illusion problem (Zhang et al., 2023), leading to incorrect answers, and\\nwhen faced with more complex questions, limited by the performance of the retriever, RAG may encounter difficulties\\nin answering based on poorly correlated or even erroneous external documents.\\nIn this paper, we propose a novel Collaborative Retrieval-Augmented Generation framework, called DuetRAG, based\\non RAG and domain knowledge fine-tuning. This framework leverages both fine-tuned models and RAG to generate\\nanswers to questions, and a referee model is employed to determine the selection of the final answer. The aim of\\nDuetRAG is not only to equip models with domain-specific knowledge but also to enable them to utilize external\\ndocuments to obtain answers when internal knowledge is uncertain. By complementing internal and external knowledge,\\nDuetRAG aims to enhance the robustness of the model. We provide a detailed description of the DuetRAG methodology\\narXiv:2405.13002v1  [cs.CL]  12 May 2024\\nDuetRAG: Collaborative Retrieval-Augmented Generation\\nA PREPRINT\\nand its performance on various datasets in Section 3. DuetRAG outperforms existing RAG and fine-tuned models on\\nthe HotPot QA dataset (Yang et al., 2018), presenting a new pipeline for QA.\\n3\\nRelated Work\\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from\\nvarious external knowledge bases has been shown to significantly improve the performance of various NLP tasks,', metadata={'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}),\n","  Document(page_content='R2AG\\nRetriever\\nLLM\\nCombine\\nQuery &\\nDocuments\\nQuery\\nTop-k\\nDocuments\\nR2-Former\\n/\\nRetriever\\nLLM\\nCombine\\nQuery &\\nDocuments\\nQuery\\nTop-k\\nDocuments\\nRetrieval-aware\\nPrompting\\nSemantic Gap\\nFigure 1: A comparison between RAG and R2AG.\\nR2AG employs a trainable R2-Former to bridge the\\nsemantic gap between retrievers and LLMs. Optionally,\\nLLMs can be fine-tuned to understand the retrieval in-\\nformation further.\\ntasks, RAG offers real-time knowledge with high\\ninterpretability to LLMs, effectively mitigating the\\nhallucination problem (Mallen et al., 2023).\\nHowever, there exists a semantic gap between re-\\ntrievers and LLMs due to their vastly different train-\\ning objectives and architectures (BehnamGhader\\net al., 2022). Specifically, retrievers, typically en-\\ncoder architecture, are designed to retrieve the most\\nrelevant documents for a query (Zhu et al., 2023b).\\nConversely, LLMs, generally decoder architecture,\\nare expected to answer questions based on their\\ninherent knowledge or given documents. How-\\never, the interaction between retrievers and LLMs\\nin RAG primarily relies on simple text concatena-\\ntion (BehnamGhader et al., 2022). This poor com-\\nmunication strategy will lead to several challenges\\nfor LLMs. Externally, it is hard for LLMs to uti-', metadata={'Published': '2024-10-30', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"}),\n","  Document(page_content='specific retrievers, including audios (Koizumi et al.,\\n2020), images (Yasunaga et al., 2023), knowledge\\ngraphs (He et al., 2024), and so on. Despite its\\nrapid growth, RAG suffers several limitations, such\\nas sensitivity to retrieval results, increased com-\\nplexity, and a semantic gap between retrievers and\\nLLMs (Kandpal et al., 2022; Zhao et al., 2024).\\n2.2\\nEnhanced RAG\\nRecent works develop many enhanced approaches\\nbased on the standard RAG framework. To directly\\nimprove the effectiveness of RAG, REPLUG (Shi\\net al., 2023) and Atlas (Izacard et al., 2022) lever-\\nage the LLM to provide a supervisory signal for\\ntraining a better retriever. However, the noise will\\ninevitably appear in retrieval results (Barnett et al.,\\n2024).\\nRecent studies focus on pre-processing\\nthe retrieved documents before providing them\\nto LLMs. Techniques such as truncation and se-\\nlection are effective methods to enhance the qual-\\nity of ranking lists without modifying the content\\nof documents (Gao et al., 2023; Xu et al., 2024).\\nCRAG (Yan et al., 2024) trains a lightweight re-\\ntrieval evaluator to exclude irrelevant documents.\\nBGM (Ke et al., 2024) is proposed to meet the\\npreference of LLMs by training a bridge model to\\nre-rank and select the documents. Some studies\\naim to train small LMs to compress the retrieval\\ndocuments, thus decreasing complexity or reducing', metadata={'Published': '2024-10-30', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"})]}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Continuar hasta que se han respondido todas las preguntas\n","\n","Los agentes responden a una de las preguntas, pero no deberían parar hasta que hayan abordado el problema de todas las maneras que tienen disponibles. Utilizaremos LangGraph, otra herramienta de ejecución de gráficos de Langchain.\n"],"metadata":{"id":"XmbDwqPjr-g5"}},{"cell_type":"markdown","source":["## Toolbelt\n","\n","Crearemos un **Toolbelt** con dos herramientas. El buscador DuckDuckGo y un buscador de artículos en Arxiv\n","\n","- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n","- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)\n"],"metadata":{"id":"Fh6yqP-usRzy"}},{"cell_type":"code","source":["from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n","from langchain_community.tools.arxiv.tool import ArxivQueryRun\n","\n","tool_belt = [\n","    DuckDuckGoSearchRun(),\n","    ArxivQueryRun()\n","]"],"metadata":{"id":"GuRcA_jZsgYI","executionInfo":{"status":"ok","timestamp":1747942241034,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langgraph.prebuilt import ToolExecutor\n","\n","tool_executor = ToolExecutor(tool_belt)"],"metadata":{"id":"GqdjP_Lwsq1Q","executionInfo":{"status":"ok","timestamp":1747942242263,"user_tz":-120,"elapsed":1,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI(temperature=0, model=\"gpt-4.1-mini\")"],"metadata":{"id":"LeKqZ06PsuHy","executionInfo":{"status":"ok","timestamp":1747942242999,"user_tz":-120,"elapsed":118,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from langchain_core.utils.function_calling import convert_to_openai_function\n","\n","functions = [convert_to_openai_function(t) for t in tool_belt]\n","model = model.bind_functions(functions)"],"metadata":{"id":"f0IWuFUesyOt","executionInfo":{"status":"ok","timestamp":1747942243562,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Modificando el estado del agente\n","\n","Dicho de manera sencilla: queremos tener una especie de objeto que podamos pasar por nuestra aplicación que contenga información sobre cuál es la situación actual (estado). Como nuestro sistema estará construido de muchas partes que se mueven de manera coordinada, queremos poder asegurarnos de que tenemos una idea comúnmente entendida de este estado.\n","\n","LangGraph aprovecha un `StatefulGraph` que utiliza un objeto `AgentState` para pasar información entre los diferentes nodos del gráfico.\n","\n","Hay más opciones de las que veremos a continuación, pero este objeto `AgentState` es uno que se guarda en un `TypedDict` con la clave `messages` y el valor es una `Sequence` de `BaseMessages` que será añadida siempre que el estado cambie.\n","\n","Pensemos en un ejemplo sencillo para ayudar a entender exactamente qué significa esto (simplificaremos mucho para intentar comunicar claramente qué está haciendo el estado):\n","\n","1. Inicializamos nuestro objeto de estado:\n","   - `{\"messages\" : []}`\n","2. Nuestro usuario envía una consulta a nuestra aplicación.\n","   - Nuevo Estado: `HumanMessage(#1)`\n","   - `{\"messages\" : [HumanMessage(#1)]}`\n","3. Pasamos nuestro objeto de estado a un nodo Agente que es capaz de leer el estado actual. Utilizará el último `HumanMessage` como entrada. Obtiene una especie de salida que añadirá al estado.\n","   - Nuevo Estado: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n","   - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n","4. Pasamos nuestro objeto de estado a un \"nodo condicional\" (más sobre esto más adelante) que lee el último estado para determinar si necesitamos utilizar una herramienta, cosa que puede determinar adecuadamente gracias a nuestro objeto proporcionado.\n"],"metadata":{"id":"1eP2cPoEyUBf"}},{"cell_type":"code","source":["from typing import TypedDict, Annotated, Sequence\n","import operator\n","from langchain_core.messages import BaseMessage\n","\n","class AgentState(TypedDict):\n","  messages: Annotated[Sequence[BaseMessage], operator.add]"],"metadata":{"id":"gO1Sk_kgytnB","executionInfo":{"status":"ok","timestamp":1747942245975,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Vamos a montar nuestro Graph\n","\n","![image](https://firebasestorage.googleapis.com/v0/b/kingsleague-22e86.appspot.com/o/Untitled-2024-01-07-11129.png?alt=media&token=3fdc6ab2-51a8-49b2-baf1-109515282cd4)\n"],"metadata":{"id":"ZexIDYWmy_hV"}},{"cell_type":"markdown","source":["Tenemos para empezar la llamada al modelo y la llamada a una herramienta:\n"],"metadata":{"id":"3RvTkkIAzI_9"}},{"cell_type":"code","source":["from langgraph.prebuilt import ToolInvocation\n","import json\n","from langchain_core.messages import FunctionMessage\n","\n","def call_model(state):\n","  messages = state[\"messages\"]\n","  response = model.invoke(messages)\n","  return {\"messages\" : [response]}\n","\n","def call_tool(state):\n","  last_message = state[\"messages\"][-1]\n","\n","  action = ToolInvocation(\n","      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n","      tool_input=json.loads(\n","          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n","      )\n","  )\n","\n","  response = tool_executor.invoke(action)\n","\n","  function_message = FunctionMessage(content=str(response), name=action.tool)\n","\n","  return {\"messages\" : [function_message]}"],"metadata":{"id":"5ujB1MZxzNZ7","executionInfo":{"status":"ok","timestamp":1747942274329,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Estas funciones serán nodos de nuestro Graph de agente\n"],"metadata":{"id":"dSOZwgHNzRF0"}},{"cell_type":"code","source":["from langgraph.graph import StateGraph, END\n","\n","workflow = StateGraph(AgentState)\n","\n","workflow.add_node(\"agent\", call_model)\n","workflow.add_node(\"action\", call_tool)"],"metadata":{"id":"3VlJ3DJ1zVce","executionInfo":{"status":"ok","timestamp":1747942290450,"user_tz":-120,"elapsed":6,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Definimos el punto de entrada del agente\n"],"metadata":{"id":"_P8KxXARzX8F"}},{"cell_type":"code","source":["workflow.set_entry_point(\"agent\")"],"metadata":{"id":"bz-XqzxVzaPB","executionInfo":{"status":"ok","timestamp":1747942291479,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["Ahora queremos construir un \"conditional edge\" que utilizará el estado de salida de un nodo para determinar qué camino seguir.\n","\n","¡Podemos ayudar a conceptualizar esto pensando en nuestro conditional edge como una condicional en un diagrama de flujo!\n","\n","Vean cómo nuestra función simplemente verifica si hay un argumento `function_call` presente.\n","\n","Luego creamos un edge donde el nodo de origen es nuestro nodo agente y el nodo de destino es o bien el nodo de acción o el FINAL (terminar el gráfico).\n","\n","Es importante destacar que el diccionario pasado como tercer parámetro (el mapeo) debería crearse teniendo en cuenta las posibles salidas de nuestra función condicional. En este caso, `should_continue` produce \"end\" o \"continue\", que posteriormente se mapean al nodo de acción o al nodo FINAL.\n"],"metadata":{"id":"i_XycR4I2i2B"}},{"cell_type":"code","source":["def should_continue(state):\n","  last_message = state[\"messages\"][-1]\n","\n","  if \"function_call\" not in last_message.additional_kwargs:\n","    return \"end\"\n","\n","  return \"continue\"\n","\n","workflow.add_conditional_edges(\n","    \"agent\",\n","    should_continue,\n","    {\n","        \"continue\" : \"action\",\n","        \"end\" : END\n","    }\n",")"],"metadata":{"id":"w577nlvO22O5","executionInfo":{"status":"ok","timestamp":1747942353007,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, podemos agregar el último edge, que conectará nuestro nodo de acción con nuestro nodo agente. Esto es porque *siempre* queremos que nuestro nodo de acción (que se utiliza para llamar nuestras herramientas) devuelva su salida a nuestro agente.\n"],"metadata":{"id":"GxELGoUQ25D9"}},{"cell_type":"code","source":["workflow.add_edge(\"action\", \"agent\")"],"metadata":{"id":"GAGu4hxj3dYh","executionInfo":{"status":"ok","timestamp":1747942354029,"user_tz":-120,"elapsed":7,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Y ya podemos compilar el Graph\n"],"metadata":{"id":"GRB_MrgB3gnc"}},{"cell_type":"code","source":["app = workflow.compile()"],"metadata":{"id":"yU_SBA3V3jA3","executionInfo":{"status":"ok","timestamp":1747942355538,"user_tz":-120,"elapsed":2,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## Utilizando el Graph\n"],"metadata":{"id":"0syWsZ4E3pZH"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","\n","inputs = {\"messages\" : [HumanMessage(content=\"¿Qué es RAG? ¿Quien es el fundador de OpenAI?\")]}\n","\n","app.invoke(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"LQLgDqCO3r9l","executionInfo":{"status":"error","timestamp":1747942362622,"user_tz":-120,"elapsed":3735,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"4c967519-76e2-49c5-b44f-ef0e21d17952"},"execution_count":29,"outputs":[{"output_type":"error","ename":"InvalidUpdateError","evalue":"Invalid update for channel __end__: LastValue can only receive one value per step.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36m_apply_writes\u001b[0;34m(checkpoint, channels, pending_writes, config, for_step)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m                 \u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/channels/last_value.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LastValue can only receive one value per step.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidUpdateError\u001b[0m: LastValue can only receive one value per step.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-6546f00b86cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"¿Qué es RAG?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     ) -> Union[dict[str, Any], Any]:\n\u001b[1;32m    568\u001b[0m         \u001b[0mlatest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     ) -> Iterator[Union[dict[str, Any], Any]]:\n\u001b[0;32m--> 605\u001b[0;31m         for chunk in self._transform_stream_with_config(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m                     \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfinal_output_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# apply writes to channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     _apply_writes(\n\u001b[0m\u001b[1;32m    351\u001b[0m                         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpending_writes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36m_apply_writes\u001b[0;34m(checkpoint, channels, pending_writes, config, for_step)\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                 raise InvalidUpdateError(\n\u001b[0m\u001b[1;32m    739\u001b[0m                     \u001b[0;34mf\"Invalid update for channel {chan}: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 ) from e\n","\u001b[0;31mInvalidUpdateError\u001b[0m: Invalid update for channel __end__: LastValue can only receive one value per step."]}]},{"cell_type":"markdown","source":["¿Qué ha hecho el graph?\n","\n","1. Nuestro objeto de estado se llena con nuestra petición.\n","2. El objeto de estado se pasa a nuestro punto de entrada (node agent) y el node agent añade un `AIMessage` al objeto de estado y lo pasa al condicional.\n","3. El condicional recibe el objeto de estado, mira el `additional_kwarg` \"function_call\", y envía el objeto de estado al nodo de acción.\n","4. El nodo de acción añade la respuesta de la función endpoint de OpenAI al objeto de estado y lo devuelve al node agent.\n","5. El node agent añade una respuesta al objeto de estado y lo devuelve al condicional.\n","6. El condicional recibe el objeto de estado, y como no encuentra `additional_kwarg` \"function_call\", finaliza el GRAPH.\n"],"metadata":{"id":"_ou6QHwK5z7O"}},{"cell_type":"markdown","source":["## Agentic RAG\n","\n","Básicamente lo que buscamos es intentar solucionar la consulta con una cadena de RAG, comprobar si la pregunta ha sido totalmente respondida, y en caso contrario aplicar el Graph, de lo contrario terminar la ejecución:\n","\n","![image](https://firebasestorage.googleapis.com/v0/b/kingsleague-22e86.appspot.com/o/Untitled-2024-01-07-1119.png?alt=media&token=9eebd891-a1c8-482b-b6f4-f850f5c3d60e)\n"],"metadata":{"id":"QVF6kZjR_m1I"}},{"cell_type":"markdown","source":["Definimos la RAG Chain\n"],"metadata":{"id":"W0ZufXNuAEG6"}},{"cell_type":"code","source":["def convert_state_to_query(state_object):\n","  return {\"question\" : state_object[\"messages\"][-1].content}\n","\n","def convert_response_to_state(response):\n","  return {\"messages\" : [response[\"response\"]]}\n","\n","langgraph_node_rag_chain = convert_state_to_query | retrieval_augmented_generation_chain | convert_response_to_state"],"metadata":{"id":"Cdc-GDeh_4-D","executionInfo":{"status":"ok","timestamp":1747942420502,"user_tz":-120,"elapsed":42,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["await langgraph_node_rag_chain.ainvoke(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ex6vgX5AQ1V","executionInfo":{"status":"ok","timestamp":1747942432157,"user_tz":-120,"elapsed":8412,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"65b3aa11-47f9-4a14-a36a-5452122f1533"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [AIMessage(content='RAG (Retrieval-Augmented Generation) es un enfoque que combina modelos de lenguaje grandes (LLMs) con un componente de recuperación de información. Funciona bajo el paradigma de \"recuperar y luego leer\": primero, se utiliza la pregunta como consulta para recuperar documentos relevantes de una base de conocimiento externa, y luego esos documentos recuperados junto con la pregunta se integran como entrada para que el modelo genere una respuesta final.\\n\\nEl objetivo de RAG es mejorar la precisión y reducir errores factuales en tareas que requieren mucho conocimiento, permitiendo que los modelos accedan a información actualizada y relevante fuera de su conocimiento interno. Sin embargo, RAG tradicional puede enfrentar problemas como la \"ilusión\" o generación de respuestas incorrectas debido a documentos externos mal correlacionados o erróneos.\\n\\nEn resumen, RAG es una técnica que potencia a los modelos de lenguaje mediante la incorporación de información recuperada externamente para mejorar la generación de respuestas en tareas de procesamiento de lenguaje natural.', response_metadata={'token_usage': {'completion_tokens': 199, 'prompt_tokens': 2529, 'total_tokens': 2728, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini', 'system_fingerprint': 'fp_79b79be41f', 'finish_reason': 'stop', 'logprobs': None}, id='run-ce2ebfc6-eddb-4f41-92c1-2c979e990673-0')]}"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Ahora añadiremos nuestros nodos - observen que estamos incluyendo nuestro componente LCEL recién construido como un nodo llamado first_action.\n","\n","La idea básica es que utilizaremos nuestro RAG privado configurado - y si se considera suficiente, devolveremos esta respuesta a nuestro usuario; y si no, aumentaremos nuestra respuesta con las otras herramientas!\n"],"metadata":{"id":"qxJzIuMjFHcU"}},{"cell_type":"code","source":["rag_agent = StateGraph(AgentState)\n","\n","rag_agent.add_node(\"agent\", call_model)\n","rag_agent.add_node(\"action\", call_tool)\n","rag_agent.add_node(\"first_action\", langgraph_node_rag_chain)"],"metadata":{"id":"cxovBbYJFODw","executionInfo":{"status":"ok","timestamp":1747942440072,"user_tz":-120,"elapsed":29,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["rag_agent.set_entry_point(\"first_action\")"],"metadata":{"id":"kwQ7qzwSFPSy","executionInfo":{"status":"ok","timestamp":1747942441392,"user_tz":-120,"elapsed":5,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["Ahora tenemos que crear la función `is_fully_answered` que decidirá si entrar o no dentro del graph.\n"],"metadata":{"id":"jjdieeq2FRkS"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from langchain.output_parsers.openai_tools import PydanticToolsParser\n","from langchain_core.utils.function_calling import convert_to_openai_tool\n","\n","def is_fully_answered(state):\n","\n","  ### Extraer la pregunta y la respuesta de nuestra cadena RAG\n","  question = state[\"messages\"][0].content\n","  answer = state[\"messages\"][-1].content\n","\n","  ### Crear un modelo Pydantic para capturar la respuesta de nuestros LLMs\n","  class answered(BaseModel):\n","    binary_score: str = Field(description=\"Completamente respondido: 'sí' o 'no'\")\n","\n","  ### Un modelo de razonamiento potente asegurará que podemos responder adecuadamente nuestra pregunta\n","  model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n","\n","  ### Crear y vincular nuestra herramienta a nuestro modelo\n","  answered_tool = convert_to_openai_tool(answered)\n","\n","  model = model.bind(\n","      tools=[answered_tool],\n","      tool_choice={\"type\" : \"function\", \"function\" : {\"name\" : \"answered\"}}\n","  )\n","\n","  ### Querremos analizar la salida en un formato utilizable\n","  parser_tool = PydanticToolsParser(tools=[answered])\n","\n","  prompt = PromptTemplate(\n","      template=\"\"\"Determinarás si la pregunta está completamente respondida por la respuesta.\\n\n","      Pregunta:\n","      {question}\n","\n","      Respuesta:\n","      {answer}\n","\n","      Responderás con 'sí' o 'no'.\"\"\",\n","      input_variables=[\"question\", \"answer\"])\n","\n","  ### ¡Cadena LCEL clásica!\n","  fully_answered_chain = prompt | model | parser_tool\n","\n","  response = fully_answered_chain.invoke({\"question\" : question, \"answer\" : answer})\n","\n","  if response[0].binary_score == \"no\":\n","    return \"continue\"\n","\n","  return \"end\"\n"],"metadata":{"id":"jJtV2eilF3-D","executionInfo":{"status":"ok","timestamp":1747942466458,"user_tz":-120,"elapsed":4,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["rag_agent.add_conditional_edges(\n","    \"first_action\",\n","    is_fully_answered,\n","    {\n","        \"continue\" : \"agent\",\n","        \"end\" : END\n","    }\n",")"],"metadata":{"id":"ARYKnCnxF6Ug","executionInfo":{"status":"ok","timestamp":1747942476642,"user_tz":-120,"elapsed":9,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def should_continue(state):\n","  last_message = state[\"messages\"][-1]\n","\n","  if \"function_call\" not in last_message.additional_kwargs:\n","    return \"end\"\n","\n","  return \"continue\"\n","\n","rag_agent.add_conditional_edges(\n","    \"agent\",\n","    should_continue,\n","    {\n","        \"continue\" : \"action\",\n","        \"end\" : END\n","    }\n",")"],"metadata":{"id":"4-h1czoZF9Nk","executionInfo":{"status":"ok","timestamp":1747942478859,"user_tz":-120,"elapsed":41,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["Definimos la finalización de nuestro agente:\n"],"metadata":{"id":"cJSAmCRnF_CB"}},{"cell_type":"code","source":["rag_agent.add_edge(\"action\", \"agent\")"],"metadata":{"id":"SQececiaGCx6","executionInfo":{"status":"ok","timestamp":1747942485068,"user_tz":-120,"elapsed":2,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["Compilamos el agente"],"metadata":{"id":"QeZQZXu2GFGf"}},{"cell_type":"code","source":["rag_agent_app = rag_agent.compile()"],"metadata":{"id":"atD-iZznGEqL","executionInfo":{"status":"ok","timestamp":1747942487707,"user_tz":-120,"elapsed":1,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["## Let's run it!"],"metadata":{"id":"cFlLiXy9GKJ0"}},{"cell_type":"code","source":["question = \"Who is the main author on the Retrieval Augmented Generation paper?\"\n","\n","inputs = {\"messages\" : [HumanMessage(content=question)]}\n","\n","rag_agent_app.invoke(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFic8N6xGMbZ","executionInfo":{"status":"ok","timestamp":1747942493092,"user_tz":-120,"elapsed":2302,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"417c6ca2-8091-4767-a18e-1c0059744fad"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='Who is the main author on the Retrieval Augmented Generation paper?'),\n","  AIMessage(content='The main author on the Retrieval Augmented Generation paper is Mike Lewis.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 2277, 'total_tokens': 2291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini', 'system_fingerprint': 'fp_71b9d4b387', 'finish_reason': 'stop', 'logprobs': None}, id='run-3bb2ab31-efff-4b73-9d1c-4abd64623f2e-0')]}"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["Esta pregunta no ha entrado en el bucle, vamos a forzar que entre en el bucle\n"],"metadata":{"id":"78kgLJ28HEyg"}},{"cell_type":"code","source":["question = \"Who is the main author on the Retrieval Augmented Generation paper - and what University did they attend? Quien es Elon Musk?\"\n","\n","inputs = {\"messages\" : [HumanMessage(content=question)]}\n","\n","rag_agent_app.invoke(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VztFJ3ldKXZE","executionInfo":{"status":"ok","timestamp":1747942519580,"user_tz":-120,"elapsed":3466,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"a4880dc2-9100-47d7-8deb-5423f88bf6c7"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='Who is the main author on the Retrieval Augmented Generation paper - and what University did they attend? Quien es Elon Musk?'),\n","  AIMessage(content='The main author of the Retrieval Augmented Generation paper is Mike Lewis. The context does not explicitly state which university he attended. \\n\\nRegarding your second question, \"¿Quién es Elon Musk?\" — Elon Musk is a well-known entrepreneur, CEO of companies such as Tesla and SpaceX, but this information is not included in the provided context. \\n\\nTherefore, based on the provided context:  \\n- Main author of the Retrieval Augmented Generation paper: Mike Lewis  \\n- University attended by Mike Lewis: I don\\'t know  \\n- Who is Elon Musk: I don\\'t know', response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 2310, 'total_tokens': 2422, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini', 'system_fingerprint': 'fp_71b9d4b387', 'finish_reason': 'stop', 'logprobs': None}, id='run-34e08777-b6cd-49de-bfa4-012dcf22b6e1-0')]}"]},"metadata":{},"execution_count":41}]}]}