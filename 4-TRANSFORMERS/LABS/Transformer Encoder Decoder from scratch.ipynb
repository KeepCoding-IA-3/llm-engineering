{"cells":[{"cell_type":"markdown","metadata":{"id":"9ZwKP3f2HGz9"},"source":["# Transformer Encoder Decoder from scratch\n","\n","Este cuaderno está basado en:\n","\n","- https://blog.floydhub.com/the-transformer-in-pytorch/\n","- https://arxiv.org/pdf/1706.03762.pdf\n","- https://txt.cohere.com/what-are-transformer-models/\n","- https://jalammar.github.io/illustrated-transformer/\n"]},{"cell_type":"markdown","metadata":{"id":"Z43kfJGvlQyA"},"source":["# Fundamentos de los bloques de la arquitectura Transformers\n","\n","Veremos el modelo encoder-decoder del artículo:\n","\n","[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n","\n","Utilizaremos la biblioteca [PyTorch]() para poder entender cómo funciona el entrenamiento de estos modelos. Este cuaderno se podría utilizar para entrenar un GPT, pero llevaría días.\n","\n","Lo haremos en un subconjunto de los datos para ver cómo funciona.\n"]},{"cell_type":"markdown","metadata":{"id":"_sd2KYudl0Hn"},"source":["## La arquitectura\n","\n","![imagen](https://i.imgur.com/YPjbqW6.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPUnyvsuovuP"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math"]},{"cell_type":"markdown","metadata":{"id":"JR8M3oT0l8fM"},"source":["## Embedding\n","\n","![imagen](https://i.imgur.com/sFlEZ2e.png)\n","\n","El primer paso será transformar nuestra secuencia de tokens en vectores de incorporación para capturar información semántica. Esta capa de incorporación mejorará durante el entrenamiento del modelo, proporcionando una representación vectorial rica de los tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddpQjPJWmXZg"},"outputs":[],"source":["# Definimos una clase para la capa de embedding de entrada que hereda de nn.Module\n","class InputEmbeddings(nn.Module):\n","    def __init__(self, d_model: int, vocab_size: int) -> None:\n","        \"\"\"\n","        Este constructor inicializa la capa de embedding.\n","\n","        Parámetros:\n","        vocab_size - el tamaño de nuestro vocabulario, es decir, el número total de palabras únicas que podemos procesar.\n","        d_model - la dimensión de nuestros embeddings y la dimensión de entrada para nuestro modelo, que también será el tamaño de las salidas de la capa de embedding.\n","        \"\"\"\n","        # Inicializamos la clase base.\n","        super().__init__()\n","\n","        # Guardamos el tamaño del vocabulario y la dimensión del modelo como atributos de la clase.\n","        self.vocab_size = vocab_size\n","        self.d_model = d_model\n","\n","        # Creamos una capa de embedding, que convertirá índices de palabras en vectores de alta dimensión.\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        # Este método procesa las entradas a través de la capa de embedding.\n","        # Multiplica las embeddings por la raíz cuadrada de la dimensión del modelo para normalizar la magnitud de los vectores de embedding.\n","        return self.embedding(x) * math.sqrt(self.d_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1746587045571,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"},"user_tz":-120},"id":"Y-gSVeknMY53","outputId":"ea30506f-36d8-44dc-81a4-2742889fe12a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Forma del output: torch.Size([16, 350, 512])\n"]}],"source":["vocab_size = 1000  # Tamaño del vocabulario: número total de palabras únicas\n","d_model = 512  # Dimensión de los embeddings y la dimensión de entrada para el modelo\n","batch_size = 16  # Tamaño del lote: número de ejemplos procesados juntos\n","sequence_length = 350  # Longitud de la secuencia: número de tokens por entrada\n","\n","embedding_layer = InputEmbeddings(d_model, vocab_size)\n","\n","dummy_input = torch.randint(0, vocab_size, (batch_size, sequence_length))\n","\n","output = embedding_layer(dummy_input)\n","\n","print(f\"Forma del output: {output.shape}\")  # Imprime la forma del output\n"]},{"cell_type":"markdown","metadata":{"id":"ra5KCa1KnfrC"},"source":["## Positional Encoding\n","\n","![imagen](https://i.imgur.com/IIA3NK3.png)\n","\n","Para indicar la posición de cada token en la secuencia sin usar recurrencia ni convoluciones, inyectaremos información posicional en las incrustaciones de entrada, siguiendo el método propuesto en el artículo, mediante una combinación específica de funciones.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5K3NXh7MoM5D"},"outputs":[],"source":["# Definimos la clase PositionalEncoding, que hereda de nn.Module (un módulo estándar de PyTorch)\n","class PositionalEncoding(nn.Module):\n","  # El constructor de la clase recibe dimensiones del modelo (d_model), longitud de la secuencia (seq_len),\n","  # y la tasa de dropout (dropout) como argumentos.\n","\n","  def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n","    super().__init__()  # Inicializamos la superclase nn.Module\n","    self.d_model = d_model  # Dimensión del modelo\n","    self.seq_len = seq_len  # Longitud de la secuencia de entrada\n","    self.dropout = nn.Dropout(dropout)  # Definimos una capa de dropout para evitar el sobreajuste\n","\n","    # Inicializamos una matriz de ceros para los embeddings posicionales\n","    positional_embeddings = torch.zeros(seq_len, d_model)\n","    # Creamos un vector de secuencia posicional usando arange, que representa el índice posicional de cada token\n","    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","    # Calculamos los valores del modelo posicional, que se utilizan para generar las funciones seno y coseno\n","    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","    # Asignamos las funciones seno a los embeddings posicionales en las posiciones pares\n","    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n","    # Asignamos las funciones coseno a los embeddings posicionales en las posiciones impares\n","    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n","    # Añadimos una dimensión extra al principio para poder sumar este tensor con los embeddings de tokens\n","    positional_embeddings = positional_embeddings.unsqueeze(0)\n","\n","    # Registramos los embeddings posicionales como un buffer que no se considerará un parámetro del modelo\n","    # y por lo tanto no se actualizará durante el entrenamiento\n","    self.register_buffer('positional_embeddings', positional_embeddings)\n","\n","  # La función forward define cómo se procesa la entrada x a través del módulo\n","  def forward(self, x):\n","    # Sumamos los embeddings posicionales a los embeddings de tokens (x), asegurándonos que no se calculan\n","    # gradientes para los embeddings posicionales durante el entrenamiento\n","    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n","    # Aplicamos la capa de dropout a los embeddings resultantes y retornamos el resultado\n","    return self.dropout(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1746587045694,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"},"user_tz":-120},"id":"qnIqUHTxO6hw","outputId":"ed0d77be-c342-4656-9c16-447d85fe63bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Forma del output: torch.Size([16, 350, 512])\n"]}],"source":["# Definimos los parámetros de nuestra codificación posicional\n","d_model = 512\n","seq_len = 350\n","dropout = 0.1\n","\n","pos_enc = PositionalEncoding(d_model, seq_len, dropout)\n","\n","batch_size = 16\n","dummy_input = torch.rand(batch_size, seq_len, d_model)\n","\n","output = pos_enc(dummy_input)\n","\n","print(f\"Forma del output: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"ueiM7LzKpcFY"},"source":["## Add & Norm\n","\n","\n","![image](https://i.imgur.com/otdEq4D.png)"]},{"cell_type":"markdown","metadata":{"id":"_lDABPLSKqOt"},"source":["### Layer Normalization\n","\n","El primer paso es añadir la normalización de capas. ¡Puedes leer más sobre esto [aquí!](https://paperswithcode.com/method/layer-normalization)!\n","\n","La idea básica es que hace que el entrenamiento del modelo sea un poco más fácil y permite al modelo generalizar un poco mejor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Nlv7BH7ruSf"},"outputs":[],"source":["class LayerNormalization(nn.Module):\n","\n","    def __init__(self, features: int, epsilon: float = 1e-6) -> None:\n","        super().__init__()\n","        self.epsilon = epsilon  # Pequeño valor añadido para estabilidad numérica durante la división\n","        # Parámetros entrenables que escalan (gamma) y desplazan (beta) la normalización\n","        self.gamma = nn.Parameter(torch.ones(features))  # Gamma: Parámetro de escala inicializado a 1\n","        self.beta = nn.Parameter(torch.zeros(features))  # Beta: Parámetro de desplazamiento inicializado a 0\n","\n","    def forward(self, x):\n","        # Calcula la media de cada muestra de manera independiente\n","        mean = x.mean(dim=-1, keepdim=True)\n","        # Calcula la desviación estándar de cada muestra\n","        standard_deviation = x.std(dim=-1, keepdim=True)\n","        # Aplica la normalización de capa y luego escala y desplaza los resultados\n","        return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"]},{"cell_type":"markdown","metadata":{"id":"RPNC9L8PV3vv"},"source":["Epsilon nos permite evitar divisiones por cero"]},{"cell_type":"markdown","metadata":{"id":"rwXt7KKKycrl"},"source":["### Residual Connection\n","\n","Otra técnica que facilita el entrenamiento del modelo es añadir una conexión residual a las salidas del Bloque de Atención - esto ayuda a prevenir la desvanecimiento del gradiente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKwGFD2-yd-Z"},"outputs":[],"source":["class ResidualConnection(nn.Module):\n","  def __init__(self, features: int, dropout: float = 0.1) -> None:\n","    super().__init__()\n","    # Creamos una capa de dropout para regularizar el aprendizaje.\n","    self.dropout = nn.Dropout(dropout)\n","    # Inicializamos la normalización de capa con el número de características dado.\n","    self.layernorm = LayerNormalization(features)\n","\n","  # La función forward define el paso hacia adelante del módulo.\n","  def forward(self, x, sublayer):\n","    # Devolvemos la suma de la entrada original x y la salida de la subcapa,\n","    # aplicando dropout y normalización de capa a la entrada antes de pasarla a la subcapa.\n","    return x + self.dropout(sublayer(self.layernorm(x)))"]},{"cell_type":"markdown","metadata":{"id":"IIOZp3xhsaXK"},"source":["## Feed Forward Network\n","\n","![image](https://i.imgur.com/woEqBjQ.png)\n","\n","Las redes feed forward tienen dos propósitos en nuestro modelo:\n","\n","1. Transforman las salidas de atención en un formato que funciona con el bloque siguiente.\n","\n","2. Ayudan a añadir complejidad para evitar que cada bloque de atención actúe de manera similar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JueNG2UBszbR"},"outputs":[],"source":["class FeedForwardBlock(nn.Module):\n","  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n","    \"\"\"\n","    Inicializa una capa feed-forward en el modelo de Transformer.\n","\n","    Argumentos:\n","    d_model (int): La dimensión del modelo, es decir, el tamaño de los embeddings o vectores de tokens.\n","    d_ff (int): La dimensión de la red feed-forward interna, usualmente mucho mayor que d_model.\n","    dropout (float): La tasa de dropout, una medida de regularización que desactiva aleatoriamente\n","                     partes de las activaciones (nodos) durante el entrenamiento para prevenir el sobreajuste.\n","    \"\"\"\n","    super().__init__()\n","    # La primera capa lineal transforma los vectores de entrada a una dimensión mayor (d_ff).\n","    self.linear_1 = nn.Linear(d_model, d_ff)\n","\n","    # Dropout para añadir regularización y prevenir el sobreajuste.\n","    self.dropout = nn.Dropout(dropout)\n","\n","    # La segunda capa lineal transforma los vectores de la dimensión d_ff de nuevo a d_model.\n","    self.linear_2 = nn.Linear(d_ff, d_model)\n","\n","  def forward(self, x):\n","    # Aplica la primera transformación lineal seguida de una función de activación ReLU,\n","    # luego aplica dropout y finalmente una segunda transformación lineal.\n","    # La función de activación ReLU se utiliza para añadir no linealidad al modelo.\n","    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"]},{"cell_type":"markdown","metadata":{"id":"bJ5EGkvdtP0O"},"source":["## Multi-Head Attention\n","\n","![image](https://i.imgur.com/4qOT46y.png)\n","\n","El nucleo del transformer!"]},{"cell_type":"markdown","metadata":{"id":"-Oto52ZyvX0k"},"source":["### Multi-Head Attention Class\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDHkw1f_vmuv"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n","    super().__init__()\n","\n","    # d_model es la dimensión de los embeddings/representaciones del modelo.\n","    self.d_model = d_model\n","\n","    # num_heads es el número de cabezas de atención.\n","    self.num_heads = num_heads\n","\n","    # Verifica que d_model sea divisible por el número de cabezas para asegurar una división igual de los vectores.\n","    assert d_model % num_heads == 0, \"d_model no es divisible por num_heads\"\n","\n","    # d_k representa la dimensión de las claves, valores y consultas para cada cabeza de atención.\n","    self.d_k = d_model // num_heads\n","\n","    # Define las matrices de pesos para las consultas, claves y valores.\n","    self.w_q = nn.Linear(d_model, d_model, bias=False)  # Pesos para las consultas\n","    self.w_k = nn.Linear(d_model, d_model, bias=False)  # Pesos para las claves\n","    self.w_v = nn.Linear(d_model, d_model, bias=False)  # Pesos para los valores\n","\n","    # Después de concatenar las salidas de las cabezas, esta matriz de pesos proyecta de nuevo al d_model original.\n","    self.w_o = nn.Linear(d_model, d_model, bias=False)\n","\n","    # Define una capa de dropout para regularizar el aprendizaje.\n","    self.dropout = nn.Dropout(dropout)\n"]},{"cell_type":"markdown","metadata":{"id":"FyD7nAM6tb0K"},"source":["### Scaled Dot-Product Attention\n","\n","![image](https://i.imgur.com/Yp48DuB.png)"]},{"cell_type":"markdown","metadata":{"id":"TvLhlQglAkdR"},"source":["Entradas Q, K, V: La atención toma tres conjuntos de entradas; las consultas (Q), las claves (K) y los valores (V). Estas son proyecciones lineales (transformaciones afines) de la entrada en diferentes espacios.\n","\n","- **MatMul entre Q y K**: Se realiza un *scaled dot* (multiplicación matricial) entre las consultas y las claves. Esto resulta en una matriz de puntuaciones de atención que indica cuánta atención se debe prestar a otros elementos en la secuencia al procesar un elemento específico.\n","\n","- **Escala**: Las puntuaciones de atención se escalan dividiéndolas por la raíz cuadrada de la dimensión de las claves (sqrt(d_k)). Esto es para evitar gradientes muy pequeños, ya que el *scaled dot* puede aumentar significativamente con el aumento de la dimensión de las claves.\n","\n","- **Máscara (opcional)**: Si se utiliza una máscara (por ejemplo, para evitar que se preste atención a los tokens de relleno o para implementar la atención causal donde cada posición solo puede atender a posiciones anteriores en la secuencia), esta se aplica antes del softmax. Los valores de la máscara suelen ser 0 (para posiciones que deben considerarse) y -inf o un número muy grande negativo (para posiciones que deben ignorarse) de tal manera que después de aplicar softmax, estas últimas posiciones reciban efectivamente una puntuación de atención de cero.\n","\n","- **SoftMax**: Se aplica la función softmax a las puntuaciones de atención escaladas para cada posición, resultando en una distribución de probabilidad que suma 1. Esto garantiza que solo se preste atención a las posiciones más relevantes.\n","\n","- **MatMul con V**: Las puntuaciones de atención softmax se utilizan para ponderar los valores (V). Esto se hace multiplicando las puntuaciones de atención softmax con los valores. Esto significa que cada valor se pondera por cuánta atención la clave correspondiente debería recibir.\n","\n","- **Salida**: El resultado de este *scaled dot* es el resultado de la atención, que luego se puede pasar a través de más capas en una red o utilizarse para producir una salida.\n","\n","Este mecanismo de atención permite que el modelo se concentre selectivamente en partes relevantes de la entrada y es fundamental para permitir que los modelos de transformadores manejen secuencias largas y realicen tareas como la traducción automática, el procesamiento del lenguaje natural y más.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKk08SvowJIc"},"outputs":[],"source":["def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n","  # Calculemos las puntuaciones de atención realizando el producto escalar de los queries y keys,\n","  # y luego dividiendo por la raíz cuadrada de la dimensión d_k para estabilizar los gradientes.\n","  attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","\n","  # Si se proporciona una máscara, la aplicamos a las puntuaciones de atención.\n","  # Esto puede ser útil para evitar que el modelo considere ciertas posiciones (por ejemplo, en los tokens rellenados o en los tokens futuros en el decodificador).\n","  if mask is not None:\n","    attention_scores = attention_scores.masked_fill_(mask == 0, -1e9)\n","\n","  # Normalizamos las puntuaciones de atención usando softmax, para que sumen 1 y se puedan interpretar como probabilidades.\n","  attention_scores = attention_scores.softmax(dim=-1)\n","\n","  # Si se ha proporcionado una capa de dropout, la aplicamos a las puntuaciones de atención.\n","  # Esto ayuda a prevenir el sobreajuste durante el entrenamiento.\n","  if dropout is not None:\n","    attention_scores = dropout(attention_scores)\n","\n","  # Retornamos la salida de la atención, que es el producto escalar de las puntuaciones de atención y los values,\n","  # junto con las puntuaciones de atención para posibles usos posteriores (como visualizaciones).\n","  return (attention_scores @ value), attention_scores"]},{"cell_type":"markdown","metadata":{"id":"Ac8k7b3SxKOC"},"source":["### Forward Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"um2Oxv1axWYK"},"outputs":[],"source":["def forward(self, query, key, value, mask):\n","  # Aplica una capa lineal a las queries, keys y values para transformarlas en un nuevo espacio.\n","  query = self.w_q(query)\n","  key = self.w_k(key)\n","  value = self.w_v(value)\n","\n","  # Reorganiza las queries, keys y values para agruparlas según los multi-heads y\n","  # prepararlas para la operación de atención, separando las dimensiones de cabezales y características.\n","  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","\n","  # Calcula la atención multi-cabeza utilizando las queries, keys y values modificadas, así como la máscara\n","  # proporcionada, y aplica un dropout si es necesario.\n","  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n","\n","  # Reorganiza la salida de la atención para convertirla de nuevo en la forma original\n","  # (tamaño del lote, longitud de secuencia, características).\n","  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n","\n","  # Aplica otra capa lineal para transformar la salida de la atención multi-cabeza en el espacio deseado.\n","  return self.w_o(x)"]},{"cell_type":"markdown","metadata":{"id":"kgr_D0mmyEgp"},"source":["### MultiHeadAttention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odYKdmwMyH4P"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n","    super().__init__()\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n","\n","    self.d_k = d_model // num_heads\n","\n","    self.w_q = nn.Linear(d_model, d_model, bias=False)\n","    self.w_k = nn.Linear(d_model, d_model, bias=False)\n","    self.w_v = nn.Linear(d_model, d_model, bias=False)\n","\n","    self.w_o = nn.Linear(d_model, d_model, bias=False)\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","  @staticmethod\n","  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n","    d_k = query.shape[-1]\n","\n","    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","\n","    if mask is not None:\n","      attention_scores.masked_fill_(mask == 0, -1e9)\n","\n","    attention_scores = attention_scores.softmax(dim=-1)\n","\n","    if dropout is not None:\n","      attention_scores = dropout(attention_scores)\n","\n","    return (attention_scores @ value), attention_scores\n","\n","  def forward(self, query, key, value, mask):\n","    query = self.w_q(query)\n","    key = self.w_k(key)\n","    value = self.w_v(value)\n","\n","    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n","\n","    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n","\n","    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n","\n","    return self.w_o(x)"]},{"cell_type":"markdown","metadata":{"id":"EkKjLhpiyz5b"},"source":["## Encoder\n","\n","Con todos los bloques implementados, vamos a unirlos para crear el Encoder\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f-0edIfMzijj"},"source":["### Encoder Block\n","\n","![image](https://i.imgur.com/nwNYZAT.png)\n","\n","\n","El codificador toma la frase en el idioma fuente (por ejemplo, inglés). Cada palabra se convierte en una representación vectorial utilizando una capa de incrustación (embedding). Luego, un codificador posicional añade información sobre la posición de cada palabra. Esto pasa por múltiples capas de autoatención, donde cada vector de palabra presta atención a todos los otros vectores de palabras para construir representaciones contextuales.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMVnZiGDy1PG"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","    super().__init__()\n","\n","    # Asigna el bloque de autoatención MultiHeadAttention a esta clase.\n","    self.self_attention_block = self_attention_block\n","\n","    # Asigna el bloque FeedForwardBlock a esta clase.\n","    self.feed_forward_block = feed_forward_block\n","\n","    # Crea una lista de conexiones residuales, con dos elementos para este bloque de codificador.\n","    # Estas son utilizadas para añadir la entrada original (residual) a la salida de cada subcapa,\n","    # seguido de una normalización de capa.\n","    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n","\n","  def forward(self, x, input_mask):\n","    # Aplica la primera conexión residual y el bloque de autoatención.\n","    # La función lambda permite pasar la salida de la autoatención como segundo argumento a la conexión residual.\n","    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n","\n","    # Aplica la segunda conexión residual y el bloque de alimentación adelante.\n","    x = self.residual_connections[1](x, self.feed_forward_block)\n","\n","    # Retorna la salida procesada del bloque de codificador.\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"a-AvnoPrzvwu"},"source":["### Encoder Stack\n","\n","\n","Siguiendo el artículo original - organizaremos estos bloques en un conjunto de 6.\n","\n","Estos 6 Bloques Codificadores (cada uno con 8 Cabezas de Atención) conformarán nuestro Stack de Codificación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOaR5SjUzxv7"},"outputs":[],"source":["class EncoderStack(nn.Module):\n","  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","    super().__init__()\n","    self.layers = layers\n","    self.norm = LayerNormalization(features) # Inicializamos la normalización de capa que se aplicará al final\n","\n","  def forward(self, x, mask):\n","    for layer in self.layers: # Iteramos sobre todas las capas del encoder\n","      x = layer(x, mask) # Aplicamos cada capa a la entrada x con la máscara proporcionada\n","    return self.norm(x) # Retornamos la salida normalizada de la última capa\n"]},{"cell_type":"markdown","metadata":{"id":"fQyujsRTz5NT"},"source":["## Decoder\n","\n","Vamos a hacer lo mismo con el Decoder!"]},{"cell_type":"markdown","metadata":{"id":"zBYgl77Kz6bx"},"source":["### Decoder Block\n","\n","![image](https://i.imgur.com/HtAAXZc.png)\n","\n","\n","\n","El decodificador repite la frase en el idioma objetivo (por ejemplo, español). También convierte las palabras en vectores y añade información posicional. Luego pasa por capas de autoatención. Aquí, se aplica una máscara de manera que cada palabra solo puede ver las palabras anteriores, no las posteriores.\n","\n","El decodificador también realiza atención sobre la salida del encoder. Esto permite que cada palabra en francés encuentre conexiones relevantes con las palabras en inglés."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIwafbqzz5-n"},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","    super().__init__()\n","    self.self_attention_block = self_attention_block\n","    self.cross_attention_block = cross_attention_block\n","    self.feed_forward_block = feed_forward_block\n","    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n","\n","  def forward(self, x, encoder_output, input_mask, target_mask):\n","    # Aplicación de la primera conexión residual y el bloque de atención propia\n","    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n","    # Aplicación de la segunda conexión residual y el bloque de atención cruzada con la salida del codificador\n","    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n","    # Aplicación de la tercera conexión residual y el bloque de avance rápido\n","    x = self.residual_connections[2](x, self.feed_forward_block)\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"sa4yiTNn0BkA"},"source":["### Decoder Stack\n","\n","Usaremos el mismo número de Bloques Decodificadores que usamos en los Bloques Codificadores - lo que nos deja con 6 Bloques Decodificadores en nuestra Pila de Decodificadores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QUXzOXk0CcT"},"outputs":[],"source":["class DecoderStack(nn.Module):\n","  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","    super().__init__()\n","    self.layers = layers\n","    self.norm = LayerNormalization(features)\n","\n","  # Definimos la función forward que será llamada durante la propagación hacia adelante\n","  def forward(self, x, encoder_output, input_mask, target_mask):\n","    # Iteramos a través de cada capa en la lista de capas\n","    for layer in self.layers:\n","      x = layer(x, encoder_output, input_mask, target_mask)  # Pasamos las entradas a través de la capa\n","    return self.norm(x)\n"]},{"cell_type":"markdown","metadata":{"id":"kRFiAP580S4b"},"source":["## Linear Projection Layer\n","\n","Después de las capas de autoatención del decodificador y de atención entre el codificador y el decodificador, tenemos un vector de contexto que representa cada palabra española. Este vector de contexto tiene una alta dimensión (por ejemplo, 512 o 1024).\n","\n","Tomamos este vector de contexto y generamos una distribución de probabilidad sobre el vocabulario francés para poder elegir la próxima palabra traducida.\n","\n","La capa de proyección lineal ayuda con esto. Proyecta el vector de contexto en un vector mucho más grande llamado la distribución del vocabulario - una entrada por cada palabra del vocabulario.\n","\n","Por ejemplo, si nuestro vocabulario catalán tiene 50,000 palabras, la distribución del vocabulario tendrá 50,000 dimensiones. Cada dimensión corresponde a la probabilidad de que aquella palabra catalana sea la traducción correcta."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkBBMAZK0WLB"},"outputs":[],"source":["class LinearProjectionLayer(nn.Module):\n","  def __init__(self, d_model, vocab_size) -> None:\n","    super().__init__()\n","    self.proj = nn.Linear(d_model, vocab_size)\n","\n","  def forward(self, x) -> None:\n","    return self.proj(x)"]},{"cell_type":"markdown","metadata":{"id":"v9ucsRWs0lG9"},"source":["## The Transformer\n","\n","Si juntamos todas las partes... tenemos el **TRANSFORMER**!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ip11mmQ0nMM"},"outputs":[],"source":["class Transformer(nn.Module):\n","  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n","    super().__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.tgt_embed = tgt_embed\n","    self.src_pos = src_pos\n","    self.tgt_pos = tgt_pos\n","    self.projection_layer = projection_layer\n","\n","  def encode(self, src, src_mask):\n","    src = self.src_embed(src)\n","    src = self.src_pos(src)\n","    return self.encoder(src, src_mask)\n","\n","  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","    tgt = self.tgt_embed(tgt)\n","    tgt = self.tgt_pos(tgt)\n","    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","\n","  def project(self, x):\n","    return self.projection_layer(x)"]},{"cell_type":"markdown","metadata":{"id":"4Ssr5nA039--"},"source":["## Construyendo Nuestro Transformer\n","\n","Utilizaremos esta función de ayuda para crear nuestro Transformer...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdCt3wNi4EvY"},"outputs":[],"source":["def build_transformer(input_vocab_size: int, target_vocab_size: int, input_seq_len: int, target_seq_len: int, d_model: int=512, N: int=6, num_heads: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n","  # Creación de las incrustaciones de entrada para el vocabulario de entrada\n","  input_embeddings = InputEmbeddings(d_model, input_vocab_size)\n","  # Creación de las incrustaciones de entrada para el vocabulario objetivo\n","  target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n","\n","  # Codificación posicional para la secuencia de entrada\n","  input_position = PositionalEncoding(d_model, input_seq_len, dropout)\n","  # Codificación posicional para la secuencia objetivo\n","  target_position = PositionalEncoding(d_model, target_seq_len, dropout)\n","\n","  # Inicialización de la lista de bloques del codificador\n","  encoder_blocks = []\n","\n","  # Construcción de los bloques del codificador, repitiendo N veces\n","  for _ in range(N):\n","    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)  # Bloque de atención para el codificador\n","    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Bloque de avance rápido\n","    encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n","    encoder_blocks.append(encoder_block)  # Añadir el bloque completo al codificador\n","\n","  # Inicialización de la lista de bloques del decodificador\n","  decoder_blocks = []\n","\n","  # Construcción de los bloques del decodificador, repitiendo N veces\n","  for _ in range(N):\n","    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)  # Bloque de atención propia del decodificador\n","    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout)  # Bloque de atención cruzada para el decodificador\n","    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Bloque de avance rápido\n","    decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n","    decoder_blocks.append(decoder_block)  # Añadir el bloque completo al decodificador\n","\n","  # Empaquetado de todos los bloques del codificador en una pila\n","  encoder_stack = EncoderStack(d_model, nn.ModuleList(encoder_blocks))\n","  # Empaquetado de todos los bloques del decodificador en una pila\n","  decoder_stack = DecoderStack(d_model, nn.ModuleList(decoder_blocks))\n","\n","  # Capa de proyección lineal que mapea las salidas del decodificador al tamaño del vocabulario objetivo\n","  linear_projection_layer = LinearProjectionLayer(d_model, target_vocab_size)\n","\n","  # Construcción del transformador con todas las partes definidas anteriormente\n","  transformer = Transformer(encoder_stack, decoder_stack, input_embeddings, target_embeddings, input_position, target_position, linear_projection_layer)\n","\n","  # Inicialización de los parámetros del transformador usando Xavier Uniform\n","  for p in transformer.parameters():\n","    if p.dim() > 1:\n","      nn.init.xavier_uniform_(p)\n","\n","  return transformer\n"]},{"cell_type":"markdown","metadata":{"id":"_2jfvmRx4ZNd"},"source":["# ¡Vamos a entrenar nuestro TRANSFORMER!\n","\n","- Utilizaremos este recurso: [repositorio](https://github.com/hkproj/pytorch-transformer/tree/main)\n","\n","Usaremos su dataset de PyTorch para traducción de inglés a francés.\n"]},{"cell_type":"markdown","metadata":{"id":"SxF6JLcg5LJp"},"source":["## Creación de Conjuntos de Datos\n","\n","El `BilingualDataset` es una herramienta específica de PyTorch diseñada para facilitar el entrenamiento de modelos de traducción automática. Utiliza tokenizadores específicos para cada idioma para convertir pares de frases en formato de texto a una forma numérica que los modelos de machine learning pueden procesar. Este proceso incluye ajustar la longitud de todas las frases a un máximo predefinido, agregando tokens especiales que indican el inicio y el final de las frases, así como tokens de relleno para asegurar que todas las entradas y salidas tengan una longitud uniforme.\n","\n","Cuando se solicita un ejemplo de este conjunto de datos, se realiza la tokenización de las frases en los idiomas fuente y objetivo, se ajustan sus longitudes, y se generan tres tipos de tensores necesarios para el modelo: la entrada del codificador, la entrada del decodificador, y las etiquetas de salida. También se elaboran máscaras específicas que ayudan al modelo a distinguir entre la información real y los tokens de relleno, y a asegurar que la predicción de cada nuevo token por el decodificador se base únicamente en información previa, evitando cualquier influencia de tokens futuros.\n","\n","Esta configuración prepara el terreno para un entrenamiento eficaz de modelos secuencia-a-secuencia, optimizando el proceso de aprendizaje al tratar la naturaleza inherentemente secuencial del lenguaje, donde la predicción de palabras depende de las precedentes y no de las que vienen después.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLnUZRgk7S-G"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class BilingualDataset(Dataset):\n","  def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n","    super().__init__()\n","    self.seq_len = seq_len\n","    self.ds = ds\n","    self.tokenizer_src = tokenizer_src\n","    self.tokenizer_tgt = tokenizer_tgt\n","    self.src_lang = src_lang\n","    self.tgt_lang = tgt_lang\n","\n","    self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","    self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","    self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","  def __len__(self):\n","    return len(self.ds)\n","\n","  def __getitem__(self, idx):\n","    src_target_pair = self.ds[idx]\n","    src_text = src_target_pair['translation'][self.src_lang]\n","    tgt_text = src_target_pair['translation'][self.tgt_lang]\n","\n","    enc_input_tokens = self.tokenizer_src.encode(src_text).ids[:self.seq_len - 2]\n","    dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids[:self.seq_len - 1]\n","\n","    enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n","    dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","\n","    encoder_input = torch.cat(\n","        [\n","            self.sos_token,\n","            torch.tensor(enc_input_tokens, dtype=torch.int64),\n","            self.eos_token,\n","            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n","        ],\n","        dim=0,\n","    )\n","\n","    decoder_input = torch.cat(\n","        [\n","            self.sos_token,\n","            torch.tensor(dec_input_tokens, dtype=torch.int64),\n","            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","        ],\n","        dim=0,\n","    )\n","\n","    label = torch.cat(\n","        [\n","            torch.tensor(dec_input_tokens, dtype=torch.int64),\n","            self.eos_token,\n","            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","        ],\n","        dim=0,\n","    )\n","\n","    assert encoder_input.size(0) == self.seq_len\n","    assert decoder_input.size(0) == self.seq_len\n","    assert label.size(0) == self.seq_len\n","\n","    return {\n","        \"encoder_input\": encoder_input,\n","        \"decoder_input\": decoder_input,\n","        \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n","        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n","        \"label\": label,\n","        \"src_text\": src_text,\n","        \"tgt_text\": tgt_text,\n","    }\n","\n","def causal_mask(size):\n","  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","  return mask == 0\n"]},{"cell_type":"markdown","metadata":{"id":"E0hSYI7F8IAI"},"source":["## Constriumos el Tokenizer\n"]},{"cell_type":"code","source":["!pip install \\\n","  nvidia-cublas-cu12==12.4.5.8 \\\n","  nvidia-cuda-cupti-cu12==12.4.127 \\\n","  nvidia-cuda-nvrtc-cu12==12.4.127 \\\n","  nvidia-cuda-runtime-cu12==12.4.127 \\\n","  nvidia-cudnn-cu12==9.1.0.70 \\\n","  nvidia-cufft-cu12==11.2.1.3 \\\n","  nvidia-curand-cu12==10.3.5.147 \\\n","  nvidia-cusolver-cu12==11.6.1.9 \\\n","  nvidia-cusparse-cu12==12.3.1.170 \\\n","  nvidia-nvjitlink-cu12==12.4.127"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRdD5OTfPqfl","executionInfo":{"status":"ok","timestamp":1746587117735,"user_tz":-120,"elapsed":71837,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"23c18aa0-b635-408e-8a97-5d541c655744"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nvidia-cublas-cu12==12.4.5.8\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfmUwnpo8qou","outputId":"e0304f6b-67d1-4fa4-ac90-68788a288319","executionInfo":{"status":"ok","timestamp":1746587122770,"user_tz":-120,"elapsed":4975,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m368.6/491.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers tokenizers datasets -qU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAUUEXFz8eGH"},"outputs":[],"source":["def get_all_sentences(ds, lang):\n","    for item in ds:\n","      yield item['translation'][lang]"]},{"cell_type":"markdown","metadata":{"id":"vUDKOCTVUIa7"},"source":["Entrenaremos rápidamente un tokenizador en nuestro conjunto de datos tanto para nuestras lenguas fuente como objetivo. Nos aseguraremos de añadir los tokens especiales [UNK], [PAD], [SOS] y [EOS].\n","\n","Esto significa que se utilizará un proceso para enseñar al tokenizador a convertir el texto en secuencias numéricas, trabajando tanto con el idioma de origen como con el idioma al que se quiere traducir. Además, se incluirán cuatro tokens especiales con funciones específicas:\n","\n","- [UNK]: Representa cualquier palabra que el tokenizador no reconozca (\"unknown\").\n","- [PAD]: Se utiliza para rellenar las secuencias hasta una longitud uniforme, facilitando el procesamiento por modelos que requieren entradas de tamaño fijo.\n","- [SOS]: Marca el inicio de una secuencia de texto (\"start of sequence\").\n","- [EOS]: Indica el final de una secuencia de texto (\"end of sequence\").\n","\n","Añadir estos tokens especiales es crucial para el procesamiento y el entrenamiento eficaz de modelos de lenguaje, permitiendo al modelo reconocer y manejar situaciones como el comienzo y el final de las frases, así como gestionar entradas de diversas longitudes de manera más eficiente.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4s1ZHzKb8hTN"},"outputs":[],"source":["from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","def build_tokenizer(config, ds, lang):\n","  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","  tokenizer.pre_tokenizer = Whitespace()\n","  trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n","  tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n","  return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"MEUDihkcVRq1"},"source":["Creación del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI7xXnLo84cE"},"outputs":[],"source":["from torch.utils.data import DataLoader, random_split\n","\n","def get_ds(config):\n","  # Carga el conjunto de datos de la fuente especificada por la configuración, para un par de idiomas.\n","  ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n","\n","  # Construye los tokenizadores para los idiomas de origen y destino basados en el conjunto de datos y la configuración.\n","  tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n","  tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n","\n","  # Divide el conjunto de datos en un 90% para entrenamiento y un 10% para validación.\n","  train_ds_size = int(0.9 * len(ds_raw))\n","  val_ds_size = len(ds_raw) - train_ds_size\n","  train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","  # Crea los conjuntos de datos bilingües de entrenamiento y validación con los tokenizadores y la máxima longitud de secuencia especificados.\n","  train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","  val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","\n","  # Calcula la longitud máxima de las frases en los idiomas de origen y destino para determinar las dimensiones de relleno necesarias.\n","  max_len_src = 0\n","  max_len_tgt = 0\n","\n","  for item in ds_raw:\n","    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n","    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n","    max_len_src = max(max_len_src, len(src_ids))\n","    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n","\n","  # Imprime la longitud máxima encontrada para las frases de origen y destino.\n","  print(f'Longitud máxima de la frase de origen: {max_len_src}')\n","  print(f'Longitud máxima de la frase de destino: {max_len_tgt}')\n","\n","  # Crea cargadores de datos para los conjuntos de entrenamiento y validación, que permiten iterar sobre los conjuntos de datos en lotes durante el entrenamiento.\n","  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n","\n","  # Retorna los cargadores de datos y los tokenizadores para ambos idiomas.\n","  return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"]},{"cell_type":"markdown","metadata":{"id":"cr3Ys-ufVW1E"},"source":["Necesitamos algunas funciones de ayuda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK6k5X829JOo"},"outputs":[],"source":["def get_model(config, vocab_src_len, vocab_tgt_len):\n","  model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCALGXpf9tnv"},"outputs":[],"source":["def get_weights_file_path(config, epoch: str):\n","  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n","  model_filename = f\"{config['model_basename']}{epoch}.pt\"\n","  return str(Path('.') / model_folder / model_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPWZgCwD9vVX"},"outputs":[],"source":["def latest_weights_file_path(config):\n","  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n","  model_filename = f\"{config['model_basename']}*\"\n","  weights_files = list(Path(model_folder).glob(model_filename))\n","  if len(weights_files) == 0:\n","      return None\n","  weights_files.sort()\n","  return str(weights_files[-1])"]},{"cell_type":"markdown","metadata":{"id":"BNjAS-LvV557"},"source":["Este código define el proceso de entrenamiento para un modelo basado en la arquitectura Transformer utilizando PyTorch. Aquí se detalla cada parte del código:\n","\n","1. **Configuración del Entorno de Ejecución**: Inicialmente, determina si se puede utilizar CUDA (para GPUs NVIDIA), MPS (para aceleración en hardware Apple), o en su defecto, CPU para el entrenamiento. Esto optimiza el rendimiento de entrenamiento según el hardware disponible.\n","\n","2. **Comprobación y Creación de Directorios**: Asegura que el directorio donde se guardarán los pesos del modelo existe, evitando errores al guardar los resultados de entrenamiento.\n","\n","3. **Carga de Datos y Modelo**: Utiliza la función `get_ds` para obtener los cargadores de datos de entrenamiento y validación, junto con los tokenizadores para las lenguas de origen y destino. Después, carga el modelo y lo asigna al dispositivo de ejecución adecuado.\n","\n","4. **Configuración de Optimizador y Función de Pérdida**: Se establece un optimizador Adam para ajustar los pesos del modelo durante el entrenamiento, junto con una función de pérdida de Cross Entropy que ignora los tokens de relleno (padding) y utiliza suavización de etiquetas para mejorar la generalización.\n","\n","5. **Bucle de Entrenamiento**: Por cada época, itera sobre los lotes del conjunto de datos de entrenamiento. Para cada lote:\n","\n","   - Se mueven los datos de entrada y etiquetas al dispositivo correcto.\n","   - Se ejecuta el proceso de codificación, decodificación y proyección dentro del modelo.\n","   - Se calcula la pérdida comparando la salida del modelo con las etiquetas reales.\n","   - Se actualizan los pesos del modelo utilizando la retropropagación (backpropagation) y el optimizador.\n","\n","6. **Guardado de los Pesos del Modelo**: Después de cada época, se guardan los estados del modelo y del optimizador, así como el paso global, en un archivo. Esto permite reanudar el entrenamiento más adelante o utilizar el modelo entrenado para inferencia.\n","\n","El código también incluye buenas prácticas como el uso de `torch.cuda.empty_cache()` para liberar memoria no utilizada en la GPU, así como la inicialización de variables para controlar la época inicial y el paso global de entrenamiento. Además, hace uso de `tqdm` para visualizar el progreso de entrenamiento en tiempo real.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL1bH7is9OfS"},"outputs":[],"source":["import warnings\n","from tqdm import tqdm\n","import os\n","from pathlib import Path\n","\n","def train_model(config):\n","  # Definimos el dispositivo de ejecución\n","  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n","  print(\"Using device:\", device)\n","  if (device == 'cuda'):\n","    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n","    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n","  else:\n","    print(\"Please ensure you're in a GPU enabled Colab Notebook instance.\")\n","  device = torch.device(device)\n","\n","  # Asegúrate de que la carpeta de pesos existe\n","  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n","\n","  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n","  model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n","\n","  initial_epoch = 0\n","  global_step = 0\n","\n","  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","  for epoch in range(initial_epoch, config['num_epochs']):\n","    torch.cuda.empty_cache()\n","    model.train()\n","    batch_iterator = tqdm(train_dataloader, desc=f\"Procesando Época {epoch:02d}\")\n","    for batch in batch_iterator:\n","      encoder_input = batch['encoder_input'].to(device)\n","      decoder_input = batch['decoder_input'].to(device)\n","      encoder_mask = batch['encoder_mask'].to(device)\n","      decoder_mask = batch['decoder_mask'].to(device)\n","\n","      encoder_output = model.encode(encoder_input, encoder_mask)\n","      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n","      proj_output = model.project(decoder_output)\n","\n","      label = batch['label'].to(device)\n","\n","      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n","      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","      optimizer.zero_grad(set_to_none=True)\n","\n","      global_step += 1\n","\n","    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n","    torch.save({\n","      'epoch': epoch,\n","      'model_state_dict': model.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      'global_step': global_step\n","    }, model_filename)\n"]},{"cell_type":"markdown","metadata":{"id":"fzEWav-jNSCz"},"source":["Opus Books => [link](https://huggingface.co/datasets/opus_books)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f21pxGoS9-gM"},"outputs":[],"source":["config = {\n","  \"batch_size\": 16,\n","  \"num_epochs\": 1, #modifica a 5 para mejores resultados\n","  \"lr\": 10**-4,\n","  \"seq_len\": 350,\n","  \"d_model\": 512,\n","  \"datasource\": 'opus_books',\n","  \"lang_src\": \"en\",\n","  \"lang_tgt\": \"es\",\n","  \"model_folder\": \"weights\",\n","  \"model_basename\": \"encoder_decoder_model_\"\n","}"]},{"cell_type":"markdown","metadata":{"id":"PREBsXqZPVcn"},"source":["Con una A100 tarda unos 20 minutos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfBcSHUo-MU6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746589887061,"user_tz":-120,"elapsed":1107531,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"outputId":"261d4ab5-1023-4d89-81ec-56cb4f62451a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Device name: NVIDIA A100-SXM4-40GB\n","Device memory: 39.55743408203125 GB\n","Longitud máxima de la frase de origen: 767\n","Longitud máxima de la frase de destino: 782\n"]},{"output_type":"stream","name":"stderr","text":["Procesando Época 00: 100%|██████████| 5258/5258 [18:02<00:00,  4.86it/s, loss=5.439]\n"]}],"source":["train_model(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yo3eNHnYJe2e","executionInfo":{"status":"ok","timestamp":1746590568306,"user_tz":-120,"elapsed":23910,"user":{"displayName":"Eric Risco de la Torre","userId":"16820333876295128124"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"81660005-94ea-43b8-c773-6d61a21b565d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Longitud máxima de la frase de origen: 767\n","Longitud máxima de la frase de destino: 782\n","Result: ¡ !\n","Result: ¿ qué ?\n"]}],"source":["import torch\n","\n","def load_model(config):\n","    # Carga el modelo y los tokenizadores\n","    _, _, tokenizer_src, tokenizer_tgt = get_ds(config)\n","    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n","\n","    # Cargar los pesos del modelo\n","    model_path = get_weights_file_path(config, \"00\")  # Asegúrate de tener un archivo final o especificar la época\n","    checkpoint = torch.load(model_path, map_location=config['device'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model, tokenizer_src, tokenizer_tgt\n","\n","def translate(text, model, tokenizer_src, tokenizer_tgt, config):\n","    model.eval()  # Poner el modelo en modo de evaluación\n","    model.to(config['device'])\n","\n","    # Preprocesar el texto de entrada\n","    tokens_src = tokenizer_src.encode(text)\n","    tokens_src_tensor = torch.tensor(tokens_src.ids).unsqueeze(0).to(config['device'])  # Correcto\n","    encoder_mask = torch.ones(tokens_src_tensor.shape).to(config['device'])\n","\n","    with torch.no_grad():\n","        # Pasar por el modelo\n","        encoder_output = model.encode(tokens_src_tensor, encoder_mask)  # Utilizar tokens_src_tensor aquí\n","        # Iniciar con el token inicial para la decodificación\n","        decoder_input = torch.tensor([[tokenizer_tgt.token_to_id('[SOS]')]]).to(config['device'])\n","        output_tokens = []\n","\n","        # Decodificación autoregresiva\n","        for _ in range(210):  # Definir un máximo de longitud de salida\n","            decoder_mask = torch.ones(decoder_input.shape).to(config['device'])\n","            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n","            proj_output = model.project(decoder_output[:, -1, :])\n","            next_token_id = proj_output.argmax(1).item()\n","            if next_token_id == tokenizer_tgt.token_to_id('[EOS]'):\n","                break  # Finalizar si se encuentra el token de fin\n","            output_tokens.append(next_token_id)\n","            decoder_input = torch.cat([decoder_input, torch.tensor([[next_token_id]], device=config['device'])], dim=1)\n","\n","        translated_text = tokenizer_tgt.decode(output_tokens)  # Decodificar los tokens de salida a texto\n","\n","    return translated_text\n","\n","# Configura el dispositivo correctamente\n","config['device'] = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps() else \"cpu\"\n","\n","# Cargar el modelo y los tokenizadores\n","model, tokenizer_src, tokenizer_tgt = load_model(config)\n","\n","# Ejemplo de uso\n","text = \"Good Morning\"\n","translated_text = translate(text, model, tokenizer_src, tokenizer_tgt, config)\n","print(f\"Result: {translated_text}\")\n","\n","text = \"Are you alive?\"\n","translated_text = translate(text, model, tokenizer_src, tokenizer_tgt, config)\n","print(f\"Result: {translated_text}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}